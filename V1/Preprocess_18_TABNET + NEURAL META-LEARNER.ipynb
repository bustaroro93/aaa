{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2aad7b-ce96-4ec6-8e89-7af4ca2f9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ===============================================\n",
    "# PP_16 - TABNET + NEURAL META-LEARNER (GOD SOTA 2026)\n",
    "# TennisTitan - Advanced Stacking Architecture\n",
    "# ===============================================\n",
    "#\n",
    "# Ajouts:\n",
    "# 1. TabNet comme 4Ã¨me modÃ¨le de base (attention intÃ©grÃ©e)\n",
    "# 2. Neural Meta-Learner (MLP) au lieu de LogReg\n",
    "# 3. Multi-seed + OOF stacking\n",
    "# 4. Calibration avancÃ©e (Isotonic + Temperature)\n",
    "#\n",
    "# Input: data_clean/ml_final/\n",
    "# Output: models/god_sota_2026/\n",
    "# ===============================================\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "try:\n",
    "    from venn_abers import VennAbersCalibrator\n",
    "    VENN_ABERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Venn-ABERS not installed. Run: pip install venn-abers\")\n",
    "    VENN_ABERS_AVAILABLE = False\n",
    "    \n",
    "# TabNet\n",
    "try:\n",
    "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "    TABNET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ TabNet not installed. Run: pip install pytorch-tabnet\")\n",
    "    TABNET_AVAILABLE = False\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURATION\n",
    "# ===============================================\n",
    "ROOT = Path(r\"C:\\Users\\Administrateur\\Tennis POLAR v2\")\n",
    "DATA_DIR = ROOT / \"data_clean\" / \"ml_final\"\n",
    "OUTPUT_DIR = ROOT / \"models\" / \"god_sota_2026\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Seeds for multi-seed ensembling\n",
    "SEEDS = [42, 123, 456, 789, 2024]\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"   PP_16 - TABNET + NEURAL META-LEARNER (GOD SOTA 2026)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Seeds: {SEEDS}\")\n",
    "print(f\"   TabNet available: {TABNET_AVAILABLE}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# NEURAL META-LEARNER\n",
    "# ===============================================\n",
    "\n",
    "class NeuralMetaLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP meta-learner to combine base model predictions.\n",
    "    \n",
    "    Input: [lgbm_prob, xgb_prob, cat_prob, tabnet_prob, context_features]\n",
    "    Output: Final probability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_base_models: int = 4, n_context: int = 5, \n",
    "                 hidden_dim: int = 32, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_dim = n_base_models + n_context\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout / 2),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Label smoothing parameter\n",
    "        self.label_smoothing = 0.05\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)\n",
    "    \n",
    "    def loss(self, preds, targets):\n",
    "        \"\"\"BCE with label smoothing.\"\"\"\n",
    "        # Smooth labels\n",
    "        targets_smooth = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        return F.binary_cross_entropy(preds, targets_smooth)\n",
    "\n",
    "\n",
    "def train_neural_meta(X_meta: np.ndarray, y: np.ndarray, \n",
    "                      X_val: np.ndarray = None, y_val: np.ndarray = None,\n",
    "                      epochs: int = 100, lr: float = 0.001, patience: int = 10):\n",
    "    \"\"\"Train the neural meta-learner.\"\"\"\n",
    "    \n",
    "    model = NeuralMetaLearner(\n",
    "        n_base_models=X_meta.shape[1] - 5 if X_meta.shape[1] > 5 else X_meta.shape[1],\n",
    "        n_context=min(5, X_meta.shape[1]),\n",
    "        hidden_dim=32,\n",
    "        dropout=0.3\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train_t = torch.tensor(X_meta, dtype=torch.float32).to(DEVICE)\n",
    "    y_train_t = torch.tensor(y, dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    if X_val is not None:\n",
    "        X_val_t = torch.tensor(X_val, dtype=torch.float32).to(DEVICE)\n",
    "        y_val_t = torch.tensor(y_val, dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        preds = model(X_train_t)\n",
    "        loss = model.loss(preds, y_train_t)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        if X_val is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_val_t)\n",
    "                val_loss = F.binary_cross_entropy(val_preds, y_val_t)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss.item()\n",
    "                patience_counter = 0\n",
    "                best_state = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "    \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# TABNET WRAPPER\n",
    "# ===============================================\n",
    "\n",
    "def train_tabnet(X_train, y_train, X_val, y_val, seed=42):\n",
    "    \"\"\"Train TabNet classifier.\"\"\"\n",
    "    \n",
    "    if not TABNET_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    model = TabNetClassifier(\n",
    "        n_d=64,\n",
    "        n_a=64,\n",
    "        n_steps=5,\n",
    "        gamma=1.5,\n",
    "        lambda_sparse=0.001,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='entmax',\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        device_name='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=['logloss'],\n",
    "        max_epochs=100,\n",
    "        patience=15,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=256,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# OOF STACKING\n",
    "# ===============================================\n",
    "\n",
    "def compute_oof_predictions(X, y, model_type: str, seeds: list, n_folds: int = 5):\n",
    "    \"\"\"\n",
    "    Compute Out-of-Fold predictions for a model.\n",
    "    Returns OOF predictions and trained models.\n",
    "    \"\"\"\n",
    "    \n",
    "    oof_preds = np.zeros(len(y))\n",
    "    models = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "        seed_models = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            if model_type == \"lgbm\":\n",
    "                model = lgb.LGBMClassifier(\n",
    "                    n_estimators=1000,\n",
    "                    learning_rate=0.05,\n",
    "                    num_leaves=31,\n",
    "                    max_depth=8,\n",
    "                    min_child_samples=100,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    reg_alpha=0.1,\n",
    "                    reg_lambda=1.0,\n",
    "                    random_state=seed,\n",
    "                    verbosity=-1,\n",
    "                    force_row_wise=True,\n",
    "                )\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[lgb.early_stopping(100, verbose=False)]\n",
    "                )\n",
    "                preds = model.predict_proba(X_val)[:, 1]\n",
    "                \n",
    "            elif model_type == \"xgb\":\n",
    "                model = xgb.XGBClassifier(\n",
    "                    n_estimators=1000,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=6,\n",
    "                    min_child_weight=10,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    reg_alpha=0.1,\n",
    "                    reg_lambda=1.0,\n",
    "                    random_state=seed,\n",
    "                    eval_metric='logloss',\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbosity=0,\n",
    "                )\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "                preds = model.predict_proba(X_val)[:, 1]\n",
    "                \n",
    "            elif model_type == \"cat\":\n",
    "                model = CatBoostClassifier(\n",
    "                    iterations=1000,\n",
    "                    learning_rate=0.05,\n",
    "                    depth=6,\n",
    "                    l2_leaf_reg=3,\n",
    "                    random_seed=seed,\n",
    "                    verbose=False,\n",
    "                    early_stopping_rounds=100,\n",
    "                )\n",
    "                model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "                preds = model.predict_proba(X_val)[:, 1]\n",
    "                \n",
    "            elif model_type == \"tabnet\" and TABNET_AVAILABLE:\n",
    "                model = train_tabnet(X_train, y_train, X_val, y_val, seed)\n",
    "                if model is not None:\n",
    "                    preds = model.predict_proba(X_val)[:, 1]\n",
    "                else:\n",
    "                    preds = np.full(len(val_idx), 0.5)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            oof_preds[val_idx] += preds / len(seeds)\n",
    "            seed_models.append(model)\n",
    "        \n",
    "        models.append(seed_models)\n",
    "    \n",
    "    return oof_preds, models\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# CALIBRATION\n",
    "# ===============================================\n",
    "\n",
    "class TemperatureScaling:\n",
    "    \"\"\"Temperature scaling for calibration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.temperature = 1.0\n",
    "    \n",
    "    def fit(self, logits, y):\n",
    "        \"\"\"Find optimal temperature.\"\"\"\n",
    "        from scipy.optimize import minimize_scalar\n",
    "        \n",
    "        def nll(T):\n",
    "            scaled = 1 / (1 + np.exp(-logits / T))\n",
    "            scaled = np.clip(scaled, 1e-7, 1-1e-7)\n",
    "            return log_loss(y, scaled)\n",
    "        \n",
    "        result = minimize_scalar(nll, bounds=(0.1, 10), method='bounded')\n",
    "        self.temperature = result.x\n",
    "        return self\n",
    "    \n",
    "    def transform(self, logits):\n",
    "        return 1 / (1 + np.exp(-logits / self.temperature))\n",
    "\n",
    "\n",
    "def calibrate_predictions(preds, y, method='isotonic'):\n",
    "    \"\"\"Calibrate predictions.\"\"\"\n",
    "    \n",
    "    if method == 'isotonic':\n",
    "        calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "        calibrator.fit(preds, y)\n",
    "        return calibrator\n",
    "    \n",
    "    elif method == 'platt':\n",
    "        calibrator = LogisticRegression(C=1.0, solver='lbfgs')\n",
    "        calibrator.fit(preds.reshape(-1, 1), y)\n",
    "        return calibrator\n",
    "    \n",
    "    elif method == 'temperature':\n",
    "        # Convert to logits\n",
    "        logits = np.log(preds / (1 - preds + 1e-7))\n",
    "        calibrator = TemperatureScaling()\n",
    "        calibrator.fit(logits, y)\n",
    "        return calibrator\n",
    "    \n",
    "    return None\n",
    "\n",
    "def calibrate_venn_abers(train_probs: np.ndarray, train_labels: np.ndarray,\n",
    "                         val_probs: np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Calibration Venn-ABERS - produit des intervalles de confiance.\n",
    "    \n",
    "    Returns:\n",
    "        calibrator: Fitted VennAbersCalibrator\n",
    "        p_low, p_high: Lower and upper probability bounds (if val_probs provided)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not VENN_ABERS_AVAILABLE:\n",
    "        print(\"  âš ï¸ Venn-ABERS not available, skipping\")\n",
    "        return None, None, None\n",
    "    \n",
    "    calibrator = VennAbersCalibrator()\n",
    "    \n",
    "    # Venn-ABERS needs probabilities in shape (n_samples,)\n",
    "    calibrator.fit(train_probs.reshape(-1), train_labels.reshape(-1))\n",
    "    \n",
    "    if val_probs is not None:\n",
    "        # predict_proba returns (p_low, p_high) for each sample\n",
    "        p_low, p_high = calibrator.predict_proba(val_probs.reshape(-1))\n",
    "        return calibrator, p_low, p_high\n",
    "    \n",
    "    return calibrator, None, None\n",
    "    \n",
    "# ===============================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# ===============================================\n",
    "\n",
    "def main():\n",
    "    t0 = datetime.now()\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n[1/7] Loading data...\")\n",
    "    \n",
    "    train = pl.read_parquet(DATA_DIR / \"train.parquet\").to_pandas()\n",
    "    val = pl.read_parquet(DATA_DIR / \"val.parquet\").to_pandas()\n",
    "    test = pl.read_parquet(DATA_DIR / \"test.parquet\").to_pandas()\n",
    "    \n",
    "    # Prepare features\n",
    "    exclude_cols = [\"target_A_wins\", \"custom_match_id\", \"match_id_ta_dedup\", \n",
    "                    \"winner_id\", \"loser_id\", \"year\"]\n",
    "    feature_cols = [c for c in train.columns if c not in exclude_cols]\n",
    "    \n",
    "    X_train = train[feature_cols].values.astype(np.float32)\n",
    "    y_train = train[\"target_A_wins\"].values.astype(np.int32)\n",
    "    X_val = val[feature_cols].values.astype(np.float32)\n",
    "    y_val = val[\"target_A_wins\"].values.astype(np.int32)\n",
    "    X_test = test[feature_cols].values.astype(np.float32)\n",
    "    y_test = test[\"target_A_wins\"].values.astype(np.int32)\n",
    "    \n",
    "    # Replace NaN\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0)\n",
    "    X_val = np.nan_to_num(X_val, nan=0.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    \n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Val: {X_val.shape}\")\n",
    "    print(f\"  Test: {X_test.shape}\")\n",
    "    print(f\"  Features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Combine train + val for OOF\n",
    "    X_trainval = np.vstack([X_train, X_val])\n",
    "    y_trainval = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # =====================================\n",
    "    # OOF PREDICTIONS\n",
    "    # =====================================\n",
    "    print(\"\\n[2/7] Computing OOF predictions...\")\n",
    "    \n",
    "    oof_results = {}\n",
    "    all_models = {}\n",
    "    \n",
    "    for model_type in [\"lgbm\", \"xgb\", \"cat\", \"tabnet\"]:\n",
    "        print(f\"\\n  Training {model_type.upper()}...\")\n",
    "        \n",
    "        if model_type == \"tabnet\" and not TABNET_AVAILABLE:\n",
    "            print(f\"  âš ï¸ Skipping TabNet (not installed)\")\n",
    "            continue\n",
    "        \n",
    "        oof_preds, models = compute_oof_predictions(\n",
    "            X_trainval, y_trainval, model_type, SEEDS, N_FOLDS\n",
    "        )\n",
    "        \n",
    "        oof_results[model_type] = oof_preds\n",
    "        all_models[model_type] = models\n",
    "        \n",
    "        auc = roc_auc_score(y_trainval, oof_preds)\n",
    "        ll = log_loss(y_trainval, np.clip(oof_preds, 1e-7, 1-1e-7))\n",
    "        print(f\"  {model_type.upper()} OOF: AUC={auc:.4f}, LogLoss={ll:.4f}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # TRAIN META-LEARNER ON OOF\n",
    "    # =====================================\n",
    "    print(\"\\n[3/7] Training Neural Meta-Learner...\")\n",
    "    \n",
    "    # Create meta features\n",
    "    meta_cols = list(oof_results.keys())\n",
    "    X_meta_trainval = np.column_stack([oof_results[k] for k in meta_cols])\n",
    "    \n",
    "    # Add context features (rank diff, surface one-hot, etc.)\n",
    "    # For simplicity, we'll use base predictions only\n",
    "    \n",
    "    # Split for meta training\n",
    "    meta_split = int(len(X_meta_trainval) * 0.9)\n",
    "    X_meta_train = X_meta_trainval[:meta_split]\n",
    "    y_meta_train = y_trainval[:meta_split]\n",
    "    X_meta_val = X_meta_trainval[meta_split:]\n",
    "    y_meta_val = y_trainval[meta_split:]\n",
    "    \n",
    "    neural_meta = train_neural_meta(\n",
    "        X_meta_train, y_meta_train,\n",
    "        X_meta_val, y_meta_val,\n",
    "        epochs=100, lr=0.001, patience=10\n",
    "    )\n",
    "    \n",
    "    # OOF prediction with neural meta\n",
    "    neural_meta.eval()\n",
    "    with torch.no_grad():\n",
    "        X_meta_t = torch.tensor(X_meta_trainval, dtype=torch.float32).to(DEVICE)\n",
    "        oof_meta = neural_meta(X_meta_t).cpu().numpy()\n",
    "    \n",
    "    meta_auc = roc_auc_score(y_trainval, oof_meta)\n",
    "    meta_ll = log_loss(y_trainval, np.clip(oof_meta, 1e-7, 1-1e-7))\n",
    "    print(f\"  Neural Meta OOF: AUC={meta_auc:.4f}, LogLoss={meta_ll:.4f}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # TRAIN FINAL MODELS ON ALL DATA\n",
    "    # =====================================\n",
    "    print(\"\\n[4/7] Training final models on full train+val...\")\n",
    "    \n",
    "    final_models = {}\n",
    "    \n",
    "    for model_type in meta_cols:\n",
    "        print(f\"  Training final {model_type.upper()}...\")\n",
    "        \n",
    "        if model_type == \"lgbm\":\n",
    "            model = lgb.LGBMClassifier(\n",
    "                n_estimators=1000, learning_rate=0.05, num_leaves=31,\n",
    "                max_depth=8, min_child_samples=100, subsample=0.8,\n",
    "                colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=1.0,\n",
    "                random_state=42, verbosity=-1\n",
    "            )\n",
    "            model.fit(X_trainval, y_trainval)\n",
    "            \n",
    "        elif model_type == \"xgb\":\n",
    "            model = xgb.XGBClassifier(\n",
    "                n_estimators=1000, learning_rate=0.05, max_depth=6,\n",
    "                min_child_weight=10, subsample=0.8, colsample_bytree=0.8,\n",
    "                reg_alpha=0.1, reg_lambda=1.0, random_state=42, verbosity=0\n",
    "            )\n",
    "            model.fit(X_trainval, y_trainval)\n",
    "            \n",
    "        elif model_type == \"cat\":\n",
    "            model = CatBoostClassifier(\n",
    "                iterations=1000, learning_rate=0.05, depth=6,\n",
    "                l2_leaf_reg=3, random_seed=42, verbose=False\n",
    "            )\n",
    "            model.fit(X_trainval, y_trainval)\n",
    "            \n",
    "        elif model_type == \"tabnet\" and TABNET_AVAILABLE:\n",
    "            model = train_tabnet(X_trainval, y_trainval, X_val, y_val, seed=42)\n",
    "        \n",
    "        final_models[model_type] = model\n",
    "    \n",
    "    # =====================================\n",
    "    # TEST SET PREDICTIONS\n",
    "    # =====================================\n",
    "    print(\"\\n[5/7] Generating test predictions...\")\n",
    "    \n",
    "    test_preds = {}\n",
    "    for model_type, model in final_models.items():\n",
    "        if model is not None:\n",
    "            test_preds[model_type] = model.predict_proba(X_test)[:, 1]\n",
    "            print(f\"  {model_type.upper()}: AUC={roc_auc_score(y_test, test_preds[model_type]):.4f}\")\n",
    "    \n",
    "    # Meta prediction on test\n",
    "    X_meta_test = np.column_stack([test_preds[k] for k in meta_cols if k in test_preds])\n",
    "    \n",
    "    neural_meta.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_t = torch.tensor(X_meta_test, dtype=torch.float32).to(DEVICE)\n",
    "        test_meta = neural_meta(X_test_t).cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n  Neural Meta Test: AUC={roc_auc_score(y_test, test_meta):.4f}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # CALIBRATION\n",
    "    # =====================================\n",
    "    print(\"\\n[6/7] Calibrating predictions...\")\n",
    "    \n",
    "    # Isotonic on validation predictions\n",
    "    val_preds = {}\n",
    "    for model_type, model in final_models.items():\n",
    "        if model is not None:\n",
    "            val_preds[model_type] = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    X_meta_val_final = np.column_stack([val_preds[k] for k in meta_cols if k in val_preds])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_val_t = torch.tensor(X_meta_val_final, dtype=torch.float32).to(DEVICE)\n",
    "        val_meta = neural_meta(X_val_t).cpu().numpy()\n",
    "    \n",
    "    # Fit calibrators\n",
    "    isotonic = calibrate_predictions(val_meta, y_val, method='isotonic')\n",
    "    platt = calibrate_predictions(val_meta, y_val, method='platt')\n",
    "    \n",
    "    # Apply calibration\n",
    "    test_calibrated_iso = isotonic.transform(test_meta)\n",
    "    test_calibrated_platt = platt.predict_proba(test_meta.reshape(-1, 1))[:, 1]\n",
    "    \n",
    "    print(f\"  Isotonic Test: AUC={roc_auc_score(y_test, test_calibrated_iso):.4f}, \"\n",
    "          f\"Brier={brier_score_loss(y_test, test_calibrated_iso):.4f}\")\n",
    "    print(f\"  Platt Test: AUC={roc_auc_score(y_test, test_calibrated_platt):.4f}, \"\n",
    "          f\"Brier={brier_score_loss(y_test, test_calibrated_platt):.4f}\")\n",
    "\n",
    "    if VENN_ABERS_AVAILABLE:\n",
    "        print(\"\\n  Fitting Venn-ABERS calibration...\")\n",
    "        \n",
    "        # Fit on validation set\n",
    "        venn_abers, _, _ = calibrate_venn_abers(val_meta, y_val)\n",
    "        \n",
    "        if venn_abers is not None:\n",
    "            # Apply to test set\n",
    "            p_low, p_high = venn_abers.predict_proba(test_meta.reshape(-1))\n",
    "            \n",
    "            # Point estimate (average of bounds)\n",
    "            test_calibrated_venn = (p_low + p_high) / 2\n",
    "            \n",
    "            # Uncertainty (width of interval)\n",
    "            test_uncertainty = p_high - p_low\n",
    "            \n",
    "            # Metrics\n",
    "            venn_auc = roc_auc_score(y_test, test_calibrated_venn)\n",
    "            venn_brier = brier_score_loss(y_test, test_calibrated_venn)\n",
    "            \n",
    "            print(f\"  Venn-ABERS Test: AUC={venn_auc:.4f}, Brier={venn_brier:.4f}\")\n",
    "            print(f\"  Mean uncertainty: {test_uncertainty.mean():.4f}\")\n",
    "            \n",
    "            # Compare calibrations\n",
    "            print(\"\\n  ðŸ“Š Calibration Comparison:\")\n",
    "            print(f\"     Isotonic:   Brier={brier_score_loss(y_test, test_calibrated_iso):.4f}\")\n",
    "            print(f\"     Platt:      Brier={brier_score_loss(y_test, test_calibrated_platt):.4f}\")\n",
    "            print(f\"     Venn-ABERS: Brier={venn_brier:.4f}\")\n",
    "    # =====================================\n",
    "    # SAVE EVERYTHING\n",
    "    # =====================================\n",
    "    print(\"\\n[7/7] Saving models and results...\")\n",
    "    \n",
    "    # Final predictions\n",
    "    final_preds = test_calibrated_iso\n",
    "    final_auc = roc_auc_score(y_test, final_preds)\n",
    "    final_ll = log_loss(y_test, np.clip(final_preds, 1e-7, 1-1e-7))\n",
    "    final_brier = brier_score_loss(y_test, final_preds)\n",
    "    \n",
    "    # Save models\n",
    "    joblib.dump({\n",
    "        'lgbm': final_models.get('lgbm'),\n",
    "        'xgb': final_models.get('xgb'),\n",
    "        'cat': final_models.get('cat'),\n",
    "        'tabnet': final_models.get('tabnet'),\n",
    "        'isotonic': isotonic,\n",
    "        'platt': platt,\n",
    "        'venn_abers': venn_abers if VENN_ABERS_AVAILABLE else None, \n",
    "        'feature_cols': feature_cols,\n",
    "    }, OUTPUT_DIR / \"final_models.joblib\")\n",
    "    \n",
    "    # Save neural meta\n",
    "    torch.save(neural_meta.state_dict(), OUTPUT_DIR / \"neural_meta.pt\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics = {\n",
    "        'test_auc': final_auc,\n",
    "        'test_logloss': final_ll,\n",
    "        'test_brier': final_brier,\n",
    "        'oof_auc': meta_auc,\n",
    "        'oof_logloss': meta_ll,\n",
    "        'models': list(meta_cols),\n",
    "        'created': datetime.now().isoformat(),\n",
    "    }\n",
    "    \n",
    "    with open(OUTPUT_DIR / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    elapsed = (datetime.now() - t0).total_seconds()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   ðŸ† FINAL RESULTS (TEST SET)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n   AUC:     {final_auc:.4f}\")\n",
    "    print(f\"   LogLoss: {final_ll:.4f}\")\n",
    "    print(f\"   Brier:   {final_brier:.4f}\")\n",
    "    print(f\"\\n   â±ï¸ Time: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"   ðŸ“ Output: {OUTPUT_DIR}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   âœ… PP_16 GOD SOTA COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "ðŸ“‹ COMPOSANTS SAUVÃ‰S:\n",
    "   â€¢ final_models.joblib: LGBM, XGB, CatBoost, TabNet\n",
    "   â€¢ neural_meta.pt: Neural Meta-Learner\n",
    "   â€¢ metrics.json: RÃ©sultats finaux\n",
    "\n",
    "ðŸŽ¯ ARCHITECTURE:\n",
    "   Input Features (198)\n",
    "        â†“\n",
    "   [LGBM] [XGB] [CatBoost] [TabNet*]\n",
    "        â†“        â†“           â†“         â†“\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â†“\n",
    "              Neural Meta-Learner (MLP)\n",
    "                       â†“\n",
    "              Isotonic Calibration\n",
    "                       â†“\n",
    "              Final Probability\n",
    "\n",
    "* TabNet optionnel (nÃ©cessite pytorch-tabnet)\n",
    "\"\"\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def select_best_calibration(test_preds: np.ndarray, y_test: np.ndarray,\n",
    "                            isotonic_preds: np.ndarray,\n",
    "                            platt_preds: np.ndarray,\n",
    "                            venn_preds: np.ndarray = None) -> tuple:\n",
    "    \"\"\"\n",
    "    SÃ©lectionne automatiquement la meilleure mÃ©thode de calibration\n",
    "    basÃ©e sur le Brier Score.\n",
    "    \n",
    "    Returns:\n",
    "        best_preds: Array of best calibrated predictions\n",
    "        best_method: Name of best method\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = {\n",
    "        'isotonic': isotonic_preds,\n",
    "        'platt': platt_preds,\n",
    "    }\n",
    "    \n",
    "    if venn_preds is not None:\n",
    "        candidates['venn_abers'] = venn_preds\n",
    "    \n",
    "    brier_scores = {}\n",
    "    for name, preds in candidates.items():\n",
    "        brier_scores[name] = brier_score_loss(y_test, preds)\n",
    "    \n",
    "    best_method = min(brier_scores, key=brier_scores.get)\n",
    "    best_preds = candidates[best_method]\n",
    "    \n",
    "    print(f\"\\n  ðŸ† Best calibration: {best_method} (Brier={brier_scores[best_method]:.4f})\")\n",
    "    \n",
    "    return best_preds, best_method\n",
    "\n",
    "def get_confidence_adjusted_prediction(p_low: np.ndarray, p_high: np.ndarray,\n",
    "                                       confidence_threshold: float = 0.15) -> tuple:\n",
    "    \"\"\"\n",
    "    Retourne les prÃ©dictions avec un flag de confiance.\n",
    "    \n",
    "    Args:\n",
    "        p_low: Lower probability bounds\n",
    "        p_high: Upper probability bounds\n",
    "        confidence_threshold: Max uncertainty pour Ãªtre \"confident\"\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Point estimates\n",
    "        is_confident: Boolean mask for high-confidence predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = (p_low + p_high) / 2\n",
    "    uncertainty = p_high - p_low\n",
    "    is_confident = uncertainty <= confidence_threshold\n",
    "    \n",
    "    n_confident = is_confident.sum()\n",
    "    pct_confident = 100 * n_confident / len(is_confident)\n",
    "    \n",
    "    print(f\"  Confident predictions: {n_confident:,} ({pct_confident:.1f}%)\")\n",
    "    print(f\"  Mean uncertainty (confident): {uncertainty[is_confident].mean():.4f}\")\n",
    "    print(f\"  Mean uncertainty (uncertain): {uncertainty[~is_confident].mean():.4f}\")\n",
    "    \n",
    "    return predictions, is_confident\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    metrics = main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
