{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bb4928-afba-4f9b-9eca-2362a46fb1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   PP_14 - PLAYER EMBEDDINGS (GNN ULTIMATE GOD SOTA v2)\n",
      "======================================================================\n",
      "   2025-12-16 20:08:19\n",
      "   Device: cuda\n",
      "   Architecture: GCN + Ranking + STABILIZED\n",
      "   Learning rate: 0.0001\n",
      "======================================================================\n",
      "\n",
      "[Loading] Matches...\n",
      "  Total: 544,245\n",
      "\n",
      "[1/4] Splitting data chronologically...\n",
      "  Total: 544,245 | Train: 489,820 | Val: 54,425\n",
      "\n",
      "[2/4] Building player set...\n",
      "  Players: 6,245\n",
      "\n",
      "[3/4] Building graph (clamped weights)...\n",
      "  Edges: 925,538\n",
      "  Weight range: [0.3679, 1.0000]\n",
      "\n",
      "[4/4] Computing features...\n",
      "  Features shape: torch.Size([6245, 8])\n",
      "\n",
      "  Train labels: 925,538\n",
      "  Val labels: 85,180\n",
      "\n",
      "==================================================\n",
      "  TRAINING (STABILIZED)\n",
      "==================================================\n",
      "  Sampling: 200,000 / 925,538\n",
      "  Params: 457,089\n",
      "  Warmup epochs: 10\n",
      "  Epoch  10: loss=0.6918, acc=0.5236 | val_loss=0.6914, val_acc=0.5708, AUC=0.5955, lr=0.000100\n",
      "  Epoch  20: loss=0.6874, acc=0.5667 | val_loss=0.6877, val_acc=0.5874, AUC=0.6159, lr=0.000100\n",
      "  Epoch  30: loss=0.6810, acc=0.5877 | val_loss=0.6829, val_acc=0.5910, AUC=0.6197, lr=0.000100\n",
      "  Epoch  40: loss=0.6717, acc=0.6020 | val_loss=0.6770, val_acc=0.5933, AUC=0.6219, lr=0.000100\n",
      "  Epoch  50: loss=0.6608, acc=0.6114 | val_loss=0.6730, val_acc=0.5949, AUC=0.6226, lr=0.000100\n",
      "  Epoch  60: loss=0.6502, acc=0.6186 | val_loss=0.6712, val_acc=0.5984, AUC=0.6267, lr=0.000100\n",
      "  Epoch  70: loss=0.6419, acc=0.6218 | val_loss=0.6710, val_acc=0.6004, AUC=0.6325, lr=0.000100\n",
      "  Epoch  80: loss=0.6364, acc=0.6278 | val_loss=0.6690, val_acc=0.6029, AUC=0.6393, lr=0.000100\n",
      "  Epoch  90: loss=0.6313, acc=0.6333 | val_loss=0.6655, val_acc=0.6043, AUC=0.6452, lr=0.000100\n",
      "  Epoch 100: loss=0.6287, acc=0.6371 | val_loss=0.6638, val_acc=0.6064, AUC=0.6484, lr=0.000100\n",
      "  Epoch 110: loss=0.6245, acc=0.6429 | val_loss=0.6631, val_acc=0.6070, AUC=0.6506, lr=0.000100\n",
      "  Epoch 120: loss=0.6226, acc=0.6441 | val_loss=0.6636, val_acc=0.6072, AUC=0.6520, lr=0.000100\n",
      "  Epoch 130: loss=0.6197, acc=0.6484 | val_loss=0.6629, val_acc=0.6079, AUC=0.6537, lr=0.000100\n",
      "  Epoch 140: loss=0.6183, acc=0.6508 | val_loss=0.6631, val_acc=0.6085, AUC=0.6545, lr=0.000100\n",
      "  Epoch 150: loss=0.6173, acc=0.6510 | val_loss=0.6663, val_acc=0.6087, AUC=0.6545, lr=0.000100\n",
      "  Epoch 160: loss=0.6161, acc=0.6523 | val_loss=0.6663, val_acc=0.6089, AUC=0.6552, lr=0.000100\n",
      "  Epoch 170: loss=0.6140, acc=0.6540 | val_loss=0.6673, val_acc=0.6089, AUC=0.6562, lr=0.000100\n",
      "  Epoch 180: loss=0.6129, acc=0.6560 | val_loss=0.6684, val_acc=0.6092, AUC=0.6567, lr=0.000100\n",
      "  Epoch 190: loss=0.6115, acc=0.6582 | val_loss=0.6680, val_acc=0.6099, AUC=0.6580, lr=0.000100\n",
      "  Epoch 200: loss=0.6114, acc=0.6572 | val_loss=0.6680, val_acc=0.6108, AUC=0.6589, lr=0.000100\n",
      "\n",
      "  ‚úÖ Best val AUC: 0.6586\n",
      "\n",
      "[Embeddings] Extracting...\n",
      "  Players: 6,245\n",
      "\n",
      "[Features] Creating match-level features...\n",
      "  Coverage: 92.9%\n",
      "\n",
      "  ‚úÖ Saved: C:\\Users\\Administrateur\\Tennis POLAR v2\\data_clean\\features\\player_embeddings\\embedding_features.parquet\n",
      "\n",
      "======================================================================\n",
      "   ‚úÖ PP_14 GNN ULTIMATE GOD SOTA v2 COMPLETE!\n",
      "======================================================================\n",
      "   ‚è±Ô∏è  Time: 50.7s\n",
      "   üìä Players: 6,245\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ===============================================\n",
    "# PP_14 - PLAYER EMBEDDINGS (GNN ULTIMATE GOD SOTA v2)\n",
    "# VERSION CORRIG√âE - Fix NaN + Stabilit√©\n",
    "# ===============================================\n",
    "#\n",
    "# CORRECTIONS v2:\n",
    "# ‚úÖ Learning rate r√©duit (0.0001)\n",
    "# ‚úÖ Edge weights clamp√©s (min 1e-4)\n",
    "# ‚úÖ NaN detection + handling\n",
    "# ‚úÖ Gradient clipping plus strict\n",
    "# ‚úÖ Warmup learning rate\n",
    "# ‚úÖ Initialization plus conservatrice\n",
    "#\n",
    "# Output: features/player_embeddings/\n",
    "# ===============================================\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURATION\n",
    "# ===============================================\n",
    "ROOT = Path(r\"C:\\Users\\Administrateur\\Tennis POLAR v2\")\n",
    "DATA_CLEAN = ROOT / \"data_clean\"\n",
    "MATCHES_BASE = DATA_CLEAN / \"matches_base\"\n",
    "OUTPUT_DIR = DATA_CLEAN / \"features\" / \"player_embeddings\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Training parameters - AJUST√âS POUR STABILIT√â\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001      # ‚úÖ R√©duit de 0.001 √† 0.0001\n",
    "SAMPLES_PER_EPOCH = 200_000  # ‚úÖ R√©duit pour plus de stabilit√©\n",
    "WEIGHT_DECAY = 1e-5\n",
    "PATIENCE = 40               # ‚úÖ Plus de patience\n",
    "WARMUP_EPOCHS = 10          # ‚úÖ Warmup\n",
    "\n",
    "# Temporal split\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"   PP_14 - PLAYER EMBEDDINGS (GNN ULTIMATE GOD SOTA v2)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Architecture: GCN + Ranking + STABILIZED\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# GCN MODEL (avec initialisation conservatrice)\n",
    "# ===============================================\n",
    "\n",
    "class PlayerGCN(nn.Module):\n",
    "    def __init__(self, num_players: int, in_channels: int, hidden_channels: int, \n",
    "                 out_channels: int, num_layers: int = 2, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # ‚úÖ Initialization plus petite\n",
    "        self.player_emb = nn.Embedding(num_players, hidden_channels // 2)\n",
    "        nn.init.normal_(self.player_emb.weight, mean=0, std=0.01)  # Petit std\n",
    "        \n",
    "        self.input_proj = nn.Linear(in_channels + hidden_channels // 2, hidden_channels)\n",
    "        nn.init.xavier_uniform_(self.input_proj.weight, gain=0.5)\n",
    "        nn.init.zeros_(self.input_proj.bias)\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = GCNConv(hidden_channels, hidden_channels)\n",
    "            self.convs.append(conv)\n",
    "            self.norms.append(nn.LayerNorm(hidden_channels))\n",
    "        \n",
    "        self.output_proj = nn.Linear(hidden_channels, out_channels)\n",
    "        nn.init.xavier_uniform_(self.output_proj.weight, gain=0.5)\n",
    "        nn.init.zeros_(self.output_proj.bias)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight, player_indices):\n",
    "        player_emb = self.player_emb(player_indices)\n",
    "        x = torch.cat([x, player_emb], dim=-1)\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x_res = x\n",
    "            x = conv(x, edge_index, edge_weight)\n",
    "            x = norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = x + x_res\n",
    "        \n",
    "        out = self.output_proj(x)\n",
    "        \n",
    "        # ‚úÖ Clamp output pour √©viter valeurs extr√™mes\n",
    "        out = torch.clamp(out, -10, 10)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class RankingPredictor(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.score_net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        # ‚úÖ Initialization conservatrice\n",
    "        for m in self.score_net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.5)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, emb_a, emb_b):\n",
    "        score_a = self.score_net(emb_a).squeeze(-1)\n",
    "        score_b = self.score_net(emb_b).squeeze(-1)\n",
    "        logit = score_a - score_b\n",
    "        # ‚úÖ Clamp logits pour stabilit√©\n",
    "        return torch.clamp(logit, -20, 20)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# DATA PREPARATION\n",
    "# ===============================================\n",
    "\n",
    "def prepare_data_zero_leakage(matches_df: pl.DataFrame, min_matches: int = 10):\n",
    "    print(\"\\n[1/4] Splitting data chronologically...\")\n",
    "    \n",
    "    matches_sorted = matches_df.sort(\"tourney_date_ta\")\n",
    "    matches_list = [\n",
    "        m for m in matches_sorted.to_dicts()\n",
    "        if m[\"winner_id\"] and m[\"loser_id\"] and m[\"tourney_date_ta\"]\n",
    "    ]\n",
    "    \n",
    "    split_idx = int(len(matches_list) * TRAIN_RATIO)\n",
    "    train_matches_raw = matches_list[:split_idx]\n",
    "    val_matches_raw = matches_list[split_idx:]\n",
    "    \n",
    "    print(f\"  Total: {len(matches_list):,} | Train: {len(train_matches_raw):,} | Val: {len(val_matches_raw):,}\")\n",
    "    \n",
    "    # ===== PLAYERS FROM TRAIN =====\n",
    "    print(\"\\n[2/4] Building player set...\")\n",
    "    \n",
    "    player_count = defaultdict(int)\n",
    "    for m in train_matches_raw:\n",
    "        player_count[m[\"winner_id\"]] += 1\n",
    "        player_count[m[\"loser_id\"]] += 1\n",
    "    \n",
    "    valid_players = {p for p, c in player_count.items() if c >= min_matches}\n",
    "    player_to_idx = {p: i for i, p in enumerate(sorted(valid_players))}\n",
    "    idx_to_player = {i: p for p, i in player_to_idx.items()}\n",
    "    num_players = len(valid_players)\n",
    "    \n",
    "    print(f\"  Players: {num_players:,}\")\n",
    "    \n",
    "    # ===== GRAPH WITH CLAMPED WEIGHTS =====\n",
    "    print(\"\\n[3/4] Building graph (clamped weights)...\")\n",
    "    \n",
    "    edge_list = []\n",
    "    edge_weights = []\n",
    "    max_train_date = max(m[\"tourney_date_ta\"] for m in train_matches_raw)\n",
    "    \n",
    "    for m in train_matches_raw:\n",
    "        w_id, l_id = m[\"winner_id\"], m[\"loser_id\"]\n",
    "        if w_id not in valid_players or l_id not in valid_players:\n",
    "            continue\n",
    "        \n",
    "        w_idx, l_idx = player_to_idx[w_id], player_to_idx[l_id]\n",
    "        \n",
    "        days_ago = (max_train_date - m[\"tourney_date_ta\"]).days\n",
    "        # ‚úÖ Clamp pour √©viter exp(-inf) et valeurs trop petites\n",
    "        days_ago = min(days_ago, 3650)  # Max 10 ans\n",
    "        weight = np.exp(-days_ago / 3650)\n",
    "        weight = max(weight, 1e-3)  # ‚úÖ Minimum weight\n",
    "        \n",
    "        edge_list.extend([[w_idx, l_idx], [l_idx, w_idx]])\n",
    "        edge_weights.extend([weight, weight])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "    edge_weight = torch.tensor(edge_weights, dtype=torch.float)\n",
    "    \n",
    "    # ‚úÖ Normalize edge weights\n",
    "    edge_weight = edge_weight / edge_weight.max()\n",
    "    \n",
    "    print(f\"  Edges: {edge_index.shape[1]:,}\")\n",
    "    print(f\"  Weight range: [{edge_weight.min():.4f}, {edge_weight.max():.4f}]\")\n",
    "    \n",
    "    # ===== FEATURES =====\n",
    "    print(\"\\n[4/4] Computing features...\")\n",
    "    \n",
    "    player_stats = defaultdict(lambda: {\n",
    "        \"wins\": 0, \"losses\": 0, \"ace_rate\": [], \"first_won\": [],\n",
    "        \"surfaces\": defaultdict(int)\n",
    "    })\n",
    "    \n",
    "    for m in train_matches_raw:\n",
    "        w_id, l_id = m[\"winner_id\"], m[\"loser_id\"]\n",
    "        surface = m.get(\"tourney_surface_ta\") or m.get(\"surface\", \"Hard\")\n",
    "        \n",
    "        for pid, is_winner in [(w_id, True), (l_id, False)]:\n",
    "            if pid not in valid_players:\n",
    "                continue\n",
    "            stats = player_stats[pid]\n",
    "            stats[\"wins\" if is_winner else \"losses\"] += 1\n",
    "            stats[\"surfaces\"][surface] += 1\n",
    "            \n",
    "            prefix = \"w_\" if is_winner else \"l_\"\n",
    "            if m.get(f\"{prefix}s_ace_p\") is not None:\n",
    "                stats[\"ace_rate\"].append(m[f\"{prefix}s_ace_p\"])\n",
    "            if m.get(f\"{prefix}s_1stWon_p\") is not None:\n",
    "                stats[\"first_won\"].append(m[f\"{prefix}s_1stWon_p\"])\n",
    "    \n",
    "    X = np.zeros((num_players, 8), dtype=np.float32)\n",
    "    for pid, idx in player_to_idx.items():\n",
    "        s = player_stats[pid]\n",
    "        total = s[\"wins\"] + s[\"losses\"]\n",
    "        if total > 0:\n",
    "            X[idx, 0] = s[\"wins\"] / total\n",
    "            X[idx, 1] = np.log1p(total)\n",
    "            X[idx, 2] = np.mean(s[\"ace_rate\"]) if s[\"ace_rate\"] else 0.05\n",
    "            X[idx, 3] = np.mean(s[\"first_won\"]) if s[\"first_won\"] else 0.65\n",
    "            X[idx, 4] = s[\"surfaces\"].get(\"Hard\", 0) / total\n",
    "            X[idx, 5] = s[\"surfaces\"].get(\"Clay\", 0) / total\n",
    "            X[idx, 6] = s[\"surfaces\"].get(\"Grass\", 0) / total\n",
    "            X[idx, 7] = s[\"surfaces\"].get(\"Carpet\", 0) / total\n",
    "    \n",
    "    # ‚úÖ Robust normalization\n",
    "    for i in range(X.shape[1]):\n",
    "        col = X[:, i]\n",
    "        std = col.std()\n",
    "        if std > 1e-6:\n",
    "            X[:, i] = (col - col.mean()) / std\n",
    "        else:\n",
    "            X[:, i] = 0  # Constant column\n",
    "    \n",
    "    # ‚úÖ Clip extreme values\n",
    "    X = np.clip(X, -5, 5)\n",
    "    \n",
    "    node_features = torch.tensor(X, dtype=torch.float)\n",
    "    print(f\"  Features shape: {node_features.shape}\")\n",
    "    \n",
    "    # ===== LABELS =====\n",
    "    def make_label_arrays(matches):\n",
    "        labels_list = []\n",
    "        for m in matches:\n",
    "            w_id, l_id = m[\"winner_id\"], m[\"loser_id\"]\n",
    "            if w_id in valid_players and l_id in valid_players:\n",
    "                w_idx, l_idx = player_to_idx[w_id], player_to_idx[l_id]\n",
    "                labels_list.append((w_idx, l_idx, 1))\n",
    "                labels_list.append((l_idx, w_idx, 0))\n",
    "        \n",
    "        n = len(labels_list)\n",
    "        if n == 0:\n",
    "            return np.array([], dtype=np.int64), np.array([], dtype=np.int64), np.array([], dtype=np.int64)\n",
    "        \n",
    "        arr_a = np.fromiter((x[0] for x in labels_list), dtype=np.int64, count=n)\n",
    "        arr_b = np.fromiter((x[1] for x in labels_list), dtype=np.int64, count=n)\n",
    "        arr_y = np.fromiter((x[2] for x in labels_list), dtype=np.int64, count=n)\n",
    "        \n",
    "        return arr_a, arr_b, arr_y\n",
    "    \n",
    "    train_a, train_b, train_y = make_label_arrays(train_matches_raw)\n",
    "    val_a, val_b, val_y = make_label_arrays(val_matches_raw)\n",
    "    \n",
    "    print(f\"\\n  Train labels: {len(train_a):,}\")\n",
    "    print(f\"  Val labels: {len(val_a):,}\")\n",
    "    \n",
    "    return {\n",
    "        \"edge_index\": edge_index,\n",
    "        \"edge_weight\": edge_weight,\n",
    "        \"node_features\": node_features,\n",
    "        \"player_to_idx\": player_to_idx,\n",
    "        \"idx_to_player\": idx_to_player,\n",
    "        \"num_players\": num_players,\n",
    "        \"train_a\": train_a, \"train_b\": train_b, \"train_y\": train_y,\n",
    "        \"val_a\": val_a, \"val_b\": val_b, \"val_y\": val_y,\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# TRAINING (STABILIZED)\n",
    "# ===============================================\n",
    "\n",
    "def compute_auc(y_true, y_proba):\n",
    "    if HAS_SKLEARN:\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_proba)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return 0.5\n",
    "\n",
    "\n",
    "def train_gnn(data):\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"  TRAINING (STABILIZED)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    edge_index = data[\"edge_index\"].to(DEVICE)\n",
    "    edge_weight = data[\"edge_weight\"].to(DEVICE)\n",
    "    node_features = data[\"node_features\"].to(DEVICE)\n",
    "    num_players = data[\"num_players\"]\n",
    "    player_indices = torch.arange(num_players, device=DEVICE)\n",
    "    \n",
    "    train_a, train_b, train_y = data[\"train_a\"], data[\"train_b\"], data[\"train_y\"]\n",
    "    val_a, val_b, val_y = data[\"val_a\"], data[\"val_b\"], data[\"val_y\"]\n",
    "    \n",
    "    n_train = len(train_a)\n",
    "    n_val = len(val_a)\n",
    "    \n",
    "    samples_per_epoch = min(SAMPLES_PER_EPOCH, n_train)\n",
    "    replace = samples_per_epoch >= n_train\n",
    "    \n",
    "    print(f\"  Sampling: {samples_per_epoch:,} / {n_train:,}\")\n",
    "    \n",
    "    # Models\n",
    "    gcn = PlayerGCN(\n",
    "        num_players=num_players,\n",
    "        in_channels=node_features.shape[1],\n",
    "        hidden_channels=HIDDEN_DIM,\n",
    "        out_channels=EMBEDDING_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    predictor = RankingPredictor(EMBEDDING_DIM).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    params = list(gcn.parameters()) + list(predictor.parameters())\n",
    "    \n",
    "    # ‚úÖ Optimizer avec LR plus bas\n",
    "    optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # ‚úÖ Scheduler avec warmup\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            return (epoch + 1) / WARMUP_EPOCHS\n",
    "        return 1.0\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    print(f\"  Params: {sum(p.numel() for p in params):,}\")\n",
    "    print(f\"  Warmup epochs: {WARMUP_EPOCHS}\")\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    nan_count = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # ===== TRAINING =====\n",
    "        gcn.train()\n",
    "        predictor.train()\n",
    "        \n",
    "        sample_idx = np.random.choice(n_train, size=samples_per_epoch, replace=replace)\n",
    "        \n",
    "        batch_a = torch.tensor(train_a[sample_idx], dtype=torch.long, device=DEVICE)\n",
    "        batch_b = torch.tensor(train_b[sample_idx], dtype=torch.long, device=DEVICE)\n",
    "        batch_y = torch.tensor(train_y[sample_idx], dtype=torch.float, device=DEVICE)\n",
    "        \n",
    "        embeddings = gcn(node_features, edge_index, edge_weight, player_indices)\n",
    "        \n",
    "        # ‚úÖ Check for NaN in embeddings\n",
    "        if torch.isnan(embeddings).any():\n",
    "            nan_count += 1\n",
    "            if nan_count > 5:\n",
    "                print(f\"  ‚ö†Ô∏è Too many NaN embeddings, stopping training\")\n",
    "                break\n",
    "            print(f\"  ‚ö†Ô∏è NaN in embeddings at epoch {epoch+1}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        emb_a = embeddings[batch_a]\n",
    "        emb_b = embeddings[batch_b]\n",
    "        \n",
    "        logits = predictor(emb_a, emb_b)\n",
    "        \n",
    "        # ‚úÖ Check for NaN in logits\n",
    "        if torch.isnan(logits).any():\n",
    "            nan_count += 1\n",
    "            if nan_count > 5:\n",
    "                print(f\"  ‚ö†Ô∏è Too many NaN logits, stopping training\")\n",
    "                break\n",
    "            print(f\"  ‚ö†Ô∏è NaN in logits at epoch {epoch+1}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(logits, batch_y)\n",
    "        \n",
    "        # ‚úÖ Check for NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            nan_count += 1\n",
    "            if nan_count > 5:\n",
    "                print(f\"  ‚ö†Ô∏è Too many NaN losses, stopping training\")\n",
    "                break\n",
    "            print(f\"  ‚ö†Ô∏è NaN loss at epoch {epoch+1}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # ‚úÖ Gradient clipping plus strict\n",
    "        torch.nn.utils.clip_grad_norm_(params, 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "        train_preds = (logits > 0).float()\n",
    "        train_acc = (train_preds == batch_y).float().mean().item()\n",
    "        \n",
    "        # ===== VALIDATION =====\n",
    "        gcn.eval()\n",
    "        predictor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embeddings = gcn(node_features, edge_index, edge_weight, player_indices)\n",
    "            \n",
    "            if torch.isnan(embeddings).any():\n",
    "                print(f\"  ‚ö†Ô∏è NaN in val embeddings at epoch {epoch+1}\")\n",
    "                continue\n",
    "            \n",
    "            all_logits = []\n",
    "            chunk_size = 50_000\n",
    "            \n",
    "            for i in range(0, n_val, chunk_size):\n",
    "                va = torch.tensor(val_a[i:i+chunk_size], dtype=torch.long, device=DEVICE)\n",
    "                vb = torch.tensor(val_b[i:i+chunk_size], dtype=torch.long, device=DEVICE)\n",
    "                \n",
    "                logits_chunk = predictor(embeddings[va], embeddings[vb])\n",
    "                all_logits.append(logits_chunk.cpu())\n",
    "            \n",
    "            all_logits = torch.cat(all_logits)\n",
    "            \n",
    "            if torch.isnan(all_logits).any():\n",
    "                print(f\"  ‚ö†Ô∏è NaN in val logits at epoch {epoch+1}\")\n",
    "                continue\n",
    "            \n",
    "            val_y_tensor = torch.tensor(val_y, dtype=torch.float)\n",
    "            \n",
    "            val_loss = criterion(all_logits, val_y_tensor).item()\n",
    "            val_preds = (all_logits > 0).float()\n",
    "            val_acc = (val_preds == val_y_tensor).float().mean().item()\n",
    "            \n",
    "            val_proba = torch.sigmoid(all_logits).numpy()\n",
    "            val_auc = compute_auc(val_y, val_proba)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"  Epoch {epoch+1:3d}: loss={train_loss:.4f}, acc={train_acc:.4f} | val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, AUC={val_auc:.4f}, lr={lr:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_auc > best_val_auc + 0.001:  # Am√©lioration significative\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            best_state = {'gcn': gcn.state_dict(), 'predictor': predictor.state_dict()}\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        gcn.load_state_dict(best_state['gcn'])\n",
    "        predictor.load_state_dict(best_state['predictor'])\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Best val AUC: {best_val_auc:.4f}\")\n",
    "    \n",
    "    return gcn, predictor\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# EXTRACT EMBEDDINGS\n",
    "# ===============================================\n",
    "\n",
    "def extract_and_create_features(gcn, data, matches_df):\n",
    "    print(\"\\n[Embeddings] Extracting...\")\n",
    "    \n",
    "    gcn.eval()\n",
    "    \n",
    "    edge_index = data[\"edge_index\"].to(DEVICE)\n",
    "    edge_weight = data[\"edge_weight\"].to(DEVICE)\n",
    "    node_features = data[\"node_features\"].to(DEVICE)\n",
    "    num_players = data[\"num_players\"]\n",
    "    idx_to_player = data[\"idx_to_player\"]\n",
    "    player_indices = torch.arange(num_players, device=DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = gcn(node_features, edge_index, edge_weight, player_indices)\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "    \n",
    "    # ‚úÖ Replace NaN with zeros\n",
    "    embeddings_np = np.nan_to_num(embeddings_np, nan=0.0)\n",
    "    \n",
    "    player_embeddings = {idx_to_player[i]: embeddings_np[i] for i in range(num_players)}\n",
    "    print(f\"  Players: {len(player_embeddings):,}\")\n",
    "    \n",
    "    print(\"\\n[Features] Creating match-level features...\")\n",
    "    \n",
    "    results = []\n",
    "    for row in matches_df.iter_rows(named=True):\n",
    "        result = {\"custom_match_id\": row[\"custom_match_id\"]}\n",
    "        \n",
    "        emb_w = player_embeddings.get(row[\"winner_id\"])\n",
    "        emb_l = player_embeddings.get(row[\"loser_id\"])\n",
    "        \n",
    "        if emb_w is not None and emb_l is not None:\n",
    "            norm_w = np.linalg.norm(emb_w)\n",
    "            norm_l = np.linalg.norm(emb_l)\n",
    "            \n",
    "            if norm_w > 1e-6 and norm_l > 1e-6:\n",
    "                result[\"emb_cosine_sim\"] = float(np.dot(emb_w, emb_l) / (norm_w * norm_l))\n",
    "            else:\n",
    "                result[\"emb_cosine_sim\"] = 0.0\n",
    "            \n",
    "            result[\"emb_l2_distance\"] = float(np.linalg.norm(emb_w - emb_l))\n",
    "            \n",
    "            diff = emb_w - emb_l\n",
    "            for i in range(min(8, EMBEDDING_DIM)):\n",
    "                result[f\"emb_diff_{i}\"] = float(diff[i])\n",
    "            \n",
    "            result[\"emb_norm_winner\"] = float(norm_w)\n",
    "            result[\"emb_norm_loser\"] = float(norm_l)\n",
    "            result[\"has_embeddings\"] = 1\n",
    "        else:\n",
    "            result[\"emb_cosine_sim\"] = None\n",
    "            result[\"emb_l2_distance\"] = None\n",
    "            for i in range(min(8, EMBEDDING_DIM)):\n",
    "                result[f\"emb_diff_{i}\"] = None\n",
    "            result[\"emb_norm_winner\"] = None\n",
    "            result[\"emb_norm_loser\"] = None\n",
    "            result[\"has_embeddings\"] = 0\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    features_df = pl.DataFrame(results, infer_schema_length=None)\n",
    "    print(f\"  Coverage: {features_df['has_embeddings'].mean():.1%}\")\n",
    "    \n",
    "    return player_embeddings, features_df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN\n",
    "# ===============================================\n",
    "\n",
    "def main():\n",
    "    t0 = datetime.now()\n",
    "    \n",
    "    print(\"\\n[Loading] Matches...\")\n",
    "    matches_df = pl.read_parquet(MATCHES_BASE)\n",
    "    print(f\"  Total: {len(matches_df):,}\")\n",
    "    \n",
    "    data = prepare_data_zero_leakage(matches_df, min_matches=10)\n",
    "    \n",
    "    gcn, predictor = train_gnn(data)\n",
    "    \n",
    "    player_embeddings, features_df = extract_and_create_features(gcn, data, matches_df)\n",
    "    \n",
    "    # Save\n",
    "    features_df.write_parquet(OUTPUT_DIR / \"embedding_features.parquet\")\n",
    "    print(f\"\\n  ‚úÖ Saved: {OUTPUT_DIR / 'embedding_features.parquet'}\")\n",
    "    \n",
    "    np.save(OUTPUT_DIR / \"player_embeddings.npy\", \n",
    "            np.array([player_embeddings[p] for p in sorted(player_embeddings.keys())]))\n",
    "    \n",
    "    import json\n",
    "    with open(OUTPUT_DIR / \"player_mapping.json\", \"w\") as f:\n",
    "        json.dump({p: i for i, p in enumerate(sorted(player_embeddings.keys()))}, f)\n",
    "    \n",
    "    torch.save({\n",
    "        'gcn': gcn.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'config': {'embedding_dim': EMBEDDING_DIM, 'hidden_dim': HIDDEN_DIM, 'num_layers': NUM_LAYERS}\n",
    "    }, OUTPUT_DIR / \"gnn_model.pt\")\n",
    "    \n",
    "    elapsed = (datetime.now() - t0).total_seconds()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   ‚úÖ PP_14 GNN ULTIMATE GOD SOTA v2 COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   ‚è±Ô∏è  Time: {elapsed:.1f}s\")\n",
    "    print(f\"   üìä Players: {len(player_embeddings):,}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab186cc-fa0e-4b28-b15e-823e117e8c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
