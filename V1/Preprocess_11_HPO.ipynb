{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6d6f6f-7486-40fb-91b7-ea268772184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "   ðŸš€ HPO OPTUNA - GOD MODE SOTA 2026 ðŸš€\n",
      "   TennisTitan - Ultimate Hyperparameter Optimization\n",
      "======================================================================\n",
      "   2025-12-06 09:32:59\n",
      "   Trials: 50 per model\n",
      "   CV: 5-fold Purged Rolling\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "   LOAD DATA\n",
      "======================================================================\n",
      "\n",
      "  Train: (388973, 209)\n",
      "  Val: (64787, 209)\n",
      "  Test (holdout): (89706, 209)\n",
      "  Combined for CV: (453760, 209)\n",
      "  Features: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 09:33:00,741] A new study created in memory with name: lgbm_hpo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "   HYPERPARAMETER OPTIMIZATION - GOD MODE\n",
      "======================================================================\n",
      "\n",
      "  CV: 5-fold Purged Rolling CV\n",
      "  Trials per model: 50\n",
      "  Monotonic constraints: 6/200 features\n",
      "\n",
      "--------------------------------------------------\n",
      "  ðŸŒ² LightGBM HPO\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d64656696d47c499aa9d17cae0f23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 09:35:07,145] Trial 0 finished with value: 0.5389224644516974 and parameters: {'learning_rate': 0.015355286838886862, 'num_leaves': 123, 'max_depth': 10, 'min_data_in_leaf': 1238, 'min_sum_hessian_in_leaf': 16.445845403801215, 'lambda_l1': 2.5348407664333426e-07, 'lambda_l2': 3.809220577048033e-08, 'feature_fraction': 0.8763968801762143, 'bagging_fraction': 0.770501755284444, 'bagging_freq': 5, 'max_bin': 66, 'boosting_type': 'gbdt'}. Best is trial 0 with value: 0.5389224644516974.\n",
      "[I 2025-12-06 09:43:03,501] Trial 1 finished with value: 0.557849386818534 and parameters: {'learning_rate': 0.009445600138094694, 'num_leaves': 36, 'max_depth': 5, 'min_data_in_leaf': 678, 'min_sum_hessian_in_leaf': 52.950886731591545, 'lambda_l1': 7.71800699380605e-05, 'lambda_l2': 8.171304639059403e-06, 'feature_fraction': 0.7365190920973087, 'bagging_fraction': 0.5627722372934189, 'bagging_freq': 3, 'max_bin': 133, 'boosting_type': 'dart', 'drop_rate': 0.06790539682592432, 'skip_drop': 0.5056937753654446}. Best is trial 0 with value: 0.5389224644516974.\n",
      "[I 2025-12-06 09:44:32,130] Trial 2 finished with value: 0.5386327435838376 and parameters: {'learning_rate': 0.029493012052163467, 'num_leaves': 21, 'max_depth': 9, 'min_data_in_leaf': 424, 'min_sum_hessian_in_leaf': 7.4401077055426725, 'lambda_l1': 3.4671276804481113, 'lambda_l2': 45.32317553128077, 'feature_fraction': 0.8446185414640536, 'bagging_fraction': 0.6370761961280168, 'bagging_freq': 1, 'max_bin': 195, 'boosting_type': 'gbdt'}. Best is trial 2 with value: 0.5386327435838376.\n",
      "[I 2025-12-06 09:45:51,138] Trial 3 finished with value: 0.5404102984213757 and parameters: {'learning_rate': 0.022039920190846215, 'num_leaves': 19, 'max_depth': 12, 'min_data_in_leaf': 591, 'min_sum_hessian_in_leaf': 66.58970615104421, 'lambda_l1': 6.388511557344611e-06, 'lambda_l2': 0.0015873774692781828, 'feature_fraction': 0.7006906536388038, 'bagging_fraction': 0.5831845049864872, 'bagging_freq': 7, 'max_bin': 212, 'boosting_type': 'gbdt'}. Best is trial 2 with value: 0.5386327435838376.\n",
      "[I 2025-12-06 09:46:54,173] Trial 4 finished with value: 0.5407704969235048 and parameters: {'learning_rate': 0.0299816694120633, 'num_leaves': 120, 'max_depth': 4, 'min_data_in_leaf': 472, 'min_sum_hessian_in_leaf': 5.477501602143269, 'lambda_l1': 8.471746987003668e-06, 'lambda_l2': 7.705004503489671e-05, 'feature_fraction': 0.5492419674756428, 'bagging_fraction': 0.8729318791183682, 'bagging_freq': 3, 'max_bin': 117, 'boosting_type': 'gbdt'}. Best is trial 2 with value: 0.5386327435838376.\n",
      "[I 2025-12-06 09:47:56,952] Trial 5 finished with value: 0.5394024557422006 and parameters: {'learning_rate': 0.055290731882815025, 'num_leaves': 24, 'max_depth': 12, 'min_data_in_leaf': 1568, 'min_sum_hessian_in_leaf': 20.67285247188307, 'lambda_l1': 1.1212412169964432e-08, 'lambda_l2': 1.427625460232564, 'feature_fraction': 0.7887715391161894, 'bagging_fraction': 0.8280532256184443, 'bagging_freq': 6, 'max_bin': 77, 'boosting_type': 'gbdt'}. Best is trial 2 with value: 0.5386327435838376.\n",
      "[I 2025-12-06 09:56:50,162] Trial 6 finished with value: 0.5359502183106121 and parameters: {'learning_rate': 0.06635802485202448, 'num_leaves': 86, 'max_depth': 6, 'min_data_in_leaf': 220, 'min_sum_hessian_in_leaf': 31.787249849850557, 'lambda_l1': 8.445977074223802e-06, 'lambda_l2': 0.19772509067672703, 'feature_fraction': 0.7506566092453673, 'bagging_fraction': 0.8992457341593469, 'bagging_freq': 4, 'max_bin': 86, 'boosting_type': 'dart', 'drop_rate': 0.1727703872951539, 'skip_drop': 0.6083868719818244}. Best is trial 6 with value: 0.5359502183106121.\n",
      "[I 2025-12-06 10:07:47,701] Trial 7 pruned. \n",
      "[I 2025-12-06 10:09:16,614] Trial 8 pruned. \n",
      "[I 2025-12-06 10:10:13,982] Trial 9 pruned. \n",
      "[I 2025-12-06 10:17:13,266] Trial 10 finished with value: 0.5370111130884497 and parameters: {'learning_rate': 0.09384358565252612, 'num_leaves': 91, 'max_depth': 6, 'min_data_in_leaf': 969, 'min_sum_hessian_in_leaf': 33.83013393031007, 'lambda_l1': 0.016301353379407614, 'lambda_l2': 0.201218984116414, 'feature_fraction': 0.4360661619180793, 'bagging_fraction': 0.9364055772801819, 'bagging_freq': 3, 'max_bin': 169, 'boosting_type': 'dart', 'drop_rate': 0.27553501152587356, 'skip_drop': 0.6958013712283888}. Best is trial 6 with value: 0.5359502183106121.\n",
      "[I 2025-12-06 10:23:59,285] Trial 11 finished with value: 0.5371559095409418 and parameters: {'learning_rate': 0.09814764260283956, 'num_leaves': 90, 'max_depth': 6, 'min_data_in_leaf': 1009, 'min_sum_hessian_in_leaf': 30.66782084454961, 'lambda_l1': 0.012040537297683125, 'lambda_l2': 0.22609380993035763, 'feature_fraction': 0.4069979608949408, 'bagging_fraction': 0.9486261387508514, 'bagging_freq': 3, 'max_bin': 167, 'boosting_type': 'dart', 'drop_rate': 0.2940381606913446, 'skip_drop': 0.6984066126989742}. Best is trial 6 with value: 0.5359502183106121.\n",
      "[I 2025-12-06 10:29:41,248] Trial 12 finished with value: 0.537827341594793 and parameters: {'learning_rate': 0.09716821483786295, 'num_leaves': 97, 'max_depth': 5, 'min_data_in_leaf': 984, 'min_sum_hessian_in_leaf': 37.641678677828786, 'lambda_l1': 0.00934669040346174, 'lambda_l2': 84.28956173853048, 'feature_fraction': 0.9396146406721686, 'bagging_fraction': 0.9437249420334898, 'bagging_freq': 4, 'max_bin': 248, 'boosting_type': 'dart', 'drop_rate': 0.2450816140166192, 'skip_drop': 0.6878723071861771}. Best is trial 6 with value: 0.5359502183106121.\n",
      "[I 2025-12-06 10:41:58,325] Trial 13 finished with value: 0.5362085246519197 and parameters: {'learning_rate': 0.056199935053303236, 'num_leaves': 61, 'max_depth': 8, 'min_data_in_leaf': 212, 'min_sum_hessian_in_leaf': 39.076553760581355, 'lambda_l1': 0.006679224651970214, 'lambda_l2': 0.04218625610374644, 'feature_fraction': 0.4083695876942157, 'bagging_fraction': 0.8236516328387856, 'bagging_freq': 2, 'max_bin': 155, 'boosting_type': 'dart', 'drop_rate': 0.19490469560693752, 'skip_drop': 0.5915317267211389}. Best is trial 6 with value: 0.5359502183106121.\n",
      "[I 2025-12-06 10:56:07,890] Trial 14 finished with value: 0.5354956377681526 and parameters: {'learning_rate': 0.05057711827364414, 'num_leaves': 60, 'max_depth': 9, 'min_data_in_leaf': 126, 'min_sum_hessian_in_leaf': 47.80311754357045, 'lambda_l1': 0.0006023339664742038, 'lambda_l2': 0.015903366876400756, 'feature_fraction': 0.6270593008727149, 'bagging_fraction': 0.8392310812473561, 'bagging_freq': 2, 'max_bin': 88, 'boosting_type': 'dart', 'drop_rate': 0.16969686657746064, 'skip_drop': 0.550967930645987}. Best is trial 14 with value: 0.5354956377681526.\n",
      "[I 2025-12-06 11:03:40,639] Trial 15 pruned. \n",
      "[I 2025-12-06 11:15:00,254] Trial 16 pruned. \n",
      "[I 2025-12-06 11:27:52,140] Trial 17 finished with value: 0.5384450895333219 and parameters: {'learning_rate': 0.061294927937754956, 'num_leaves': 103, 'max_depth': 8, 'min_data_in_leaf': 348, 'min_sum_hessian_in_leaf': 51.34822718039493, 'lambda_l1': 0.001033568355687213, 'lambda_l2': 9.246834718734279e-07, 'feature_fraction': 0.6109160277236141, 'bagging_fraction': 0.6717422352190162, 'bagging_freq': 2, 'max_bin': 99, 'boosting_type': 'dart', 'drop_rate': 0.19028634013541088, 'skip_drop': 0.5927596611981404}. Best is trial 14 with value: 0.5354956377681526.\n",
      "[I 2025-12-06 11:33:07,434] Trial 18 pruned. \n",
      "[I 2025-12-06 11:38:31,274] Trial 19 pruned. \n",
      "[I 2025-12-06 11:43:03,354] Trial 20 pruned. \n",
      "[I 2025-12-06 11:55:06,261] Trial 21 finished with value: 0.5373401908722235 and parameters: {'learning_rate': 0.07024557937689278, 'num_leaves': 63, 'max_depth': 8, 'min_data_in_leaf': 100, 'min_sum_hessian_in_leaf': 40.57297147998465, 'lambda_l1': 0.051335945226045496, 'lambda_l2': 0.05891557548710448, 'feature_fraction': 0.4702275388569679, 'bagging_fraction': 0.8446331656109073, 'bagging_freq': 2, 'max_bin': 144, 'boosting_type': 'dart', 'drop_rate': 0.20741545721364193, 'skip_drop': 0.6069671019212989}. Best is trial 14 with value: 0.5354956377681526.\n",
      "[I 2025-12-06 12:09:23,841] Trial 22 finished with value: 0.5354866351443903 and parameters: {'learning_rate': 0.04598056401871342, 'num_leaves': 108, 'max_depth': 8, 'min_data_in_leaf': 314, 'min_sum_hessian_in_leaf': 27.474208014918247, 'lambda_l1': 0.0010243465403903457, 'lambda_l2': 1.3810841779333944, 'feature_fraction': 0.7233795702159724, 'bagging_fraction': 0.791611721608461, 'bagging_freq': 2, 'max_bin': 107, 'boosting_type': 'dart', 'drop_rate': 0.17008530145597325, 'skip_drop': 0.5552044310742978}. Best is trial 22 with value: 0.5354866351443903.\n",
      "[I 2025-12-06 12:19:03,107] Trial 23 pruned. \n",
      "[I 2025-12-06 12:34:43,554] Trial 24 finished with value: 0.534572319179253 and parameters: {'learning_rate': 0.028622707150310074, 'num_leaves': 115, 'max_depth': 11, 'min_data_in_leaf': 283, 'min_sum_hessian_in_leaf': 57.91358026953439, 'lambda_l1': 0.2032994126001902, 'lambda_l2': 0.3418528635810129, 'feature_fraction': 0.6761253318437276, 'bagging_fraction': 0.784073057841516, 'bagging_freq': 2, 'max_bin': 115, 'boosting_type': 'dart', 'drop_rate': 0.1102690525619016, 'skip_drop': 0.6426075909666971}. Best is trial 24 with value: 0.534572319179253.\n",
      "[I 2025-12-06 12:43:13,561] Trial 25 pruned. \n",
      "[I 2025-12-06 12:57:19,221] Trial 26 pruned. \n",
      "[I 2025-12-06 13:08:38,601] Trial 27 pruned. \n",
      "[I 2025-12-06 13:15:01,951] Trial 28 pruned. \n",
      "[I 2025-12-06 13:25:17,537] Trial 29 pruned. \n",
      "[I 2025-12-06 13:43:34,983] Trial 30 finished with value: 0.5344418274278087 and parameters: {'learning_rate': 0.03258702547449935, 'num_leaves': 110, 'max_depth': 11, 'min_data_in_leaf': 305, 'min_sum_hessian_in_leaf': 18.078407580125447, 'lambda_l1': 0.00019127608205542893, 'lambda_l2': 1.4060679416080204e-08, 'feature_fraction': 0.6019786987059477, 'bagging_fraction': 0.8573828755179134, 'bagging_freq': 3, 'max_bin': 123, 'boosting_type': 'dart', 'drop_rate': 0.1652439337494428, 'skip_drop': 0.563104510005064}. Best is trial 30 with value: 0.5344418274278087.\n",
      "[I 2025-12-06 14:01:39,836] Trial 31 finished with value: 0.5344360140693358 and parameters: {'learning_rate': 0.03261762141476745, 'num_leaves': 116, 'max_depth': 11, 'min_data_in_leaf': 324, 'min_sum_hessian_in_leaf': 15.899998013492961, 'lambda_l1': 0.00024226236713205732, 'lambda_l2': 1.6904402126739538e-08, 'feature_fraction': 0.5891890935259665, 'bagging_fraction': 0.8522335262533808, 'bagging_freq': 3, 'max_bin': 118, 'boosting_type': 'dart', 'drop_rate': 0.16906042022882006, 'skip_drop': 0.5631875025813916}. Best is trial 31 with value: 0.5344360140693358.\n",
      "[I 2025-12-06 14:18:50,382] Trial 32 finished with value: 0.5354056122101293 and parameters: {'learning_rate': 0.03177412702553478, 'num_leaves': 116, 'max_depth': 11, 'min_data_in_leaf': 626, 'min_sum_hessian_in_leaf': 19.41717180547331, 'lambda_l1': 9.276479011615285e-05, 'lambda_l2': 1.1477755796429457e-08, 'feature_fraction': 0.6027414779293389, 'bagging_fraction': 0.8606112133195631, 'bagging_freq': 3, 'max_bin': 125, 'boosting_type': 'dart', 'drop_rate': 0.17345575222103143, 'skip_drop': 0.5716350963734276}. Best is trial 31 with value: 0.5344360140693358.\n",
      "[I 2025-12-06 14:33:02,040] Trial 33 finished with value: 0.5348575553683138 and parameters: {'learning_rate': 0.03034788264342395, 'num_leaves': 118, 'max_depth': 11, 'min_data_in_leaf': 616, 'min_sum_hessian_in_leaf': 1.2174772059467927, 'lambda_l1': 4.46630791040213e-05, 'lambda_l2': 1.6324743032219512e-08, 'feature_fraction': 0.6032565191846893, 'bagging_fraction': 0.866004089220525, 'bagging_freq': 3, 'max_bin': 133, 'boosting_type': 'dart', 'drop_rate': 0.08980717356882845, 'skip_drop': 0.6385614716880612}. Best is trial 31 with value: 0.5344360140693358.\n",
      "[I 2025-12-06 14:46:45,122] Trial 34 finished with value: 0.5337848585035608 and parameters: {'learning_rate': 0.02476583870929504, 'num_leaves': 120, 'max_depth': 12, 'min_data_in_leaf': 398, 'min_sum_hessian_in_leaf': 1.198658869717196, 'lambda_l1': 3.9653009952802845e-05, 'lambda_l2': 9.23980228417893e-08, 'feature_fraction': 0.5329466538875125, 'bagging_fraction': 0.8998234940333566, 'bagging_freq': 3, 'max_bin': 134, 'boosting_type': 'dart', 'drop_rate': 0.04074063786458355, 'skip_drop': 0.6485581352259466}. Best is trial 34 with value: 0.5337848585035608.\n",
      "[I 2025-12-06 14:49:37,387] Trial 35 finished with value: 0.5333416985611913 and parameters: {'learning_rate': 0.02581936503955135, 'num_leaves': 127, 'max_depth': 12, 'min_data_in_leaf': 417, 'min_sum_hessian_in_leaf': 12.416697970714216, 'lambda_l1': 5.776241441289446e-07, 'lambda_l2': 1.0429562411579763e-07, 'feature_fraction': 0.525933689705239, 'bagging_fraction': 0.9099273320513573, 'bagging_freq': 3, 'max_bin': 141, 'boosting_type': 'gbdt'}. Best is trial 35 with value: 0.5333416985611913.\n",
      "[I 2025-12-06 14:52:56,313] Trial 36 finished with value: 0.5328422220023425 and parameters: {'learning_rate': 0.02262581492417439, 'num_leaves': 128, 'max_depth': 12, 'min_data_in_leaf': 438, 'min_sum_hessian_in_leaf': 13.377218000816148, 'lambda_l1': 2.462045248431029e-07, 'lambda_l2': 1.1733208404034257e-07, 'feature_fraction': 0.5309976860402901, 'bagging_fraction': 0.9180907833000288, 'bagging_freq': 4, 'max_bin': 185, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 14:56:15,747] Trial 37 finished with value: 0.5329679767468715 and parameters: {'learning_rate': 0.023586760260745607, 'num_leaves': 122, 'max_depth': 12, 'min_data_in_leaf': 423, 'min_sum_hessian_in_leaf': 11.214184583821096, 'lambda_l1': 4.820648119669682e-07, 'lambda_l2': 1.3023255538578364e-07, 'feature_fraction': 0.5269169684264112, 'bagging_fraction': 0.9128385753154741, 'bagging_freq': 5, 'max_bin': 190, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 14:59:32,863] Trial 38 finished with value: 0.533005270960391 and parameters: {'learning_rate': 0.02328607653351533, 'num_leaves': 126, 'max_depth': 12, 'min_data_in_leaf': 441, 'min_sum_hessian_in_leaf': 9.38998546534385, 'lambda_l1': 1.3184821798197712e-07, 'lambda_l2': 1.102471486968771e-07, 'feature_fraction': 0.5287369216564215, 'bagging_fraction': 0.9084871880394982, 'bagging_freq': 6, 'max_bin': 195, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:02:39,365] Trial 39 finished with value: 0.5357169516875779 and parameters: {'learning_rate': 0.014571908762161096, 'num_leaves': 128, 'max_depth': 12, 'min_data_in_leaf': 861, 'min_sum_hessian_in_leaf': 10.28735884459882, 'lambda_l1': 9.88546558260648e-08, 'lambda_l2': 1.571111445128232e-07, 'feature_fraction': 0.483679311851772, 'bagging_fraction': 0.916395787269424, 'bagging_freq': 6, 'max_bin': 194, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:06:08,206] Trial 40 finished with value: 0.5332862567926905 and parameters: {'learning_rate': 0.02029267247168986, 'num_leaves': 128, 'max_depth': 12, 'min_data_in_leaf': 447, 'min_sum_hessian_in_leaf': 7.354839468876335, 'lambda_l1': 6.979750239230845e-07, 'lambda_l2': 2.749763657736464e-06, 'feature_fraction': 0.4549891608945557, 'bagging_fraction': 0.9259097693790588, 'bagging_freq': 6, 'max_bin': 210, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:09:32,499] Trial 41 finished with value: 0.5334796772952098 and parameters: {'learning_rate': 0.01944512700010018, 'num_leaves': 123, 'max_depth': 12, 'min_data_in_leaf': 485, 'min_sum_hessian_in_leaf': 6.5936964827947655, 'lambda_l1': 7.973208783675653e-07, 'lambda_l2': 2.5554343991039653e-06, 'feature_fraction': 0.45510128146514717, 'bagging_fraction': 0.9093326836623038, 'bagging_freq': 6, 'max_bin': 210, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:13:14,537] Trial 42 finished with value: 0.5341751529508887 and parameters: {'learning_rate': 0.01268577196639283, 'num_leaves': 128, 'max_depth': 12, 'min_data_in_leaf': 578, 'min_sum_hessian_in_leaf': 12.73461117010195, 'lambda_l1': 6.134853937520072e-08, 'lambda_l2': 9.245670829270877e-08, 'feature_fraction': 0.5453844555538717, 'bagging_fraction': 0.9236460280290759, 'bagging_freq': 6, 'max_bin': 193, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:16:44,621] Trial 43 finished with value: 0.5333554754794567 and parameters: {'learning_rate': 0.021315352636858704, 'num_leaves': 122, 'max_depth': 12, 'min_data_in_leaf': 432, 'min_sum_hessian_in_leaf': 7.673148419652823, 'lambda_l1': 2.3319453239450925e-06, 'lambda_l2': 3.3585699569238474e-07, 'feature_fraction': 0.5211844418565619, 'bagging_fraction': 0.8925060619130185, 'bagging_freq': 7, 'max_bin': 225, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:19:39,276] Trial 44 finished with value: 0.5343537178572957 and parameters: {'learning_rate': 0.024302369721639728, 'num_leaves': 121, 'max_depth': 12, 'min_data_in_leaf': 672, 'min_sum_hessian_in_leaf': 14.157403450158355, 'lambda_l1': 5.649518450003939e-07, 'lambda_l2': 4.192057412103216e-06, 'feature_fraction': 0.49163232647600896, 'bagging_fraction': 0.8873498260940134, 'bagging_freq': 5, 'max_bin': 184, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:22:40,868] Trial 45 finished with value: 0.5336593658919866 and parameters: {'learning_rate': 0.018402862060926357, 'num_leaves': 97, 'max_depth': 10, 'min_data_in_leaf': 427, 'min_sum_hessian_in_leaf': 23.576139877876713, 'lambda_l1': 6.054767302103732e-08, 'lambda_l2': 5.5432169224056904e-08, 'feature_fraction': 0.4390773328456906, 'bagging_fraction': 0.9287829521737229, 'bagging_freq': 5, 'max_bin': 208, 'boosting_type': 'gbdt'}. Best is trial 36 with value: 0.5328422220023425.\n",
      "[I 2025-12-06 15:26:51,037] Trial 46 finished with value: 0.5323810278852052 and parameters: {'learning_rate': 0.015246242097511301, 'num_leaves': 104, 'max_depth': 12, 'min_data_in_leaf': 221, 'min_sum_hessian_in_leaf': 4.123446298067507, 'lambda_l1': 4.01046681841633e-06, 'lambda_l2': 3.497419016525011e-05, 'feature_fraction': 0.5075493418267852, 'bagging_fraction': 0.9484194813591086, 'bagging_freq': 6, 'max_bin': 236, 'boosting_type': 'gbdt'}. Best is trial 46 with value: 0.5323810278852052.\n",
      "[I 2025-12-06 15:31:12,292] Trial 47 finished with value: 0.5324099130616761 and parameters: {'learning_rate': 0.013836521520094512, 'num_leaves': 103, 'max_depth': 12, 'min_data_in_leaf': 220, 'min_sum_hessian_in_leaf': 4.257335635566083, 'lambda_l1': 1.4199023784539106e-05, 'lambda_l2': 1.636396051585026e-05, 'feature_fraction': 0.566237215182601, 'bagging_fraction': 0.9491181117121593, 'bagging_freq': 6, 'max_bin': 237, 'boosting_type': 'gbdt'}. Best is trial 46 with value: 0.5323810278852052.\n",
      "[I 2025-12-06 15:34:23,951] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 15:38:28,521] A new study created in memory with name: xgb_hpo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 15:38:28,411] Trial 49 pruned. \n",
      "\n",
      "  âœ… LGBM Best LogLoss: 0.5324\n",
      "     Best AUC: 0.8044\n",
      "\n",
      "--------------------------------------------------\n",
      "  ðŸš€ XGBoost HPO\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92944ee0bb8d49c6a1d7663f8de844e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 15:40:59,641] Trial 0 finished with value: 0.5346718060211871 and parameters: {'xgb_learning_rate': 0.015355286838886862, 'xgb_max_depth': 12, 'xgb_min_child_weight': 73.46740023932911, 'xgb_subsample': 0.7693963178886665, 'xgb_colsample_bytree': 0.48581025224334007, 'xgb_reg_alpha': 2.5348407664333426e-07, 'xgb_reg_lambda': 3.809220577048033e-08, 'xgb_gamma': 0.3426417745118369, 'xgb_max_bin': 359}. Best is trial 0 with value: 0.5346718060211871.\n",
      "[I 2025-12-06 15:41:45,808] Trial 1 finished with value: 0.5439732651389815 and parameters: {'xgb_learning_rate': 0.04170553216181044, 'xgb_max_depth': 3, 'xgb_min_child_weight': 97.02107536403744, 'xgb_subsample': 0.8745991883601898, 'xgb_colsample_bytree': 0.5167865108730519, 'xgb_reg_alpha': 4.329370014459266e-07, 'xgb_reg_lambda': 6.8240955406304e-07, 'xgb_gamma': 4.4319427891510175e-06, 'xgb_max_bin': 330}. Best is trial 0 with value: 0.5346718060211871.\n",
      "[I 2025-12-06 15:42:47,400] Trial 2 finished with value: 0.5408894883807766 and parameters: {'xgb_learning_rate': 0.018236581424556052, 'xgb_max_depth': 5, 'xgb_min_child_weight': 61.573436577515565, 'xgb_subsample': 0.5627722372934189, 'xgb_colsample_bytree': 0.56067955669437, 'xgb_reg_alpha': 1.9826980964985924e-05, 'xgb_reg_lambda': 0.0003636636071695854, 'xgb_gamma': 0.06764288719681069, 'xgb_max_bin': 204}. Best is trial 0 with value: 0.5346718060211871.\n",
      "[I 2025-12-06 15:44:30,947] Trial 3 finished with value: 0.5342830700510477 and parameters: {'xgb_learning_rate': 0.023334818836184625, 'xgb_max_depth': 8, 'xgb_min_child_weight': 5.598590859279775, 'xgb_subsample': 0.7733951833556472, 'xgb_colsample_bytree': 0.49378826802801035, 'xgb_reg_alpha': 3.850031979199519e-08, 'xgb_reg_lambda': 30.821613670416532, 'xgb_gamma': 2.5119100929409792, 'xgb_max_bin': 439}. Best is trial 3 with value: 0.5342830700510477.\n",
      "[I 2025-12-06 15:45:17,600] Trial 4 finished with value: 0.551876140056289 and parameters: {'xgb_learning_rate': 0.012453219846912198, 'xgb_max_depth': 3, 'xgb_min_child_weight': 68.73906962470353, 'xgb_subsample': 0.6980686221828205, 'xgb_colsample_bytree': 0.4671210291646284, 'xgb_reg_alpha': 0.00028614897264046574, 'xgb_reg_lambda': 2.2074212100007785e-08, 'xgb_gamma': 0.8131129951713353, 'xgb_max_bin': 227}. Best is trial 3 with value: 0.5342830700510477.\n",
      "[I 2025-12-06 15:46:27,187] Trial 5 finished with value: 0.5367697611145488 and parameters: {'xgb_learning_rate': 0.036385753170854684, 'xgb_max_depth': 6, 'xgb_min_child_weight': 52.48673409660327, 'xgb_subsample': 0.7460196257044758, 'xgb_colsample_bytree': 0.5016699505390398, 'xgb_reg_alpha': 5.324289357128436, 'xgb_reg_lambda': 0.5640638061927871, 'xgb_gamma': 1.4882404753442915, 'xgb_max_bin': 472}. Best is trial 3 with value: 0.5342830700510477.\n",
      "[I 2025-12-06 15:47:57,876] Trial 6 finished with value: 0.5376242460969655 and parameters: {'xgb_learning_rate': 0.0299816694120633, 'xgb_max_depth': 12, 'xgb_min_child_weight': 9.76075770314003, 'xgb_subsample': 0.5881922880886153, 'xgb_colsample_bytree': 0.42487500890079594, 'xgb_reg_alpha': 8.471746987003668e-06, 'xgb_reg_lambda': 7.705004503489671e-05, 'xgb_gamma': 2.293280024184598e-06, 'xgb_max_bin': 447}. Best is trial 3 with value: 0.5342830700510477.\n",
      "[I 2025-12-06 15:48:43,030] Trial 7 pruned. \n",
      "[I 2025-12-06 15:50:44,002] Trial 8 pruned. \n",
      "[I 2025-12-06 15:51:47,538] Trial 9 finished with value: 0.5361488806369445 and parameters: {'xgb_learning_rate': 0.03235188302117385, 'xgb_max_depth': 6, 'xgb_min_child_weight': 7.29227667831634, 'xgb_subsample': 0.639942044772048, 'xgb_colsample_bytree': 0.5788508271147108, 'xgb_reg_alpha': 0.036851536911881845, 'xgb_reg_lambda': 0.023745138854603057, 'xgb_gamma': 0.5222002015831414, 'xgb_max_bin': 309}. Best is trial 3 with value: 0.5342830700510477.\n",
      "[I 2025-12-06 15:52:38,495] Trial 10 finished with value: 0.535237149802753 and parameters: {'xgb_learning_rate': 0.08484397325237239, 'xgb_max_depth': 9, 'xgb_min_child_weight': 26.289438670657542, 'xgb_subsample': 0.9195306538603043, 'xgb_colsample_bytree': 0.6855292017927148, 'xgb_reg_alpha': 0.016301353379407614, 'xgb_reg_lambda': 53.80662040700942, 'xgb_gamma': 0.00165442898348124, 'xgb_max_bin': 381}. Best is trial 3 with value: 0.5342830700510477.\n",
      "[I 2025-12-06 15:54:01,754] Trial 11 pruned. \n",
      "[I 2025-12-06 15:55:13,400] Trial 12 pruned. \n",
      "[I 2025-12-06 15:55:34,779] Trial 13 pruned. \n",
      "[I 2025-12-06 15:57:36,464] Trial 14 finished with value: 0.534050894455013 and parameters: {'xgb_learning_rate': 0.02270088497937799, 'xgb_max_depth': 12, 'xgb_min_child_weight': 80.99853741026925, 'xgb_subsample': 0.7969766750680127, 'xgb_colsample_bytree': 0.938595366006461, 'xgb_reg_alpha': 0.0001419058913501889, 'xgb_reg_lambda': 0.005859124067964499, 'xgb_gamma': 0.00011654886220782481, 'xgb_max_bin': 380}. Best is trial 14 with value: 0.534050894455013.\n",
      "[I 2025-12-06 15:59:32,187] Trial 15 finished with value: 0.5323166882991152 and parameters: {'xgb_learning_rate': 0.025225713930981417, 'xgb_max_depth': 10, 'xgb_min_child_weight': 18.458622866319512, 'xgb_subsample': 0.8354261768622411, 'xgb_colsample_bytree': 0.9226250986539348, 'xgb_reg_alpha': 0.001112544529924749, 'xgb_reg_lambda': 2.597174276407312, 'xgb_gamma': 2.286250021981542e-05, 'xgb_max_bin': 412}. Best is trial 15 with value: 0.5323166882991152.\n",
      "[I 2025-12-06 16:00:22,559] Trial 16 finished with value: 0.5340641000594294 and parameters: {'xgb_learning_rate': 0.056924400428297564, 'xgb_max_depth': 11, 'xgb_min_child_weight': 21.44652789625755, 'xgb_subsample': 0.940840924856553, 'xgb_colsample_bytree': 0.9364281945549994, 'xgb_reg_alpha': 0.0025598782214258154, 'xgb_reg_lambda': 0.007331433304683844, 'xgb_gamma': 2.4093037914138593e-05, 'xgb_max_bin': 133}. Best is trial 15 with value: 0.5323166882991152.\n",
      "[I 2025-12-06 16:02:20,734] Trial 17 finished with value: 0.5325805292283441 and parameters: {'xgb_learning_rate': 0.024070928870843798, 'xgb_max_depth': 10, 'xgb_min_child_weight': 41.14311307269314, 'xgb_subsample': 0.8553293031347204, 'xgb_colsample_bytree': 0.9308175366625537, 'xgb_reg_alpha': 0.00023373987019070525, 'xgb_reg_lambda': 1.7664491072354358, 'xgb_gamma': 0.0002464830336298858, 'xgb_max_bin': 414}. Best is trial 15 with value: 0.5323166882991152.\n",
      "[I 2025-12-06 16:03:21,306] Trial 18 finished with value: 0.5336639842136435 and parameters: {'xgb_learning_rate': 0.05081214118249061, 'xgb_max_depth': 10, 'xgb_min_child_weight': 41.88108685989062, 'xgb_subsample': 0.8851170557737664, 'xgb_colsample_bytree': 0.8504870569289754, 'xgb_reg_alpha': 0.2765704642444978, 'xgb_reg_lambda': 2.072924023774736, 'xgb_gamma': 6.087253411665074e-08, 'xgb_max_bin': 265}. Best is trial 15 with value: 0.5323166882991152.\n",
      "[I 2025-12-06 16:05:07,810] Trial 19 finished with value: 0.5328010583247537 and parameters: {'xgb_learning_rate': 0.02696198723694754, 'xgb_max_depth': 10, 'xgb_min_child_weight': 40.69565752757269, 'xgb_subsample': 0.8664325941064607, 'xgb_colsample_bytree': 0.7885654148164116, 'xgb_reg_alpha': 0.002101149274757109, 'xgb_reg_lambda': 1.408633448223654, 'xgb_gamma': 0.0006743179223094592, 'xgb_max_bin': 415}. Best is trial 15 with value: 0.5323166882991152.\n",
      "[I 2025-12-06 16:06:12,038] Trial 20 pruned. \n",
      "[I 2025-12-06 16:08:02,510] Trial 21 finished with value: 0.5327372242099111 and parameters: {'xgb_learning_rate': 0.026445835367864066, 'xgb_max_depth': 10, 'xgb_min_child_weight': 42.67716979776655, 'xgb_subsample': 0.8774788431677225, 'xgb_colsample_bytree': 0.8835466219584007, 'xgb_reg_alpha': 0.0020282083207591563, 'xgb_reg_lambda': 0.19775806505811777, 'xgb_gamma': 0.0004048534596110632, 'xgb_max_bin': 412}. Best is trial 15 with value: 0.5323166882991152.\n",
      "[I 2025-12-06 16:09:03,347] Trial 22 finished with value: 0.533660203837155 and parameters: {'xgb_learning_rate': 0.04517917130394658, 'xgb_max_depth': 9, 'xgb_min_child_weight': 42.58850549286624, 'xgb_subsample': 0.9043739701153868, 'xgb_colsample_bytree': 0.9005198176318394, 'xgb_reg_alpha': 0.0017444979223948946, 'xgb_reg_lambda': 0.10144355182928416, 'xgb_gamma': 0.00010482969113276812, 'xgb_max_bin': 353}. Best is trial 15 with value: 0.5323166882991152.\n",
      "[I 2025-12-06 16:09:55,607] Trial 23 pruned. \n",
      "[I 2025-12-06 16:13:08,134] Trial 24 finished with value: 0.5321157797494598 and parameters: {'xgb_learning_rate': 0.01108658352606943, 'xgb_max_depth': 10, 'xgb_min_child_weight': 16.70232505200572, 'xgb_subsample': 0.9465844661847105, 'xgb_colsample_bytree': 0.8920449712762915, 'xgb_reg_alpha': 0.18498669965948222, 'xgb_reg_lambda': 6.819011001391611, 'xgb_gamma': 5.421558666319439e-07, 'xgb_max_bin': 482}. Best is trial 24 with value: 0.5321157797494598.\n",
      "[I 2025-12-06 16:15:01,012] Trial 25 pruned. \n",
      "[I 2025-12-06 16:17:57,306] Trial 26 pruned. \n",
      "[I 2025-12-06 16:19:56,554] Trial 27 pruned. \n",
      "[I 2025-12-06 16:21:05,904] Trial 28 pruned. \n",
      "[I 2025-12-06 16:23:37,967] Trial 29 finished with value: 0.5320591883953294 and parameters: {'xgb_learning_rate': 0.017667137327493913, 'xgb_max_depth': 12, 'xgb_min_child_weight': 59.460880247178224, 'xgb_subsample': 0.9451976539308322, 'xgb_colsample_bytree': 0.7566099988745344, 'xgb_reg_alpha': 0.06627049303128432, 'xgb_reg_lambda': 0.6561910241920923, 'xgb_gamma': 6.237356403173438e-06, 'xgb_max_bin': 338}. Best is trial 29 with value: 0.5320591883953294.\n",
      "[I 2025-12-06 16:26:02,744] Trial 30 finished with value: 0.5316726287500573 and parameters: {'xgb_learning_rate': 0.017814381567323113, 'xgb_max_depth': 12, 'xgb_min_child_weight': 57.086892554766564, 'xgb_subsample': 0.9489747010989122, 'xgb_colsample_bytree': 0.7142541780295959, 'xgb_reg_alpha': 0.09751582447236345, 'xgb_reg_lambda': 0.0015388616311083133, 'xgb_gamma': 5.206735421827751e-06, 'xgb_max_bin': 274}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:28:36,889] Trial 31 finished with value: 0.5318813683434718 and parameters: {'xgb_learning_rate': 0.017025391276516103, 'xgb_max_depth': 12, 'xgb_min_child_weight': 60.437059231844664, 'xgb_subsample': 0.937875205726902, 'xgb_colsample_bytree': 0.7427492385356002, 'xgb_reg_alpha': 0.1481413157129917, 'xgb_reg_lambda': 0.0020159965167996356, 'xgb_gamma': 7.3459520269564365e-06, 'xgb_max_bin': 272}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:31:07,875] Trial 32 finished with value: 0.5318296147409338 and parameters: {'xgb_learning_rate': 0.017057902812517362, 'xgb_max_depth': 12, 'xgb_min_child_weight': 61.45259088737396, 'xgb_subsample': 0.9468049630333082, 'xgb_colsample_bytree': 0.7123041270895221, 'xgb_reg_alpha': 0.08875994608583612, 'xgb_reg_lambda': 0.0005167880977777581, 'xgb_gamma': 8.008448766275866e-06, 'xgb_max_bin': 270}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:33:35,713] Trial 33 finished with value: 0.5321450572488117 and parameters: {'xgb_learning_rate': 0.017549357117883635, 'xgb_max_depth': 12, 'xgb_min_child_weight': 61.400214122192025, 'xgb_subsample': 0.8980907011721428, 'xgb_colsample_bytree': 0.7164439473079699, 'xgb_reg_alpha': 2.353817409134767, 'xgb_reg_lambda': 0.0005976241250023581, 'xgb_gamma': 4.94286252484572e-06, 'xgb_max_bin': 273}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:36:05,074] Trial 34 finished with value: 0.5322369363183204 and parameters: {'xgb_learning_rate': 0.01692563436715503, 'xgb_max_depth': 12, 'xgb_min_child_weight': 63.048445245258435, 'xgb_subsample': 0.9241920219485225, 'xgb_colsample_bytree': 0.626974433864968, 'xgb_reg_alpha': 0.035576826662921224, 'xgb_reg_lambda': 5.200858150207495e-06, 'xgb_gamma': 5.919492257996337e-06, 'xgb_max_bin': 271}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:38:31,924] Trial 35 finished with value: 0.5329432199151183 and parameters: {'xgb_learning_rate': 0.014410690614837013, 'xgb_max_depth': 12, 'xgb_min_child_weight': 78.25617856988711, 'xgb_subsample': 0.9440599329897778, 'xgb_colsample_bytree': 0.6468291935478576, 'xgb_reg_alpha': 0.0791302314615925, 'xgb_reg_lambda': 0.0020191943114609753, 'xgb_gamma': 2.1005379607214668e-06, 'xgb_max_bin': 333}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:40:41,923] Trial 36 finished with value: 0.532287851637286 and parameters: {'xgb_learning_rate': 0.019699563491002117, 'xgb_max_depth': 11, 'xgb_min_child_weight': 57.265645801410095, 'xgb_subsample': 0.9085826565090529, 'xgb_colsample_bytree': 0.7182933674196281, 'xgb_reg_alpha': 1.3017571761489386, 'xgb_reg_lambda': 0.0001493653331001993, 'xgb_gamma': 8.159252650515154e-05, 'xgb_max_bin': 242}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:43:14,502] Trial 37 finished with value: 0.5323544563881878 and parameters: {'xgb_learning_rate': 0.013759708309744038, 'xgb_max_depth': 12, 'xgb_min_child_weight': 67.1632218363949, 'xgb_subsample': 0.9474842616108737, 'xgb_colsample_bytree': 0.7546740518947519, 'xgb_reg_alpha': 0.006619639613073056, 'xgb_reg_lambda': 1.7132030499216147e-05, 'xgb_gamma': 9.438161058478293e-06, 'xgb_max_bin': 292}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:45:20,738] Trial 38 finished with value: 0.53223369345445 and parameters: {'xgb_learning_rate': 0.020133024886405716, 'xgb_max_depth': 11, 'xgb_min_child_weight': 49.32510097527923, 'xgb_subsample': 0.886470968942407, 'xgb_colsample_bytree': 0.7107510409993801, 'xgb_reg_alpha': 0.11019493531285514, 'xgb_reg_lambda': 1.1823306364230256e-07, 'xgb_gamma': 1.0901690869324035e-06, 'xgb_max_bin': 174}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:48:00,768] Trial 39 finished with value: 0.531846557326201 and parameters: {'xgb_learning_rate': 0.015350451966709252, 'xgb_max_depth': 12, 'xgb_min_child_weight': 49.36942679866629, 'xgb_subsample': 0.92152547966794, 'xgb_colsample_bytree': 0.6557257153578833, 'xgb_reg_alpha': 0.8834943185210481, 'xgb_reg_lambda': 0.0011269530115420277, 'xgb_gamma': 9.669978196481256e-08, 'xgb_max_bin': 239}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:48:36,389] Trial 40 pruned. \n",
      "[I 2025-12-06 16:51:11,045] Trial 41 finished with value: 0.5318852117676637 and parameters: {'xgb_learning_rate': 0.015904530188003554, 'xgb_max_depth': 12, 'xgb_min_child_weight': 58.82921423306464, 'xgb_subsample': 0.9193067705918274, 'xgb_colsample_bytree': 0.7941747211376302, 'xgb_reg_alpha': 2.0742842993432657, 'xgb_reg_lambda': 0.0002512549386024215, 'xgb_gamma': 1.3450865870348084e-08, 'xgb_max_bin': 209}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:53:40,638] Trial 42 finished with value: 0.5326561689053563 and parameters: {'xgb_learning_rate': 0.01590760559859509, 'xgb_max_depth': 12, 'xgb_min_child_weight': 66.73716642941272, 'xgb_subsample': 0.920718538005131, 'xgb_colsample_bytree': 0.5498011770887545, 'xgb_reg_alpha': 3.80147899483338, 'xgb_reg_lambda': 0.00014511366632003652, 'xgb_gamma': 1.4680335375191797e-08, 'xgb_max_bin': 209}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:55:10,550] Trial 43 pruned. \n",
      "[I 2025-12-06 16:57:55,861] Trial 44 finished with value: 0.5320355303414559 and parameters: {'xgb_learning_rate': 0.02168931497523654, 'xgb_max_depth': 12, 'xgb_min_child_weight': 53.83878341972746, 'xgb_subsample': 0.9196582256834359, 'xgb_colsample_bytree': 0.6610934153656448, 'xgb_reg_alpha': 9.953925060633265, 'xgb_reg_lambda': 0.003092786436831338, 'xgb_gamma': 1.576362742928328e-07, 'xgb_max_bin': 252}. Best is trial 30 with value: 0.5316726287500573.\n",
      "[I 2025-12-06 16:59:22,539] Trial 45 pruned. \n",
      "[I 2025-12-06 17:01:08,278] Trial 46 pruned. \n",
      "[I 2025-12-06 17:02:46,923] Trial 47 pruned. \n",
      "[I 2025-12-06 17:04:10,440] Trial 48 pruned. \n",
      "[I 2025-12-06 17:05:33,547] Trial 49 pruned. \n",
      "\n",
      "  âœ… XGB Best LogLoss: 0.5317\n",
      "     Best AUC: 0.8051\n",
      "\n",
      "--------------------------------------------------\n",
      "  ðŸ± CatBoost HPO\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 17:05:33,734] A new study created in memory with name: cat_hpo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ad3a025c9b48bfae218604b86563f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-12-06 17:05:33,906] Trial 0 failed with parameters: {'cat_learning_rate': 0.01787356461300122, 'cat_depth': 10, 'cat_l2_leaf_reg': 29.10635913133069, 'cat_min_data_in_leaf': 303, 'cat_random_strength': 1.6445845403801216, 'cat_bagging_temperature': 0.15599452033620265, 'cat_border_count': 45} because of the following error: CatBoostError('catboost/private/libs/options/json_helper.h:185: Error: change of option monotone_constraints is unimplemented for task type GPU and was not default in previous run').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrateur\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrateur\\AppData\\Local\\Temp\\ipykernel_8464\\3860065153.py\", line 451, in objective\n",
      "    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
      "  File \"C:\\Users\\Administrateur\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"C:\\Users\\Administrateur\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"C:\\Users\\Administrateur\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5017, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5066, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/private/libs/options/json_helper.h:185: Error: change of option monotone_constraints is unimplemented for task type GPU and was not default in previous run\n",
      "[W 2025-12-06 17:05:33,922] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/private/libs/options/json_helper.h:185: Error: change of option monotone_constraints is unimplemented for task type GPU and was not default in previous run",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 904\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_params, best_scores, final_metrics\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 904\u001b[0m     best_params, best_scores, final_metrics \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[1;32mIn[1], line 853\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    850\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    852\u001b[0m \u001b[38;5;66;03m# Run HPO\u001b[39;00m\n\u001b[1;32m--> 853\u001b[0m best_params, best_scores, studies \u001b[38;5;241m=\u001b[39m run_hpo(X_cv, y_cv, feature_cols)\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# Split for final training (70/15/15 from CV data)\u001b[39;00m\n\u001b[0;32m    856\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_cv)\n",
      "Cell \u001b[1;32mIn[1], line 574\u001b[0m, in \u001b[0;36mrun_hpo\u001b[1;34m(X, y, feature_cols)\u001b[0m\n\u001b[0;32m    565\u001b[0m study_cat \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    566\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    567\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mRANDOM_SEED),\n\u001b[0;32m    568\u001b[0m     pruner\u001b[38;5;241m=\u001b[39mMedianPruner(n_startup_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m    569\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_hpo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n\u001b[0;32m    572\u001b[0m objective_cat \u001b[38;5;241m=\u001b[39m create_catboost_objective(X, y, cv, monotonic_constraints)\n\u001b[1;32m--> 574\u001b[0m study_cat\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m    575\u001b[0m     objective_cat,\n\u001b[0;32m    576\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39mN_TRIALS,\n\u001b[0;32m    577\u001b[0m     show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     gc_after_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[0;32m    581\u001b[0m best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m study_cat\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m    582\u001b[0m best_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: study_cat\u001b[38;5;241m.\u001b[39mbest_value,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m: study_cat\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39muser_attrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    585\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[1], line 451\u001b[0m, in \u001b[0;36mcreate_catboost_objective.<locals>.objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    448\u001b[0m y_train, y_val \u001b[38;5;241m=\u001b[39m y[train_idx], y[val_idx]\n\u001b[0;32m    450\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 451\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m(X_val, y_val))\n\u001b[0;32m    453\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    454\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(y_pred, \u001b[38;5;241m1e-7\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1e-7\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:5245\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5243\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, graph, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline, use_best_model,\n\u001b[0;32m   5246\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[0;32m   5247\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[0;32m   5248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   2411\u001b[0m         train_pool,\n\u001b[0;32m   2412\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2413\u001b[0m         params,\n\u001b[0;32m   2414\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2415\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2416\u001b[0m     )\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;28;01mif\u001b[39;00m init_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:5017\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5066\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: catboost/private/libs/options/json_helper.h:185: Error: change of option monotone_constraints is unimplemented for task type GPU and was not default in previous run"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ===============================================\n",
    "# HPO OPTUNA - GOD MODE SOTA 2026\n",
    "# TennisTitan - Ultimate Hyperparameter Optimization\n",
    "# ===============================================\n",
    "#\n",
    "# FEATURES GOD MODE:\n",
    "# âœ… Optuna TPE Sampler (Tree-structured Parzen Estimator)\n",
    "# âœ… Purged Rolling CV (anti-leakage temporel)\n",
    "# âœ… Contraintes monotones (Eloâ†‘ = probaâ†‘)\n",
    "# âœ… Multi-objectif: LogLoss + AUC\n",
    "# âœ… DART mode pour LGBM (dropout trees)\n",
    "# âœ… Early pruning (MedianPruner)\n",
    "# âœ… Feature importance intÃ©grÃ©e\n",
    "# âœ… Warm start depuis params actuels\n",
    "#\n",
    "# Input: data_clean/ml_final/\n",
    "# Output: models/hpo_sota_2026/\n",
    "# ===============================================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import optuna\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURATION\n",
    "# ===============================================\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / \"data_clean\" / \"ml_final\"\n",
    "OUT_DIR = ROOT / \"models\" / \"hpo_sota_2026\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# HPO Configuration\n",
    "N_TRIALS = 50  # Nombre de trials Optuna (augmenter pour meilleurs rÃ©sultats)\n",
    "N_FOLDS = 5    # Nombre de folds pour rolling CV\n",
    "PURGE_DAYS = 7 # Jours de purge entre train/val (anti-leakage)\n",
    "EMBARGO_DAYS = 3  # Jours d'embargo aprÃ¨s val\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOPPING_ROUNDS = 150\n",
    "MAX_BOOST_ROUNDS = 3000\n",
    "\n",
    "# Colonnes Ã  exclure\n",
    "ID_COLS = [\n",
    "    \"custom_match_id\", \"match_id_ta_dedup\", \"match_id_ta_source\",\n",
    "    \"winner_id\", \"loser_id\", \"tourney_name_ta\", \"tourney_slug_ta\",\n",
    "]\n",
    "\n",
    "# ===============================================\n",
    "# MONOTONIC CONSTRAINTS - GOD MODE\n",
    "# ===============================================\n",
    "# Format: 1 = increasing (plus haut = plus de proba victoire)\n",
    "#        -1 = decreasing (plus haut = moins de proba)\n",
    "#         0 = no constraint\n",
    "\n",
    "MONOTONIC_FEATURES = {\n",
    "    # Elo/Glicko ratings: higher = better\n",
    "    \"g2_global_rating_A\": 1,\n",
    "    \"g2_global_rating_B\": -1,\n",
    "    \"g2_hard_rating_A\": 1,\n",
    "    \"g2_hard_rating_B\": -1,\n",
    "    \"g2_clay_rating_A\": 1,\n",
    "    \"g2_clay_rating_B\": -1,\n",
    "    \"g2_grass_rating_A\": 1,\n",
    "    \"g2_grass_rating_B\": -1,\n",
    "    \n",
    "    # Rankings: lower rank = better (so higher rank_A = worse)\n",
    "    \"winner_rank_ta\": -1,  # A is winner, lower rank = better\n",
    "    \"loser_rank_ta\": 1,    # B is loser\n",
    "    \n",
    "    # Win rates: higher = better\n",
    "    \"win_rate_r20_A\": 1,\n",
    "    \"win_rate_r20_B\": -1,\n",
    "    \n",
    "    # Odds (NOUVEAU!)\n",
    "    \"odds_implied_prob_A\": 1,\n",
    "    \"odds_implied_prob_B\": -1,\n",
    "    \n",
    "    # H2H: higher for A = better\n",
    "    \"h2h_win_rate_A\": 1,\n",
    "    \"h2h_win_rate_B\": -1,\n",
    "    \"h2h_dominance_A\": 1,\n",
    "    \"h2h_dominance_B\": -1,\n",
    "    \n",
    "    # Mental/Clutch\n",
    "    \"mental_toughness_score_A\": 1,\n",
    "    \"mental_toughness_score_B\": -1,\n",
    "    \"clutch_score_A\": 1,\n",
    "    \"clutch_score_B\": -1,\n",
    "    \n",
    "    # Service dominance\n",
    "    \"srv_rating_A\": 1,\n",
    "    \"srv_rating_B\": -1,\n",
    "    \"ret_rating_A\": 1,\n",
    "    \"ret_rating_B\": -1,\n",
    "}\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# LOAD DATA\n",
    "# ===============================================\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Charge les donnÃ©es preprocessÃ©es.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   LOAD DATA\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    train = pl.read_parquet(DATA_DIR / \"train.parquet\")\n",
    "    val = pl.read_parquet(DATA_DIR / \"val.parquet\")\n",
    "    test = pl.read_parquet(DATA_DIR / \"test.parquet\")\n",
    "    \n",
    "    # Combine train + val pour le CV (test reste holdout)\n",
    "    combined = pl.concat([train, val])\n",
    "    \n",
    "    print(f\"\\n  Train: {train.shape}\")\n",
    "    print(f\"  Val: {val.shape}\")\n",
    "    print(f\"  Test (holdout): {test.shape}\")\n",
    "    print(f\"  Combined for CV: {combined.shape}\")\n",
    "    \n",
    "    # Load feature list\n",
    "    feature_list_path = DATA_DIR / \"feature_list.json\"\n",
    "    if feature_list_path.exists():\n",
    "        with open(feature_list_path) as f:\n",
    "            feature_cols = json.load(f)\n",
    "    else:\n",
    "        exclude = [\"target_A_wins\", \"year\"] + ID_COLS\n",
    "        feature_cols = [c for c in train.columns if c not in exclude]\n",
    "    \n",
    "    print(f\"  Features: {len(feature_cols)}\")\n",
    "    \n",
    "    return combined, test, feature_cols\n",
    "\n",
    "\n",
    "def prepare_cv_data(df: pl.DataFrame, feature_cols: list):\n",
    "    \"\"\"PrÃ©pare les donnÃ©es pour le CV.\"\"\"\n",
    "    \n",
    "    # Filter existing columns\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    \n",
    "    X = df.select(feature_cols).to_numpy().astype(np.float32)\n",
    "    y = df[\"target_A_wins\"].to_numpy().astype(np.int32)\n",
    "    \n",
    "    # Get dates for temporal CV\n",
    "    if \"year\" in df.columns:\n",
    "        years = df[\"year\"].to_numpy()\n",
    "    else:\n",
    "        years = np.zeros(len(df))\n",
    "    \n",
    "    # Replace NaN\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    \n",
    "    return X, y, years, feature_cols\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# PURGED ROLLING CV - GOD MODE\n",
    "# ===============================================\n",
    "\n",
    "class PurgedRollingCV:\n",
    "    \"\"\"\n",
    "    Cross-validation temporelle avec purge et embargo.\n",
    "    \n",
    "    Ã‰vite le leakage en:\n",
    "    1. Purge: supprime les donnÃ©es proches de la frontiÃ¨re train/val\n",
    "    2. Embargo: ajoute un gap aprÃ¨s le val\n",
    "    \n",
    "    SchÃ©ma:\n",
    "    [========TRAIN========][PURGE][====VAL====][EMBARGO]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, purge_pct=0.01, embargo_pct=0.005):\n",
    "        self.n_splits = n_splits\n",
    "        self.purge_pct = purge_pct\n",
    "        self.embargo_pct = embargo_pct\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"GÃ©nÃ¨re les indices train/val pour chaque fold.\"\"\"\n",
    "        n = len(X)\n",
    "        \n",
    "        # Taille de chaque fold\n",
    "        fold_size = n // (self.n_splits + 1)\n",
    "        purge_size = int(n * self.purge_pct)\n",
    "        embargo_size = int(n * self.embargo_pct)\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            # Train: du dÃ©but jusqu'Ã  la fin du fold i\n",
    "            train_end = fold_size * (i + 1)\n",
    "            \n",
    "            # Val: aprÃ¨s purge, taille = fold_size\n",
    "            val_start = train_end + purge_size\n",
    "            val_end = val_start + fold_size\n",
    "            \n",
    "            # VÃ©rifier qu'on ne dÃ©passe pas\n",
    "            if val_end + embargo_size > n:\n",
    "                break\n",
    "            \n",
    "            train_idx = np.arange(0, train_end)\n",
    "            val_idx = np.arange(val_start, val_end)\n",
    "            \n",
    "            yield train_idx, val_idx\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MONOTONIC CONSTRAINTS BUILDER\n",
    "# ===============================================\n",
    "\n",
    "def build_monotonic_constraints(feature_cols: list) -> list:\n",
    "    \"\"\"Construit le vecteur de contraintes monotones.\"\"\"\n",
    "    \n",
    "    constraints = []\n",
    "    n_constrained = 0\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if col in MONOTONIC_FEATURES:\n",
    "            constraints.append(MONOTONIC_FEATURES[col])\n",
    "            n_constrained += 1\n",
    "        else:\n",
    "            constraints.append(0)\n",
    "    \n",
    "    print(f\"  Monotonic constraints: {n_constrained}/{len(feature_cols)} features\")\n",
    "    \n",
    "    return constraints\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# OPTUNA OBJECTIVES\n",
    "# ===============================================\n",
    "\n",
    "def create_lgbm_objective(X, y, cv, monotonic_constraints):\n",
    "    \"\"\"CrÃ©e l'objectif Optuna pour LightGBM.\"\"\"\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.metrics import log_loss, roc_auc_score\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": \"binary_logloss\",\n",
    "            \"verbosity\": -1,\n",
    "            \"force_row_wise\": True,\n",
    "            \"random_state\": RANDOM_SEED,\n",
    "            \"device\": \"gpu\",\n",
    "            \"gpu_platform_id\": 0,\n",
    "            \"gpu_device_id\": 0,\n",
    "            \n",
    "            # HyperparamÃ¨tres Ã  optimiser\n",
    "            \"n_estimators\": MAX_BOOST_ROUNDS,\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.1, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 100, 2000),\n",
    "            \"min_sum_hessian_in_leaf\": trial.suggest_float(\"min_sum_hessian_in_leaf\", 1.0, 100.0),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 100.0, log=True),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 0.95),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 0.95),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"max_bin\": trial.suggest_int(\"max_bin\", 63, 255),\n",
    "            \n",
    "            # DART mode (optionnel)\n",
    "            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),\n",
    "        }\n",
    "        \n",
    "        # DART specific params\n",
    "        if params[\"boosting_type\"] == \"dart\":\n",
    "            params[\"drop_rate\"] = trial.suggest_float(\"drop_rate\", 0.01, 0.3)\n",
    "            params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 0.3, 0.7)\n",
    "        \n",
    "        # Monotonic constraints\n",
    "        if monotonic_constraints:\n",
    "            params[\"monotone_constraints\"] = monotonic_constraints\n",
    "        \n",
    "        # Cross-validation\n",
    "        scores_ll = []\n",
    "        scores_auc = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Train\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            \n",
    "            callbacks = [\n",
    "                lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False),\n",
    "            ]\n",
    "            \n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[val_data],\n",
    "                callbacks=callbacks,\n",
    "            )\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "            \n",
    "            ll = log_loss(y_val, y_pred)\n",
    "            auc = roc_auc_score(y_val, y_pred)\n",
    "            \n",
    "            scores_ll.append(ll)\n",
    "            scores_auc.append(auc)\n",
    "            \n",
    "            # Pruning\n",
    "            trial.report(ll, fold_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        mean_ll = np.mean(scores_ll)\n",
    "        mean_auc = np.mean(scores_auc)\n",
    "        \n",
    "        # Store AUC as user attribute\n",
    "        trial.set_user_attr(\"auc\", mean_auc)\n",
    "        trial.set_user_attr(\"std_ll\", np.std(scores_ll))\n",
    "        \n",
    "        return mean_ll\n",
    "    \n",
    "    return objective\n",
    "\n",
    "\n",
    "def create_xgb_objective(X, y, cv, monotonic_constraints):\n",
    "    \"\"\"CrÃ©e l'objectif Optuna pour XGBoost.\"\"\"\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import log_loss, roc_auc_score\n",
    "    \n",
    "    # Convert constraints to XGB format (string)\n",
    "    if monotonic_constraints:\n",
    "        mc_str = \"(\" + \",\".join(str(c) for c in monotonic_constraints) + \")\"\n",
    "    else:\n",
    "        mc_str = None\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"tree_method\": \"gpu_hist\",\n",
    "            \"gpu_id\": 0,\n",
    "            \"random_state\": RANDOM_SEED,\n",
    "            \"verbosity\": 0,\n",
    "            \n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.005, 0.1, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 12),\n",
    "            \"min_child_weight\": trial.suggest_float(\"xgb_min_child_weight\", 1, 100),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 0.95),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.4, 0.95),\n",
    "            \"reg_alpha\": trial.suggest_float(\"xgb_reg_alpha\", 1e-8, 10.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"xgb_reg_lambda\", 1e-8, 100.0, log=True),\n",
    "            \"gamma\": trial.suggest_float(\"xgb_gamma\", 1e-8, 5.0, log=True),\n",
    "            \"max_bin\": trial.suggest_int(\"xgb_max_bin\", 128, 512),\n",
    "        }\n",
    "        \n",
    "        if mc_str:\n",
    "            params[\"monotone_constraints\"] = mc_str\n",
    "        \n",
    "        scores_ll = []\n",
    "        scores_auc = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "            \n",
    "            model = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=MAX_BOOST_ROUNDS,\n",
    "                evals=[(dval, \"val\")],\n",
    "                early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                verbose_eval=False,\n",
    "            )\n",
    "            \n",
    "            y_pred = model.predict(dval)\n",
    "            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "            \n",
    "            ll = log_loss(y_val, y_pred)\n",
    "            auc = roc_auc_score(y_val, y_pred)\n",
    "            \n",
    "            scores_ll.append(ll)\n",
    "            scores_auc.append(auc)\n",
    "            \n",
    "            trial.report(ll, fold_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        mean_ll = np.mean(scores_ll)\n",
    "        trial.set_user_attr(\"auc\", np.mean(scores_auc))\n",
    "        \n",
    "        return mean_ll\n",
    "    \n",
    "    return objective\n",
    "\n",
    "\n",
    "def create_catboost_objective(X, y, cv, monotonic_constraints):\n",
    "    \"\"\"CrÃ©e l'objectif Optuna pour CatBoost.\"\"\"\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.metrics import log_loss, roc_auc_score\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"loss_function\": \"Logloss\",\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"verbose\": False,\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"devices\": \"0\",\n",
    "            \"iterations\": MAX_BOOST_ROUNDS,\n",
    "            \"early_stopping_rounds\": EARLY_STOPPING_ROUNDS,\n",
    "            \n",
    "            \"learning_rate\": trial.suggest_float(\"cat_learning_rate\", 0.005, 0.15, log=True),\n",
    "            \"depth\": trial.suggest_int(\"cat_depth\", 4, 10),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"cat_l2_leaf_reg\", 1.0, 100.0, log=True),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"cat_min_data_in_leaf\", 10, 500),\n",
    "            \"random_strength\": trial.suggest_float(\"cat_random_strength\", 0.1, 10.0),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"cat_bagging_temperature\", 0.0, 1.0),\n",
    "            \"border_count\": trial.suggest_int(\"cat_border_count\", 32, 255),\n",
    "        }\n",
    "        \n",
    "        scores_ll = []\n",
    "        scores_auc = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            model = CatBoostClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "            \n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "            \n",
    "            ll = log_loss(y_val, y_pred)\n",
    "            auc = roc_auc_score(y_val, y_pred)\n",
    "            \n",
    "            scores_ll.append(ll)\n",
    "            scores_auc.append(auc)\n",
    "            \n",
    "            trial.report(ll, fold_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        mean_ll = np.mean(scores_ll)\n",
    "        trial.set_user_attr(\"auc\", np.mean(scores_auc))\n",
    "        \n",
    "        return mean_ll\n",
    "    \n",
    "    return objective\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# RUN HPO\n",
    "# ===============================================\n",
    "\n",
    "def run_hpo(X, y, feature_cols):\n",
    "    \"\"\"ExÃ©cute l'optimisation Optuna pour tous les modÃ¨les.\"\"\"\n",
    "    import optuna\n",
    "    from optuna.pruners import MedianPruner\n",
    "    from optuna.samplers import TPESampler\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   HYPERPARAMETER OPTIMIZATION - GOD MODE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Setup CV\n",
    "    cv = PurgedRollingCV(n_splits=N_FOLDS, purge_pct=0.01, embargo_pct=0.005)\n",
    "    print(f\"\\n  CV: {N_FOLDS}-fold Purged Rolling CV\")\n",
    "    print(f\"  Trials per model: {N_TRIALS}\")\n",
    "    \n",
    "    # Build monotonic constraints\n",
    "    monotonic_constraints = build_monotonic_constraints(feature_cols)\n",
    "    \n",
    "    # Results storage\n",
    "    best_params = {}\n",
    "    best_scores = {}\n",
    "    \n",
    "    # ===== LGBM =====\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"  ðŸŒ² LightGBM HPO\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    study_lgbm = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(seed=RANDOM_SEED),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2),\n",
    "        study_name=\"lgbm_hpo\"\n",
    "    )\n",
    "    \n",
    "    objective_lgbm = create_lgbm_objective(X, y, cv, monotonic_constraints)\n",
    "    \n",
    "    study_lgbm.optimize(\n",
    "        objective_lgbm,\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        gc_after_trial=True,\n",
    "    )\n",
    "    \n",
    "    best_params[\"lgbm\"] = study_lgbm.best_params\n",
    "    best_scores[\"lgbm\"] = {\n",
    "        \"logloss\": study_lgbm.best_value,\n",
    "        \"auc\": study_lgbm.best_trial.user_attrs.get(\"auc\", 0),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  âœ… LGBM Best LogLoss: {study_lgbm.best_value:.4f}\")\n",
    "    print(f\"     Best AUC: {best_scores['lgbm']['auc']:.4f}\")\n",
    "    \n",
    "    # ===== XGBoost =====\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"  ðŸš€ XGBoost HPO\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    study_xgb = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(seed=RANDOM_SEED),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2),\n",
    "        study_name=\"xgb_hpo\"\n",
    "    )\n",
    "    \n",
    "    objective_xgb = create_xgb_objective(X, y, cv, monotonic_constraints)\n",
    "    \n",
    "    study_xgb.optimize(\n",
    "        objective_xgb,\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        gc_after_trial=True,\n",
    "    )\n",
    "    \n",
    "    best_params[\"xgb\"] = study_xgb.best_params\n",
    "    best_scores[\"xgb\"] = {\n",
    "        \"logloss\": study_xgb.best_value,\n",
    "        \"auc\": study_xgb.best_trial.user_attrs.get(\"auc\", 0),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  âœ… XGB Best LogLoss: {study_xgb.best_value:.4f}\")\n",
    "    print(f\"     Best AUC: {best_scores['xgb']['auc']:.4f}\")\n",
    "    \n",
    "    # ===== CatBoost =====\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(\"  ðŸ± CatBoost HPO\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    study_cat = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(seed=RANDOM_SEED),\n",
    "        pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2),\n",
    "        study_name=\"cat_hpo\"\n",
    "    )\n",
    "    \n",
    "    objective_cat = create_catboost_objective(X, y, cv, monotonic_constraints)\n",
    "    \n",
    "    study_cat.optimize(\n",
    "        objective_cat,\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        gc_after_trial=True,\n",
    "    )\n",
    "    \n",
    "    best_params[\"cat\"] = study_cat.best_params\n",
    "    best_scores[\"cat\"] = {\n",
    "        \"logloss\": study_cat.best_value,\n",
    "        \"auc\": study_cat.best_trial.user_attrs.get(\"auc\", 0),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n  âœ… CatBoost Best LogLoss: {study_cat.best_value:.4f}\")\n",
    "    print(f\"     Best AUC: {best_scores['cat']['auc']:.4f}\")\n",
    "    \n",
    "    return best_params, best_scores, {\n",
    "        \"lgbm\": study_lgbm,\n",
    "        \"xgb\": study_xgb,\n",
    "        \"cat\": study_cat,\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# FINAL TRAINING WITH BEST PARAMS\n",
    "# ===============================================\n",
    "\n",
    "def train_final_models(X_train, y_train, X_val, y_val, X_test, y_test, \n",
    "                       feature_cols, best_params):\n",
    "    \"\"\"EntraÃ®ne les modÃ¨les finaux avec les meilleurs hyperparamÃ¨tres.\"\"\"\n",
    "    import lightgbm as lgb\n",
    "    import xgboost as xgb\n",
    "    from catboost import CatBoostClassifier\n",
    "    from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   FINAL TRAINING WITH OPTIMIZED PARAMS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    monotonic_constraints = build_monotonic_constraints(feature_cols)\n",
    "    \n",
    "    models = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    # ===== LGBM =====\n",
    "    print(\"\\n  Training LightGBM (optimized)...\")\n",
    "    \n",
    "    lgbm_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"n_estimators\": MAX_BOOST_ROUNDS,\n",
    "        \"verbosity\": -1,\n",
    "        \"force_row_wise\": True,\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "        \"device\": \"gpu\",\n",
    "        \"gpu_platform_id\": 0,\n",
    "        \"gpu_device_id\": 0,\n",
    "        \"monotone_constraints\": monotonic_constraints,\n",
    "        **{k: v for k, v in best_params[\"lgbm\"].items() if not k.startswith(\"xgb_\") and not k.startswith(\"cat_\")}\n",
    "    }\n",
    "    \n",
    "    # Handle DART params\n",
    "    if lgbm_params.get(\"boosting_type\") != \"dart\":\n",
    "        lgbm_params.pop(\"drop_rate\", None)\n",
    "        lgbm_params.pop(\"skip_drop\", None)\n",
    "    \n",
    "    lgbm_model = lgb.LGBMClassifier(**lgbm_params)\n",
    "    lgbm_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"logloss\",\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    models[\"lgbm\"] = lgbm_model\n",
    "    predictions[\"lgbm\"] = {\n",
    "        \"val\": lgbm_model.predict_proba(X_val)[:, 1],\n",
    "        \"test\": lgbm_model.predict_proba(X_test)[:, 1],\n",
    "    }\n",
    "    \n",
    "    # ===== XGBoost =====\n",
    "    print(\"  Training XGBoost (optimized)...\")\n",
    "    \n",
    "    xgb_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"tree_method\": \"gpu_hist\",\n",
    "        \"gpu_id\": 0,\n",
    "        \"random_state\": RANDOM_SEED,\n",
    "        **{k.replace(\"xgb_\", \"\"): v for k, v in best_params[\"xgb\"].items() if k.startswith(\"xgb_\")}\n",
    "    }\n",
    "    \n",
    "    # Add monotonic\n",
    "    if monotonic_constraints:\n",
    "        xgb_params[\"monotone_constraints\"] = \"(\" + \",\".join(str(c) for c in monotonic_constraints) + \")\"\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    \n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=MAX_BOOST_ROUNDS,\n",
    "        evals=[(dval, \"val\")],\n",
    "        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "    \n",
    "    models[\"xgb\"] = xgb_model\n",
    "    predictions[\"xgb\"] = {\n",
    "        \"val\": xgb_model.predict(dval),\n",
    "        \"test\": xgb_model.predict(dtest),\n",
    "    }\n",
    "    \n",
    "    # ===== CatBoost =====\n",
    "    print(\"  Training CatBoost (optimized)...\")\n",
    "    \n",
    "    cat_params = {\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "        \"task_type\": \"GPU\",\n",
    "        \"devices\": \"0\",\n",
    "        \"iterations\": MAX_BOOST_ROUNDS,\n",
    "        \"early_stopping_rounds\": EARLY_STOPPING_ROUNDS,\n",
    "        **{k.replace(\"cat_\", \"\"): v for k, v in best_params[\"cat\"].items() if k.startswith(\"cat_\")}\n",
    "    }\n",
    "    \n",
    "    cat_model = CatBoostClassifier(**cat_params)\n",
    "    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "    \n",
    "    models[\"cat\"] = cat_model\n",
    "    predictions[\"cat\"] = {\n",
    "        \"val\": cat_model.predict_proba(X_val)[:, 1],\n",
    "        \"test\": cat_model.predict_proba(X_test)[:, 1],\n",
    "    }\n",
    "    \n",
    "    # ===== Stacking =====\n",
    "    print(\"\\n  Building meta-learner...\")\n",
    "    \n",
    "    M_val = np.column_stack([predictions[m][\"val\"] for m in [\"lgbm\", \"xgb\", \"cat\"]])\n",
    "    M_test = np.column_stack([predictions[m][\"test\"] for m in [\"lgbm\", \"xgb\", \"cat\"]])\n",
    "    \n",
    "    meta_model = LogisticRegression(C=1.0, max_iter=2000, solver=\"lbfgs\", random_state=RANDOM_SEED)\n",
    "    meta_model.fit(M_val, y_val)\n",
    "    \n",
    "    p_meta_val = meta_model.predict_proba(M_val)[:, 1]\n",
    "    p_meta_test = meta_model.predict_proba(M_test)[:, 1]\n",
    "    \n",
    "    models[\"meta\"] = meta_model\n",
    "    \n",
    "    # ===== Platt Calibration =====\n",
    "    print(\"  Applying Platt calibration...\")\n",
    "    \n",
    "    platt_model = LogisticRegression(C=1.0, max_iter=1000, solver=\"lbfgs\")\n",
    "    platt_model.fit(p_meta_val.reshape(-1, 1), y_val)\n",
    "    \n",
    "    p_final_test = platt_model.predict_proba(p_meta_test.reshape(-1, 1))[:, 1]\n",
    "    p_final_test = np.clip(p_final_test, 1e-5, 1 - 1e-5)\n",
    "    \n",
    "    models[\"platt\"] = platt_model\n",
    "    \n",
    "    # ===== Final Evaluation =====\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   ðŸ† FINAL RESULTS (TEST SET)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Individual models\n",
    "    print(\"\\n  Individual models:\")\n",
    "    for name in [\"lgbm\", \"xgb\", \"cat\"]:\n",
    "        p = predictions[name][\"test\"]\n",
    "        p = np.clip(p, 1e-7, 1 - 1e-7)\n",
    "        ll = log_loss(y_test, p)\n",
    "        auc = roc_auc_score(y_test, p)\n",
    "        print(f\"    {name.upper()}: LogLoss={ll:.4f}, AUC={auc:.4f}\")\n",
    "    \n",
    "    # Stack\n",
    "    ll_final = log_loss(y_test, p_final_test)\n",
    "    auc_final = roc_auc_score(y_test, p_final_test)\n",
    "    brier_final = brier_score_loss(y_test, p_final_test)\n",
    "    \n",
    "    print(f\"\\n  ðŸ“Š STACK FINAL (after Platt):\")\n",
    "    print(f\"     LogLoss: {ll_final:.4f}\")\n",
    "    print(f\"     AUC:     {auc_final:.4f}\")\n",
    "    print(f\"     Brier:   {brier_final:.4f}\")\n",
    "    \n",
    "    return models, p_final_test, {\n",
    "        \"test_logloss\": ll_final,\n",
    "        \"test_auc\": auc_final,\n",
    "        \"test_brier\": brier_final,\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SAVE RESULTS\n",
    "# ===============================================\n",
    "\n",
    "def save_results(best_params, best_scores, models, final_metrics, feature_cols):\n",
    "    \"\"\"Sauvegarde tous les rÃ©sultats.\"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   SAVE RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Save best params\n",
    "    params_path = OUT_DIR / \"best_params.json\"\n",
    "    with open(params_path, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    print(f\"  âœ… Best params: {params_path}\")\n",
    "    \n",
    "    # Save HPO scores\n",
    "    scores_path = OUT_DIR / \"hpo_scores.json\"\n",
    "    with open(scores_path, \"w\") as f:\n",
    "        json.dump(best_scores, f, indent=2)\n",
    "    print(f\"  âœ… HPO scores: {scores_path}\")\n",
    "    \n",
    "    # Save models\n",
    "    models_path = OUT_DIR / \"models_optimized.joblib\"\n",
    "    joblib.dump({\n",
    "        \"lgbm\": models[\"lgbm\"],\n",
    "        \"xgb\": models[\"xgb\"],\n",
    "        \"cat\": models[\"cat\"],\n",
    "        \"meta\": models[\"meta\"],\n",
    "        \"platt\": models[\"platt\"],\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"metrics\": final_metrics,\n",
    "        \"created\": datetime.now().isoformat(),\n",
    "    }, models_path)\n",
    "    print(f\"  âœ… Models: {models_path}\")\n",
    "    \n",
    "    # Save final metrics\n",
    "    metrics_path = OUT_DIR / \"final_metrics.json\"\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(final_metrics, f, indent=2)\n",
    "    print(f\"  âœ… Metrics: {metrics_path}\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN\n",
    "# ===============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline HPO complet.\"\"\"\n",
    "    import optuna\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   ðŸš€ HPO OPTUNA - GOD MODE SOTA 2026 ðŸš€\")\n",
    "    print(\"   TennisTitan - Ultimate Hyperparameter Optimization\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"   Trials: {N_TRIALS} per model\")\n",
    "    print(f\"   CV: {N_FOLDS}-fold Purged Rolling\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    # Load data\n",
    "    combined, test, feature_cols = load_data()\n",
    "    \n",
    "    # Prepare CV data\n",
    "    X_cv, y_cv, years_cv, feature_cols = prepare_cv_data(combined, feature_cols)\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test = test.select(feature_cols).to_numpy().astype(np.float32)\n",
    "    y_test = test[\"target_A_wins\"].to_numpy().astype(np.int32)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "    \n",
    "    del combined, test\n",
    "    gc.collect()\n",
    "    \n",
    "    # Run HPO\n",
    "    best_params, best_scores, studies = run_hpo(X_cv, y_cv, feature_cols)\n",
    "    \n",
    "    # Split for final training (70/15/15 from CV data)\n",
    "    n = len(X_cv)\n",
    "    train_end = int(n * 0.82)  # ~70% of original train+val\n",
    "    \n",
    "    X_train = X_cv[:train_end]\n",
    "    y_train = y_cv[:train_end]\n",
    "    X_val = X_cv[train_end:]\n",
    "    y_val = y_cv[train_end:]\n",
    "    \n",
    "    print(f\"\\n  Final split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "    \n",
    "    # Final training\n",
    "    models, predictions, final_metrics = train_final_models(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        feature_cols, best_params\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    save_results(best_params, best_scores, models, final_metrics, feature_cols)\n",
    "    \n",
    "    elapsed = time.perf_counter() - t0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   âœ… HPO GOD MODE COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\n  â±ï¸  Total time: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"\\n  ðŸ“Š Best Results:\")\n",
    "    print(f\"     LGBM CV LogLoss: {best_scores['lgbm']['logloss']:.4f}\")\n",
    "    print(f\"     XGB CV LogLoss:  {best_scores['xgb']['logloss']:.4f}\")\n",
    "    print(f\"     CAT CV LogLoss:  {best_scores['cat']['logloss']:.4f}\")\n",
    "    print(f\"\\n     Final Test LogLoss: {final_metrics['test_logloss']:.4f}\")\n",
    "    print(f\"     Final Test AUC:     {final_metrics['test_auc']:.4f}\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "ðŸ“‹ NEXT STEPS:\n",
    "\n",
    "1. Compare with baseline:\n",
    "   - Baseline: LogLoss=0.51, AUC=0.825 (avec odds)\n",
    "   - HPO:      LogLoss=???, AUC=???\n",
    "\n",
    "2. If improvement, update production model\n",
    "\n",
    "3. Script 3/3: Monte Carlo simulation for score prediction\n",
    "\"\"\")\n",
    "    \n",
    "    return best_params, best_scores, final_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_params, best_scores, final_metrics = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c4896f-4461-44f2-8511-cf8d6a61d6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best params LGBM & XGB rÃ©cupÃ©rÃ©s depuis les logs\n",
      "   LGBM: LogLoss=0.5324\n",
      "   XGB:  LogLoss=0.5317\n",
      "\n",
      "======================================================================\n",
      "   LOAD DATA\n",
      "======================================================================\n",
      "\n",
      "  Train: (388973, 209)\n",
      "  Val: (64787, 209)\n",
      "  Test (holdout): (89706, 209)\n",
      "  Combined for CV: (453760, 209)\n",
      "  Features: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 19:32:42,943] A new study created in memory with name: cat_hpo_fixed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "  ðŸ± CatBoost HPO (FIXED - No Monotonic Constraints)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2871923d4e42d3ab379c8d94dda7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-06 19:36:29,488] Trial 0 finished with value: 0.5334419884376422 and parameters: {'cat_learning_rate': 0.01787356461300122, 'cat_depth': 10, 'cat_l2_leaf_reg': 29.10635913133069, 'cat_min_data_in_leaf': 303, 'cat_random_strength': 1.6445845403801216, 'cat_bagging_temperature': 0.15599452033620265, 'cat_border_count': 45}. Best is trial 0 with value: 0.5334419884376422.\n",
      "[I 2025-12-06 19:37:29,879] Trial 1 finished with value: 0.5339651876386668 and parameters: {'cat_learning_rate': 0.09515184190867908, 'cat_depth': 8, 'cat_l2_leaf_reg': 26.070247583707673, 'cat_min_data_in_leaf': 20, 'cat_random_strength': 9.702107536403744, 'cat_bagging_temperature': 0.8324426408004217, 'cat_border_count': 79}. Best is trial 0 with value: 0.5334419884376422.\n",
      "[I 2025-12-06 19:38:45,606] Trial 2 finished with value: 0.5471712790098293 and parameters: {'cat_learning_rate': 0.009279990423245293, 'cat_depth': 5, 'cat_l2_leaf_reg': 4.059611610484305, 'cat_min_data_in_leaf': 267, 'cat_random_strength': 4.376255684556946, 'cat_bagging_temperature': 0.2912291401980419, 'cat_border_count': 169}. Best is trial 0 with value: 0.5334419884376422.\n",
      "[I 2025-12-06 19:40:14,429] Trial 3 finished with value: 0.5459712800750328 and parameters: {'cat_learning_rate': 0.008035619256019939, 'cat_depth': 6, 'cat_l2_leaf_reg': 5.404103854647328, 'cat_min_data_in_leaf': 233, 'cat_random_strength': 7.873242017790835, 'cat_bagging_temperature': 0.19967378215835974, 'cat_border_count': 147}. Best is trial 0 with value: 0.5334419884376422.\n",
      "[I 2025-12-06 19:41:18,461] Trial 4 finished with value: 0.5398644097987885 and parameters: {'cat_learning_rate': 0.037500594752084934, 'cat_depth': 4, 'cat_l2_leaf_reg': 16.40928673064792, 'cat_min_data_in_leaf': 93, 'cat_random_strength': 0.7440107705542672, 'cat_bagging_temperature': 0.9488855372533332, 'cat_border_count': 248}. Best is trial 0 with value: 0.5334419884376422.\n",
      "[I 2025-12-06 19:42:20,101] Trial 5 finished with value: 0.5333871865398832 and parameters: {'cat_learning_rate': 0.07817554354066729, 'cat_depth': 6, 'cat_l2_leaf_reg': 1.5679933916723008, 'cat_min_data_in_leaf': 345, 'cat_random_strength': 4.457509688022053, 'cat_bagging_temperature': 0.12203823484477883, 'cat_border_count': 142}. Best is trial 5 with value: 0.5333871865398832.\n",
      "[I 2025-12-06 19:46:15,438] Trial 6 pruned. \n",
      "[I 2025-12-06 19:50:44,400] Trial 7 pruned. \n",
      "[I 2025-12-06 19:51:41,238] Trial 8 pruned. \n",
      "[I 2025-12-06 19:52:37,404] Trial 9 pruned. \n",
      "[I 2025-12-06 19:53:27,648] Trial 10 finished with value: 0.5345773273174216 and parameters: {'cat_learning_rate': 0.13844301696546463, 'cat_depth': 8, 'cat_l2_leaf_reg': 86.9108948612498, 'cat_min_data_in_leaf': 437, 'cat_random_strength': 6.436876174428809, 'cat_bagging_temperature': 0.4064288599162643, 'cat_border_count': 105}. Best is trial 5 with value: 0.5333871865398832.\n",
      "[I 2025-12-06 19:54:45,046] Trial 11 finished with value: 0.533320713143361 and parameters: {'cat_learning_rate': 0.04378563413653758, 'cat_depth': 7, 'cat_l2_leaf_reg': 1.0321261785469564, 'cat_min_data_in_leaf': 362, 'cat_random_strength': 1.4974288486768805, 'cat_bagging_temperature': 0.02470462440613677, 'cat_border_count': 45}. Best is trial 11 with value: 0.533320713143361.\n",
      "[I 2025-12-06 19:56:04,804] Trial 12 finished with value: 0.5324196080438612 and parameters: {'cat_learning_rate': 0.052635179031765504, 'cat_depth': 7, 'cat_l2_leaf_reg': 1.0516709292666337, 'cat_min_data_in_leaf': 386, 'cat_random_strength': 2.3053602513332567, 'cat_bagging_temperature': 0.003731757323367113, 'cat_border_count': 99}. Best is trial 12 with value: 0.5324196080438612.\n",
      "[I 2025-12-06 19:57:33,747] Trial 13 finished with value: 0.5329913119670624 and parameters: {'cat_learning_rate': 0.043986529561967753, 'cat_depth': 8, 'cat_l2_leaf_reg': 1.1188717579685379, 'cat_min_data_in_leaf': 407, 'cat_random_strength': 2.3104089575951563, 'cat_bagging_temperature': 0.0030978777513035547, 'cat_border_count': 39}. Best is trial 12 with value: 0.5324196080438612.\n",
      "[I 2025-12-06 19:59:10,034] Trial 14 finished with value: 0.5325581366731986 and parameters: {'cat_learning_rate': 0.04629693351916816, 'cat_depth': 8, 'cat_l2_leaf_reg': 2.409569834005802, 'cat_min_data_in_leaf': 424, 'cat_random_strength': 2.5224140971849263, 'cat_bagging_temperature': 0.6634064286119825, 'cat_border_count': 85}. Best is trial 12 with value: 0.5324196080438612.\n",
      "[I 2025-12-06 20:00:24,838] Trial 15 finished with value: 0.5339532542354055 and parameters: {'cat_learning_rate': 0.06826302475003551, 'cat_depth': 9, 'cat_l2_leaf_reg': 2.269120663806072, 'cat_min_data_in_leaf': 496, 'cat_random_strength': 0.14315274433341507, 'cat_bagging_temperature': 0.6926302989931522, 'cat_border_count': 96}. Best is trial 12 with value: 0.5324196080438612.\n",
      "[I 2025-12-06 20:01:40,877] Trial 16 pruned. \n",
      "[I 2025-12-06 20:02:32,575] Trial 17 pruned. \n",
      "[I 2025-12-06 20:03:34,045] Trial 18 pruned. \n",
      "[I 2025-12-06 20:04:03,495] Trial 19 pruned. \n",
      "[I 2025-12-06 20:05:27,767] Trial 20 finished with value: 0.5322084646046468 and parameters: {'cat_learning_rate': 0.05239001333480895, 'cat_depth': 7, 'cat_l2_leaf_reg': 3.064531274546584, 'cat_min_data_in_leaf': 296, 'cat_random_strength': 2.1231339372844134, 'cat_bagging_temperature': 0.5818167223201713, 'cat_border_count': 72}. Best is trial 20 with value: 0.5322084646046468.\n",
      "[I 2025-12-06 20:06:51,258] Trial 21 finished with value: 0.5324295647922752 and parameters: {'cat_learning_rate': 0.05076401968893755, 'cat_depth': 7, 'cat_l2_leaf_reg': 1.6620112445980921, 'cat_min_data_in_leaf': 304, 'cat_random_strength': 2.0633181226301858, 'cat_bagging_temperature': 0.5994277017517573, 'cat_border_count': 71}. Best is trial 20 with value: 0.5322084646046468.\n",
      "[I 2025-12-06 20:08:21,108] Trial 22 pruned. \n",
      "[I 2025-12-06 20:09:00,729] Trial 23 pruned. \n",
      "[I 2025-12-06 20:09:36,240] Trial 24 pruned. \n",
      "[I 2025-12-06 20:10:03,394] Trial 25 pruned. \n",
      "[I 2025-12-06 20:10:53,217] Trial 26 pruned. \n",
      "[I 2025-12-06 20:12:22,848] Trial 27 pruned. \n",
      "[I 2025-12-06 20:13:41,230] Trial 28 finished with value: 0.5328451217547847 and parameters: {'cat_learning_rate': 0.05530523401924041, 'cat_depth': 7, 'cat_l2_leaf_reg': 1.0268582663689674, 'cat_min_data_in_leaf': 270, 'cat_random_strength': 2.7357233828986685, 'cat_bagging_temperature': 0.6127553969407205, 'cat_border_count': 80}. Best is trial 20 with value: 0.5322084646046468.\n",
      "[I 2025-12-06 20:14:21,548] Trial 29 pruned. \n",
      "[I 2025-12-06 20:15:13,319] Trial 30 pruned. \n",
      "[I 2025-12-06 20:16:58,740] Trial 31 finished with value: 0.5321474173825455 and parameters: {'cat_learning_rate': 0.04448610628716166, 'cat_depth': 8, 'cat_l2_leaf_reg': 2.7442458435670303, 'cat_min_data_in_leaf': 386, 'cat_random_strength': 2.5087001272268834, 'cat_bagging_temperature': 0.6854914735361015, 'cat_border_count': 82}. Best is trial 31 with value: 0.5321474173825455.\n",
      "[I 2025-12-06 20:17:34,093] Trial 32 pruned. \n",
      "[I 2025-12-06 20:18:36,511] Trial 33 pruned. \n",
      "[I 2025-12-06 20:19:05,382] Trial 34 pruned. \n",
      "[I 2025-12-06 20:21:06,772] Trial 35 finished with value: 0.5325493633550806 and parameters: {'cat_learning_rate': 0.05480587524524865, 'cat_depth': 9, 'cat_l2_leaf_reg': 7.279891844388863, 'cat_min_data_in_leaf': 226, 'cat_random_strength': 0.9981780348625782, 'cat_bagging_temperature': 0.7422004388217438, 'cat_border_count': 119}. Best is trial 31 with value: 0.5321474173825455.\n",
      "[I 2025-12-06 20:23:13,435] Trial 36 finished with value: 0.5320587437734926 and parameters: {'cat_learning_rate': 0.03885853654799112, 'cat_depth': 8, 'cat_l2_leaf_reg': 1.9941373519801435, 'cat_min_data_in_leaf': 280, 'cat_random_strength': 2.5185403221289824, 'cat_bagging_temperature': 0.8645440342049154, 'cat_border_count': 136}. Best is trial 36 with value: 0.5320587437734926.\n",
      "[I 2025-12-06 20:25:24,709] Trial 37 finished with value: 0.5319625947365009 and parameters: {'cat_learning_rate': 0.036559009585992494, 'cat_depth': 8, 'cat_l2_leaf_reg': 4.150859691832259, 'cat_min_data_in_leaf': 242, 'cat_random_strength': 3.450136591924119, 'cat_bagging_temperature': 0.8602949447023844, 'cat_border_count': 135}. Best is trial 37 with value: 0.5319625947365009.\n",
      "[I 2025-12-06 20:28:55,091] Trial 38 pruned. \n",
      "[I 2025-12-06 20:31:01,434] Trial 39 finished with value: 0.5320417138812198 and parameters: {'cat_learning_rate': 0.038126356290088176, 'cat_depth': 8, 'cat_l2_leaf_reg': 23.5826257719897, 'cat_min_data_in_leaf': 232, 'cat_random_strength': 4.971989520478293, 'cat_bagging_temperature': 0.8698323807570632, 'cat_border_count': 139}. Best is trial 37 with value: 0.5319625947365009.\n",
      "[I 2025-12-06 20:35:07,006] Trial 40 finished with value: 0.532413390085801 and parameters: {'cat_learning_rate': 0.03819953025319351, 'cat_depth': 10, 'cat_l2_leaf_reg': 23.445690099078863, 'cat_min_data_in_leaf': 236, 'cat_random_strength': 6.738823655315697, 'cat_bagging_temperature': 0.8681595058186854, 'cat_border_count': 137}. Best is trial 37 with value: 0.5319625947365009.\n",
      "[I 2025-12-06 20:36:50,673] Trial 41 pruned. \n",
      "[I 2025-12-06 20:38:36,306] Trial 42 pruned. \n",
      "[I 2025-12-06 20:41:33,791] Trial 43 finished with value: 0.5320704721164558 and parameters: {'cat_learning_rate': 0.041166238655473156, 'cat_depth': 9, 'cat_l2_leaf_reg': 65.97347630522098, 'cat_min_data_in_leaf': 151, 'cat_random_strength': 4.464981239630541, 'cat_bagging_temperature': 0.8448061377627922, 'cat_border_count': 132}. Best is trial 37 with value: 0.5319625947365009.\n",
      "[I 2025-12-06 20:44:31,579] Trial 44 finished with value: 0.531855934380552 and parameters: {'cat_learning_rate': 0.04151826270243284, 'cat_depth': 9, 'cat_l2_leaf_reg': 71.48852755927336, 'cat_min_data_in_leaf': 114, 'cat_random_strength': 4.431746445037019, 'cat_bagging_temperature': 0.8354457253916076, 'cat_border_count': 134}. Best is trial 44 with value: 0.531855934380552.\n",
      "[I 2025-12-06 20:47:36,380] Trial 45 finished with value: 0.532145687130977 and parameters: {'cat_learning_rate': 0.0402212623733999, 'cat_depth': 9, 'cat_l2_leaf_reg': 83.04230952308114, 'cat_min_data_in_leaf': 57, 'cat_random_strength': 5.383620247207972, 'cat_bagging_temperature': 0.8378450432210565, 'cat_border_count': 137}. Best is trial 44 with value: 0.531855934380552.\n",
      "[I 2025-12-06 20:52:51,140] Trial 46 pruned. \n",
      "[I 2025-12-06 20:55:24,686] Trial 47 pruned. \n",
      "[I 2025-12-06 20:56:47,630] Trial 48 pruned. \n",
      "[I 2025-12-06 21:00:34,575] Trial 49 pruned. \n",
      "\n",
      "  âœ… CatBoost Best LogLoss: 0.5319\n",
      "     Best AUC: 0.8051\n",
      "\n",
      "======================================================================\n",
      "   FINAL TRAINING WITH OPTIMIZED PARAMS\n",
      "======================================================================\n",
      "\n",
      "  Final split: Train=372083, Val=81677, Test=89706\n",
      "\n",
      "  Training LightGBM (optimized)...\n",
      "  Training XGBoost (optimized)...\n",
      "  Training CatBoost (optimized)...\n",
      "\n",
      "  Building meta-learner...\n",
      "  Applying Platt calibration...\n",
      "\n",
      "======================================================================\n",
      "   ðŸ† FINAL RESULTS (TEST SET)\n",
      "======================================================================\n",
      "\n",
      "  Individual models:\n",
      "    LGBM: LogLoss=0.5392, AUC=0.7974\n",
      "    XGB: LogLoss=0.5508, AUC=0.7865\n",
      "    CAT: LogLoss=0.5374, AUC=0.7998\n",
      "\n",
      "  ðŸ“Š STACK FINAL (after Platt):\n",
      "     LogLoss: 0.5466\n",
      "     AUC:     0.7999\n",
      "     Brier:   0.1844\n",
      "\n",
      "======================================================================\n",
      "   SAVE RESULTS\n",
      "======================================================================\n",
      "  âœ… Best params: C:\\Users\\Administrateur\\Tennis POLAR\\models\\hpo_sota_2026\\best_params.json\n",
      "  âœ… HPO scores: C:\\Users\\Administrateur\\Tennis POLAR\\models\\hpo_sota_2026\\hpo_scores.json\n",
      "  âœ… Models: C:\\Users\\Administrateur\\Tennis POLAR\\models\\hpo_sota_2026\\models_optimized.joblib\n",
      "  âœ… Metrics: C:\\Users\\Administrateur\\Tennis POLAR\\models\\hpo_sota_2026\\final_metrics.json\n",
      "\n",
      "======================================================================\n",
      "   âœ… HPO GOD MODE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "  ðŸ“Š CV Results:\n",
      "     LGBM: 0.5324\n",
      "     XGB:  0.5317\n",
      "     CAT:  0.5319\n",
      "\n",
      "  ðŸ† Final Test: LogLoss=0.5466, AUC=0.7999\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# RECOVERY + CATBOOST HPO + FINAL TRAINING\n",
    "# ===============================================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ===============================================\n",
    "# CONFIG\n",
    "# ===============================================\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / \"data_clean\" / \"ml_final\"\n",
    "OUT_DIR = ROOT / \"models\" / \"hpo_sota_2026\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "N_TRIALS = 50\n",
    "N_FOLDS = 5\n",
    "EARLY_STOPPING_ROUNDS = 150\n",
    "MAX_BOOST_ROUNDS = 3000\n",
    "\n",
    "ID_COLS = [\n",
    "    \"custom_match_id\", \"match_id_ta_dedup\", \"match_id_ta_source\",\n",
    "    \"winner_id\", \"loser_id\", \"tourney_name_ta\", \"tourney_slug_ta\",\n",
    "]\n",
    "\n",
    "# ===============================================\n",
    "# BEST PARAMS LGBM & XGB (depuis tes logs)\n",
    "# ===============================================\n",
    "best_params = {\n",
    "    \"lgbm\": {\n",
    "        \"learning_rate\": 0.015246242097511301,\n",
    "        \"num_leaves\": 104,\n",
    "        \"max_depth\": 12,\n",
    "        \"min_data_in_leaf\": 221,\n",
    "        \"min_sum_hessian_in_leaf\": 4.123446298067507,\n",
    "        \"lambda_l1\": 4.01046681841633e-06,\n",
    "        \"lambda_l2\": 3.497419016525011e-05,\n",
    "        \"feature_fraction\": 0.5075493418267852,\n",
    "        \"bagging_fraction\": 0.9484194813591086,\n",
    "        \"bagging_freq\": 6,\n",
    "        \"max_bin\": 236,\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    },\n",
    "    \"xgb\": {\n",
    "        \"xgb_learning_rate\": 0.017814381567323113,\n",
    "        \"xgb_max_depth\": 12,\n",
    "        \"xgb_min_child_weight\": 57.086892554766564,\n",
    "        \"xgb_subsample\": 0.9489747010989122,\n",
    "        \"xgb_colsample_bytree\": 0.7142541780295959,\n",
    "        \"xgb_reg_alpha\": 0.09751582447236345,\n",
    "        \"xgb_reg_lambda\": 0.0015388616311083133,\n",
    "        \"xgb_gamma\": 5.206735421827751e-06,\n",
    "        \"xgb_max_bin\": 274\n",
    "    }\n",
    "}\n",
    "\n",
    "best_scores = {\n",
    "    \"lgbm\": {\"logloss\": 0.5324, \"auc\": 0.8044},\n",
    "    \"xgb\": {\"logloss\": 0.5317, \"auc\": 0.8051},\n",
    "}\n",
    "\n",
    "print(\"âœ… Best params LGBM & XGB rÃ©cupÃ©rÃ©s depuis les logs\")\n",
    "print(f\"   LGBM: LogLoss={best_scores['lgbm']['logloss']:.4f}\")\n",
    "print(f\"   XGB:  LogLoss={best_scores['xgb']['logloss']:.4f}\")\n",
    "\n",
    "# ===============================================\n",
    "# LOAD DATA\n",
    "# ===============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"   LOAD DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "train = pl.read_parquet(DATA_DIR / \"train.parquet\")\n",
    "val = pl.read_parquet(DATA_DIR / \"val.parquet\")\n",
    "test = pl.read_parquet(DATA_DIR / \"test.parquet\")\n",
    "\n",
    "combined = pl.concat([train, val])\n",
    "\n",
    "print(f\"\\n  Train: {train.shape}\")\n",
    "print(f\"  Val: {val.shape}\")\n",
    "print(f\"  Test (holdout): {test.shape}\")\n",
    "print(f\"  Combined for CV: {combined.shape}\")\n",
    "\n",
    "# Load feature list\n",
    "feature_list_path = DATA_DIR / \"feature_list.json\"\n",
    "if feature_list_path.exists():\n",
    "    with open(feature_list_path) as f:\n",
    "        feature_cols = json.load(f)\n",
    "else:\n",
    "    exclude = [\"target_A_wins\", \"year\"] + ID_COLS\n",
    "    feature_cols = [c for c in train.columns if c not in exclude]\n",
    "\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = [c for c in feature_cols if c in combined.columns]\n",
    "X_cv = combined.select(feature_cols).to_numpy().astype(np.float32)\n",
    "y_cv = combined[\"target_A_wins\"].to_numpy().astype(np.int32)\n",
    "X_cv = np.nan_to_num(X_cv, nan=0.0)\n",
    "\n",
    "X_test = test.select(feature_cols).to_numpy().astype(np.float32)\n",
    "y_test = test[\"target_A_wins\"].to_numpy().astype(np.int32)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "del train, val, combined\n",
    "gc.collect()\n",
    "\n",
    "# ===============================================\n",
    "# PURGED ROLLING CV\n",
    "# ===============================================\n",
    "class PurgedRollingCV:\n",
    "    def __init__(self, n_splits=5, purge_pct=0.01, embargo_pct=0.005):\n",
    "        self.n_splits = n_splits\n",
    "        self.purge_pct = purge_pct\n",
    "        self.embargo_pct = embargo_pct\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        fold_size = n // (self.n_splits + 1)\n",
    "        purge_size = int(n * self.purge_pct)\n",
    "        embargo_size = int(n * self.embargo_pct)\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            train_end = fold_size * (i + 1)\n",
    "            val_start = train_end + purge_size\n",
    "            val_end = val_start + fold_size\n",
    "            \n",
    "            if val_end + embargo_size > n:\n",
    "                break\n",
    "            \n",
    "            train_idx = np.arange(0, train_end)\n",
    "            val_idx = np.arange(val_start, val_end)\n",
    "            \n",
    "            yield train_idx, val_idx\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "cv = PurgedRollingCV(n_splits=N_FOLDS, purge_pct=0.01, embargo_pct=0.005)\n",
    "\n",
    "# ===============================================\n",
    "# CATBOOST HPO (No Monotonic Constraints)\n",
    "# ===============================================\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"  ðŸ± CatBoost HPO (FIXED - No Monotonic Constraints)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def create_catboost_objective_fixed(X, y, cv):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"loss_function\": \"Logloss\",\n",
    "            \"random_seed\": RANDOM_SEED,\n",
    "            \"verbose\": False,\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"devices\": \"0\",\n",
    "            \"iterations\": MAX_BOOST_ROUNDS,\n",
    "            \"early_stopping_rounds\": EARLY_STOPPING_ROUNDS,\n",
    "            \n",
    "            \"learning_rate\": trial.suggest_float(\"cat_learning_rate\", 0.005, 0.15, log=True),\n",
    "            \"depth\": trial.suggest_int(\"cat_depth\", 4, 10),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"cat_l2_leaf_reg\", 1.0, 100.0, log=True),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"cat_min_data_in_leaf\", 10, 500),\n",
    "            \"random_strength\": trial.suggest_float(\"cat_random_strength\", 0.1, 10.0),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"cat_bagging_temperature\", 0.0, 1.0),\n",
    "            \"border_count\": trial.suggest_int(\"cat_border_count\", 32, 255),\n",
    "        }\n",
    "        \n",
    "        scores_ll = []\n",
    "        scores_auc = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            model = CatBoostClassifier(**params)\n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "            \n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "            y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "            \n",
    "            ll = log_loss(y_val, y_pred)\n",
    "            auc = roc_auc_score(y_val, y_pred)\n",
    "            \n",
    "            scores_ll.append(ll)\n",
    "            scores_auc.append(auc)\n",
    "            \n",
    "            trial.report(ll, fold_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        mean_ll = np.mean(scores_ll)\n",
    "        trial.set_user_attr(\"auc\", np.mean(scores_auc))\n",
    "        \n",
    "        return mean_ll\n",
    "    \n",
    "    return objective\n",
    "\n",
    "study_cat = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=TPESampler(seed=RANDOM_SEED),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=2),\n",
    "    study_name=\"cat_hpo_fixed\"\n",
    ")\n",
    "\n",
    "objective_cat = create_catboost_objective_fixed(X_cv, y_cv, cv)\n",
    "\n",
    "study_cat.optimize(\n",
    "    objective_cat,\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True,\n",
    "    gc_after_trial=True,\n",
    ")\n",
    "\n",
    "best_params[\"cat\"] = study_cat.best_params\n",
    "best_scores[\"cat\"] = {\n",
    "    \"logloss\": study_cat.best_value,\n",
    "    \"auc\": study_cat.best_trial.user_attrs.get(\"auc\", 0),\n",
    "}\n",
    "\n",
    "print(f\"\\n  âœ… CatBoost Best LogLoss: {study_cat.best_value:.4f}\")\n",
    "print(f\"     Best AUC: {best_scores['cat']['auc']:.4f}\")\n",
    "\n",
    "# ===============================================\n",
    "# FINAL TRAINING\n",
    "# ===============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"   FINAL TRAINING WITH OPTIMIZED PARAMS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split final\n",
    "n = len(X_cv)\n",
    "train_end = int(n * 0.82)\n",
    "\n",
    "X_train = X_cv[:train_end]\n",
    "y_train = y_cv[:train_end]\n",
    "X_val = X_cv[train_end:]\n",
    "y_val = y_cv[train_end:]\n",
    "\n",
    "print(f\"\\n  Final split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "# ===== LGBM =====\n",
    "print(\"\\n  Training LightGBM (optimized)...\")\n",
    "\n",
    "lgbm_params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"n_estimators\": MAX_BOOST_ROUNDS,\n",
    "    \"verbosity\": -1,\n",
    "    \"force_row_wise\": True,\n",
    "    \"random_state\": RANDOM_SEED,\n",
    "    **best_params[\"lgbm\"]\n",
    "}\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(**lgbm_params)\n",
    "lgbm_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"logloss\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)]\n",
    ")\n",
    "\n",
    "models[\"lgbm\"] = lgbm_model\n",
    "predictions[\"lgbm\"] = {\n",
    "    \"val\": lgbm_model.predict_proba(X_val)[:, 1],\n",
    "    \"test\": lgbm_model.predict_proba(X_test)[:, 1],\n",
    "}\n",
    "\n",
    "# ===== XGBoost =====\n",
    "print(\"  Training XGBoost (optimized)...\")\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"gpu_id\": 0,\n",
    "    \"random_state\": RANDOM_SEED,\n",
    "    \"learning_rate\": best_params[\"xgb\"][\"xgb_learning_rate\"],\n",
    "    \"max_depth\": best_params[\"xgb\"][\"xgb_max_depth\"],\n",
    "    \"min_child_weight\": best_params[\"xgb\"][\"xgb_min_child_weight\"],\n",
    "    \"subsample\": best_params[\"xgb\"][\"xgb_subsample\"],\n",
    "    \"colsample_bytree\": best_params[\"xgb\"][\"xgb_colsample_bytree\"],\n",
    "    \"reg_alpha\": best_params[\"xgb\"][\"xgb_reg_alpha\"],\n",
    "    \"reg_lambda\": best_params[\"xgb\"][\"xgb_reg_lambda\"],\n",
    "    \"gamma\": best_params[\"xgb\"][\"xgb_gamma\"],\n",
    "    \"max_bin\": best_params[\"xgb\"][\"xgb_max_bin\"],\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=MAX_BOOST_ROUNDS,\n",
    "    evals=[(dval, \"val\")],\n",
    "    early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "    verbose_eval=False,\n",
    ")\n",
    "\n",
    "models[\"xgb\"] = xgb_model\n",
    "predictions[\"xgb\"] = {\n",
    "    \"val\": xgb_model.predict(dval),\n",
    "    \"test\": xgb_model.predict(dtest),\n",
    "}\n",
    "\n",
    "# ===== CatBoost =====\n",
    "print(\"  Training CatBoost (optimized)...\")\n",
    "\n",
    "cat_params = {\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"verbose\": False,\n",
    "    \"task_type\": \"GPU\",\n",
    "    \"devices\": \"0\",\n",
    "    \"iterations\": MAX_BOOST_ROUNDS,\n",
    "    \"early_stopping_rounds\": EARLY_STOPPING_ROUNDS,\n",
    "    \"learning_rate\": best_params[\"cat\"][\"cat_learning_rate\"],\n",
    "    \"depth\": best_params[\"cat\"][\"cat_depth\"],\n",
    "    \"l2_leaf_reg\": best_params[\"cat\"][\"cat_l2_leaf_reg\"],\n",
    "    \"min_data_in_leaf\": best_params[\"cat\"][\"cat_min_data_in_leaf\"],\n",
    "    \"random_strength\": best_params[\"cat\"][\"cat_random_strength\"],\n",
    "    \"bagging_temperature\": best_params[\"cat\"][\"cat_bagging_temperature\"],\n",
    "    \"border_count\": best_params[\"cat\"][\"cat_border_count\"],\n",
    "}\n",
    "\n",
    "cat_model = CatBoostClassifier(**cat_params)\n",
    "cat_model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "\n",
    "models[\"cat\"] = cat_model\n",
    "predictions[\"cat\"] = {\n",
    "    \"val\": cat_model.predict_proba(X_val)[:, 1],\n",
    "    \"test\": cat_model.predict_proba(X_test)[:, 1],\n",
    "}\n",
    "\n",
    "# ===== Stacking =====\n",
    "print(\"\\n  Building meta-learner...\")\n",
    "\n",
    "M_val = np.column_stack([predictions[m][\"val\"] for m in [\"lgbm\", \"xgb\", \"cat\"]])\n",
    "M_test = np.column_stack([predictions[m][\"test\"] for m in [\"lgbm\", \"xgb\", \"cat\"]])\n",
    "\n",
    "meta_model = LogisticRegression(C=1.0, max_iter=2000, solver=\"lbfgs\", random_state=RANDOM_SEED)\n",
    "meta_model.fit(M_val, y_val)\n",
    "\n",
    "p_meta_val = meta_model.predict_proba(M_val)[:, 1]\n",
    "p_meta_test = meta_model.predict_proba(M_test)[:, 1]\n",
    "\n",
    "models[\"meta\"] = meta_model\n",
    "\n",
    "# ===== Platt Calibration =====\n",
    "print(\"  Applying Platt calibration...\")\n",
    "\n",
    "platt_model = LogisticRegression(C=1.0, max_iter=1000, solver=\"lbfgs\")\n",
    "platt_model.fit(p_meta_val.reshape(-1, 1), y_val)\n",
    "\n",
    "p_final_test = platt_model.predict_proba(p_meta_test.reshape(-1, 1))[:, 1]\n",
    "p_final_test = np.clip(p_final_test, 1e-5, 1 - 1e-5)\n",
    "\n",
    "models[\"platt\"] = platt_model\n",
    "\n",
    "# ===============================================\n",
    "# FINAL RESULTS\n",
    "# ===============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"   ðŸ† FINAL RESULTS (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n  Individual models:\")\n",
    "for name in [\"lgbm\", \"xgb\", \"cat\"]:\n",
    "    p = predictions[name][\"test\"]\n",
    "    p = np.clip(p, 1e-7, 1 - 1e-7)\n",
    "    ll = log_loss(y_test, p)\n",
    "    auc = roc_auc_score(y_test, p)\n",
    "    print(f\"    {name.upper()}: LogLoss={ll:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "ll_final = log_loss(y_test, p_final_test)\n",
    "auc_final = roc_auc_score(y_test, p_final_test)\n",
    "brier_final = brier_score_loss(y_test, p_final_test)\n",
    "\n",
    "print(f\"\\n  ðŸ“Š STACK FINAL (after Platt):\")\n",
    "print(f\"     LogLoss: {ll_final:.4f}\")\n",
    "print(f\"     AUC:     {auc_final:.4f}\")\n",
    "print(f\"     Brier:   {brier_final:.4f}\")\n",
    "\n",
    "final_metrics = {\n",
    "    \"test_logloss\": ll_final,\n",
    "    \"test_auc\": auc_final,\n",
    "    \"test_brier\": brier_final,\n",
    "}\n",
    "\n",
    "# ===============================================\n",
    "# SAVE\n",
    "# ===============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"   SAVE RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with open(OUT_DIR / \"best_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(f\"  âœ… Best params: {OUT_DIR / 'best_params.json'}\")\n",
    "\n",
    "with open(OUT_DIR / \"hpo_scores.json\", \"w\") as f:\n",
    "    json.dump(best_scores, f, indent=2)\n",
    "print(f\"  âœ… HPO scores: {OUT_DIR / 'hpo_scores.json'}\")\n",
    "\n",
    "joblib.dump({\n",
    "    \"lgbm\": models[\"lgbm\"],\n",
    "    \"xgb\": models[\"xgb\"],\n",
    "    \"cat\": models[\"cat\"],\n",
    "    \"meta\": models[\"meta\"],\n",
    "    \"platt\": models[\"platt\"],\n",
    "    \"feature_cols\": feature_cols,\n",
    "    \"metrics\": final_metrics,\n",
    "    \"created\": datetime.now().isoformat(),\n",
    "}, OUT_DIR / \"models_optimized.joblib\")\n",
    "print(f\"  âœ… Models: {OUT_DIR / 'models_optimized.joblib'}\")\n",
    "\n",
    "with open(OUT_DIR / \"final_metrics.json\", \"w\") as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "print(f\"  âœ… Metrics: {OUT_DIR / 'final_metrics.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"   âœ… HPO GOD MODE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n  ðŸ“Š CV Results:\")\n",
    "print(f\"     LGBM: {best_scores['lgbm']['logloss']:.4f}\")\n",
    "print(f\"     XGB:  {best_scores['xgb']['logloss']:.4f}\")\n",
    "print(f\"     CAT:  {best_scores['cat']['logloss']:.4f}\")\n",
    "print(f\"\\n  ðŸ† Final Test: LogLoss={ll_final:.4f}, AUC={auc_final:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca41363-b26a-4bf4-a884-f16895918c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
