{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debd6c1-5dd1-40d5-a5bf-125908bf1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ===============================================\n",
    "# PREPROCESS 9 - ULTRA GOD MODE SOTA 2026\n",
    "# Calcule TOUTES les features manquantes depuis z√©ro\n",
    "# ===============================================\n",
    "#\n",
    "# Ce script reproduit les Cellules A-E de l'ancien pipeline\n",
    "# en 100% Polars, puis applique le pipeline ML standard.\n",
    "#\n",
    "# Input: data_clean/ml_ready/matches_ml_ready.parquet\n",
    "# Output: data_clean/ml_final/\n",
    "# ===============================================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURATION\n",
    "# ===============================================\n",
    "ROOT = Path.cwd()\n",
    "DATA_CLEAN = ROOT / \"data_clean\"\n",
    "\n",
    "def find_latest_sota_file(ml_ready_dir: Path):\n",
    "    \"\"\"D√©tecte automatiquement la derni√®re version SOTA (v5, v6, v7, v8...).\"\"\"\n",
    "    import re\n",
    "    \n",
    "    sota_files = list(ml_ready_dir.glob(\"matches_ml_ready_SOTA_v*.parquet\"))\n",
    "    \n",
    "    if not sota_files:\n",
    "        # Fallback to base file\n",
    "        base = ml_ready_dir / \"matches_ml_ready.parquet\"\n",
    "        if base.exists():\n",
    "            return \"BASE\", base\n",
    "        raise FileNotFoundError(\"No ML-ready file found!\")\n",
    "    \n",
    "    # Extract version numbers and sort descending\n",
    "    def get_version(p):\n",
    "        match = re.search(r'SOTA_v(\\d+)', p.stem)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    sota_files.sort(key=get_version, reverse=True)\n",
    "    latest = sota_files[0]\n",
    "    version = f\"SOTA_v{get_version(latest)}\"\n",
    "    \n",
    "    return version, latest\n",
    "\n",
    "# Usage:\n",
    "ML_READY_VERSION, ML_READY_FILE = find_latest_sota_file(DATA_CLEAN / \"ml_ready\")\n",
    "print(f\"üìÅ Auto-detected: {ML_READY_VERSION} ‚Üí {ML_READY_FILE.name}\")\n",
    "\n",
    "# Players master file\n",
    "PLAYERS_MASTER_FILE = ROOT / \"data_atp_detailed\" / \"atp_master_players.csv\"\n",
    "PLAYERS_MASTER_PARQUET = ROOT / \"data_atp_detailed\" / \"atp_master_players.parquet\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = DATA_CLEAN / \"ml_final\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "GENDER = \"atp\"\n",
    "RANDOM_SEED = 42\n",
    "PI2 = 2 * np.pi\n",
    "\n",
    "# Split temporel\n",
    "TRAIN_END_YEAR = 2019\n",
    "VAL_END_YEAR = 2022\n",
    "\n",
    "# Feature selection\n",
    "MAX_FEATURES = 200\n",
    "MIN_VARIANCE_THRESHOLD = 0.001\n",
    "MAX_NULL_RATE = 0.95\n",
    "\n",
    "# SOTA features to ALWAYS include (protected from elimination)\n",
    "PROTECTED_SOTA_FEATURES = [\n",
    "    # === EXISTANTES ===\n",
    "    \"tourney_speed_index\",\n",
    "    \"pref_ssi_A\", \"pref_ssi_B\", \"diff_pref_ssi\", \"diff_ssi_mismatch\",\n",
    "    \"r20_win_rate_vs_top10_A\", \"r20_win_rate_vs_top10_B\",\n",
    "    \"r20_win_rate_vs_top50_A\", \"r20_win_rate_vs_top50_B\",\n",
    "    \"r20_upset_rate_A\", \"r20_upset_rate_B\",\n",
    "    \"r20_choke_rate_A\", \"r20_choke_rate_B\",\n",
    "    \"r20_comeback_rate_A\", \"r20_comeback_rate_B\",\n",
    "    \n",
    "    # === NOUVELLES PP_12-15 ===\n",
    "    \"bt_rating_A\", \"bt_rating_B\", \"bt_prob_winner\",\n",
    "    \"bt_surface_rating_A\", \"bt_surface_rating_B\",\n",
    "    \"bt_recent_rating_A\", \"bt_recent_rating_B\",\n",
    "    \"bt_form_momentum_A\", \"bt_form_momentum_B\",\n",
    "    \"travel_distance_A\", \"travel_distance_B\",\n",
    "    \"timezone_shift_A\", \"timezone_shift_B\",\n",
    "    \"home_advantage_A\", \"home_advantage_B\",\n",
    "    \"is_high_altitude\", \"tourney_altitude_m\",\n",
    "    \"emb_cosine_sim\", \"emb_l2_distance\",\n",
    "    \"seq_cosine_sim\", \"seq_l2_distance\",\n",
    "    \"odds_implied_prob_A\", \"odds_implied_prob_B\",\n",
    "]\n",
    "\n",
    "# Colonnes leakage\n",
    "LEAKAGE_COLS = [\n",
    "    \"score_ta\", \"duration_minutes_ta\",\n",
    "    \"w_ace\", \"w_df\", \"w_svpt\", \"w_1stIn\", \"w_1stWon\", \"w_2ndWon\",\n",
    "    \"l_ace\", \"l_df\", \"l_svpt\", \"l_1stIn\", \"l_1stWon\", \"l_2ndWon\",\n",
    "    \"w_bpSaved\", \"w_bpFaced\", \"l_bpSaved\", \"l_bpFaced\",\n",
    "    \"winner_name\", \"loser_name\",\n",
    "]\n",
    "\n",
    "ID_COLS = [\n",
    "    \"custom_match_id\", \"match_id_ta_dedup\", \"match_id_ta_source\",\n",
    "    \"winner_id\", \"loser_id\", \"tourney_name_ta\", \"tourney_slug_ta\",\n",
    "]\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 1: LOAD & INITIAL CLEAN\n",
    "# ===============================================\n",
    "\n",
    "def load_dataset() -> pl.DataFrame:\n",
    "    \"\"\"Charge le dataset ML ready.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   SECTION 1: LOAD DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n  Loading {ML_READY_FILE}...\")\n",
    "    df = pl.read_parquet(ML_READY_FILE)\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    # Ensure year column exists\n",
    "    if \"year\" not in df.columns:\n",
    "        if \"tourney_date_ta\" in df.columns:\n",
    "            if df[\"tourney_date_ta\"].dtype in [pl.Int64, pl.Int32]:\n",
    "                df = df.with_columns([\n",
    "                    (pl.col(\"tourney_date_ta\") // 10000).cast(pl.Int32).alias(\"year\")\n",
    "                ])\n",
    "            else:\n",
    "                df = df.with_columns([\n",
    "                    pl.col(\"tourney_date_ta\").dt.year().cast(pl.Int32).alias(\"year\")\n",
    "                ])\n",
    "    \n",
    "    # Filter valid surfaces\n",
    "    if \"tourney_surface_ta\" in df.columns:\n",
    "        n_before = len(df)\n",
    "        df = df.filter(pl.col(\"tourney_surface_ta\").is_not_null())\n",
    "        print(f\"  After surface filter: {len(df)} (removed {n_before - len(df)})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 2A: CELLULE A - Form/Fatigue Features\n",
    "# ===============================================\n",
    "\n",
    "def add_cellule_a_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Cellule A: Rolling features de forme et fatigue.\n",
    "    Calcule: is_vs_top10/50, rounds_played_tourney, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   CELLULE A: FORM/FATIGUE FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    n_before = len(df.columns)\n",
    "    \n",
    "    # --- is_vs_top10 / is_vs_top50 ---\n",
    "    # Pour A: adversaire = loser, donc loser_rank_ta\n",
    "    # Pour B: adversaire = winner, donc winner_rank_ta\n",
    "    print(\"\\n[A.1] Adding is_vs_top10/50...\")\n",
    "    \n",
    "    if \"loser_rank_ta\" in df.columns:\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"loser_rank_ta\").fill_null(9999) <= 10).cast(pl.Int8).alias(\"is_vs_top10_A\"),\n",
    "            (pl.col(\"loser_rank_ta\").fill_null(9999) <= 50).cast(pl.Int8).alias(\"is_vs_top50_A\"),\n",
    "        ])\n",
    "        print(\"  ‚úÖ is_vs_top10_A, is_vs_top50_A\")\n",
    "    \n",
    "    if \"winner_rank_ta\" in df.columns:\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"winner_rank_ta\").fill_null(9999) <= 10).cast(pl.Int8).alias(\"is_vs_top10_B\"),\n",
    "            (pl.col(\"winner_rank_ta\").fill_null(9999) <= 50).cast(pl.Int8).alias(\"is_vs_top50_B\"),\n",
    "        ])\n",
    "        print(\"  ‚úÖ is_vs_top10_B, is_vs_top50_B\")\n",
    "    \n",
    "    # --- rounds_played_tourney ---\n",
    "    print(\"\\n[A.2] Adding rounds_played_tourney...\")\n",
    "    \n",
    "    # Round order mapping\n",
    "    round_order = {\n",
    "        \"Q\": 0, \"Q1\": 0, \"Q2\": 1, \"Q3\": 2,\n",
    "        \"RR\": 16, \"R128\": 20, \"R64\": 30, \"R56\": 35, \"R48\": 37,\n",
    "        \"R32\": 40, \"R28\": 45, \"R24\": 47, \"R16\": 50, \"QF\": 60, \"SF\": 70, \"F\": 80,\n",
    "        \"3RD\": 76, \"BRONZE\": 75\n",
    "    }\n",
    "    \n",
    "    if \"round_ta\" in df.columns:\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"round_ta\").str.to_uppercase().replace(round_order, default=0).cast(pl.Int16).alias(\"round_ord\")\n",
    "        ])\n",
    "    \n",
    "    # Pour calculer rounds_played_tourney, on a besoin de trier par (joueur, tournoi, date, round)\n",
    "    # et compter le cumsum. Mais ici on n'a pas la vue joueur-level, on a match-level.\n",
    "    # On va approximer avec round_ord directement.\n",
    "    if \"round_ord\" in df.columns:\n",
    "        # Plus le round_ord est √©lev√©, plus le joueur a jou√© de tours\n",
    "        # Approximation: round_ord / 10 donne ~nombre de tours jou√©s\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"round_ord\") / 10).cast(pl.Int8).alias(\"rounds_played_approx_A\"),\n",
    "            (pl.col(\"round_ord\") / 10).cast(pl.Int8).alias(\"rounds_played_approx_B\"),\n",
    "        ])\n",
    "        print(\"  ‚úÖ rounds_played_approx_A/B (approximation)\")\n",
    "    \n",
    "    # --- is_qualifier / is_wildcard / is_lucky_loser ---\n",
    "    print(\"\\n[A.3] Adding entry type flags...\")\n",
    "    \n",
    "    # Check if entry columns exist (they might come from other sources)\n",
    "    for suffix in [\"_A\", \"_B\"]:\n",
    "        # Default to 0 if not available\n",
    "        for flag in [\"is_qualifier\", \"is_wildcard\", \"is_lucky_loser\"]:\n",
    "            col_name = f\"{flag}{suffix}\"\n",
    "            if col_name not in df.columns:\n",
    "                df = df.with_columns([pl.lit(0).cast(pl.Int8).alias(col_name)])\n",
    "    print(\"  ‚úÖ is_qualifier/wildcard/lucky_loser (defaulted to 0)\")\n",
    "    \n",
    "    # --- fatigue_qualifs ---\n",
    "    print(\"\\n[A.4] Adding fatigue_qualifs...\")\n",
    "    \n",
    "    # Si on a des colonnes qualifs\n",
    "    for suffix in [\"_A\", \"_B\"]:\n",
    "        q_minutes_col = f\"p_q_minutes{suffix}\"\n",
    "        if q_minutes_col in df.columns:\n",
    "            df = df.with_columns([\n",
    "                pl.col(q_minutes_col).fill_null(0).cast(pl.Float32).alias(f\"fatigue_qualifs{suffix}\")\n",
    "            ])\n",
    "        else:\n",
    "            df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(f\"fatigue_qualifs{suffix}\")])\n",
    "    print(\"  ‚úÖ fatigue_qualifs_A/B\")\n",
    "    \n",
    "    # --- r20_win_rate_vs_top10 ---\n",
    "    # Check if SOTA version already exists (from Preprocess 2.1b)\n",
    "    print(\"\\n[A.5] Checking r20_win_rate_vs_top10...\")\n",
    "    \n",
    "    for suffix in [\"_A\", \"_B\"]:\n",
    "        col_name = f\"r20_win_rate_vs_top10{suffix}\"\n",
    "        \n",
    "        if col_name in df.columns:\n",
    "            coverage = df[col_name].is_not_null().mean()\n",
    "            print(f\"  ‚úÖ {col_name} already exists (SOTA, coverage: {coverage:.1%})\")\n",
    "        else:\n",
    "            # Try to find a suitable proxy\n",
    "            win_rate_col = None\n",
    "            for candidate in [f\"win_rate_20{suffix}\", f\"win_rate_r20{suffix}\", f\"r20_win_rate{suffix}\"]:\n",
    "                if candidate in df.columns:\n",
    "                    win_rate_col = candidate\n",
    "                    break\n",
    "            \n",
    "            if win_rate_col:\n",
    "                df = df.with_columns([\n",
    "                    pl.col(win_rate_col).fill_null(0.5).cast(pl.Float32).alias(col_name)\n",
    "                ])\n",
    "                print(f\"  ‚ö†Ô∏è {col_name} (approximated from {win_rate_col})\")\n",
    "            else:\n",
    "                df = df.with_columns([pl.lit(None).cast(pl.Float32).alias(col_name)])\n",
    "                print(f\"  ‚ö†Ô∏è {col_name} (no proxy found, will be imputed later)\")\n",
    "    \n",
    "    n_after = len(df.columns)\n",
    "    print(f\"\\n  Cellule A: {n_after - n_before} features added\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 2B: CELLULE B - Cyclic & Fatigue Index\n",
    "# ===============================================\n",
    "\n",
    "def add_cellule_b_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Cellule B: Encodage cyclique temporel + Index de fatigue cumul√©e.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   CELLULE B: CYCLIC ENCODING & FATIGUE INDEX\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    n_before = len(df.columns)\n",
    "    \n",
    "    # --- Cyclic temporal encoding ---\n",
    "    print(\"\\n[B.1] Adding cyclic temporal features...\")\n",
    "    \n",
    "    if \"tourney_date_ta\" in df.columns:\n",
    "        # Convert to datetime if needed\n",
    "        if df[\"tourney_date_ta\"].dtype in [pl.Int64, pl.Int32]:\n",
    "            df = df.with_columns([\n",
    "                pl.col(\"tourney_date_ta\").cast(pl.Utf8).str.to_datetime(\"%Y%m%d\").alias(\"_dt_temp\")\n",
    "            ])\n",
    "        else:\n",
    "            df = df.with_columns([pl.col(\"tourney_date_ta\").alias(\"_dt_temp\")])\n",
    "        \n",
    "        # Extract components\n",
    "        df = df.with_columns([\n",
    "            pl.col(\"_dt_temp\").dt.month().alias(\"_month\"),\n",
    "            pl.col(\"_dt_temp\").dt.week().alias(\"_week\"),\n",
    "            pl.col(\"_dt_temp\").dt.weekday().alias(\"_dow\"),\n",
    "        ])\n",
    "        \n",
    "        # Cyclic encoding\n",
    "        df = df.with_columns([\n",
    "            (PI2 * pl.col(\"_month\") / 12).sin().cast(pl.Float32).alias(\"month_sin\"),\n",
    "            (PI2 * pl.col(\"_month\") / 12).cos().cast(pl.Float32).alias(\"month_cos\"),\n",
    "            (PI2 * pl.col(\"_week\") / 52).sin().cast(pl.Float32).alias(\"week_sin\"),\n",
    "            (PI2 * pl.col(\"_week\") / 52).cos().cast(pl.Float32).alias(\"week_cos\"),\n",
    "            (PI2 * pl.col(\"_dow\") / 7).sin().cast(pl.Float32).alias(\"dow_sin\"),\n",
    "            (PI2 * pl.col(\"_dow\") / 7).cos().cast(pl.Float32).alias(\"dow_cos\"),\n",
    "            ((pl.col(\"_week\").clip(1, 48) - 1) / 47).cast(pl.Float32).alias(\"season_progress\"),\n",
    "        ])\n",
    "        \n",
    "        df = df.drop([\"_dt_temp\", \"_month\", \"_week\", \"_dow\"])\n",
    "        print(\"  ‚úÖ month_sin/cos, week_sin/cos, dow_sin/cos, season_progress\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è tourney_date_ta not found, skipping cyclic encoding\")\n",
    "    \n",
    "    # --- Cumulative fatigue index ---\n",
    "    print(\"\\n[B.2] Adding cumulative fatigue index...\")\n",
    "    \n",
    "    for suffix in [\"_A\", \"_B\"]:\n",
    "        fatigue_components = []\n",
    "        component_names = []\n",
    "        \n",
    "        # Matches last 7d (30%)\n",
    "        matches_7d_candidates = [f\"matches_last_7d{suffix}\", f\"matches_7d{suffix}\"]\n",
    "        matches_7d_col = next((c for c in matches_7d_candidates if c in df.columns), None)\n",
    "        if matches_7d_col:\n",
    "            fatigue_components.append((pl.col(matches_7d_col).fill_null(0) / 5).clip(0, 1) * 0.30)\n",
    "            component_names.append(\"matches_7d\")\n",
    "        \n",
    "        # Minutes last 7d (30%)\n",
    "        minutes_7d_candidates = [f\"minutes_last_7d{suffix}\", f\"minutes_7d{suffix}\"]\n",
    "        minutes_7d_col = next((c for c in minutes_7d_candidates if c in df.columns), None)\n",
    "        if minutes_7d_col:\n",
    "            fatigue_components.append((pl.col(minutes_7d_col).fill_null(0) / 600).clip(0, 1) * 0.30)\n",
    "            component_names.append(\"minutes_7d\")\n",
    "        \n",
    "        # Minutes last 30d (20%)\n",
    "        minutes_30d_candidates = [f\"minutes_last_30d{suffix}\", f\"minutes_30d{suffix}\"]\n",
    "        minutes_30d_col = next((c for c in minutes_30d_candidates if c in df.columns), None)\n",
    "        if minutes_30d_col:\n",
    "            fatigue_components.append((pl.col(minutes_30d_col).fill_null(0) / 2000).clip(0, 1) * 0.20)\n",
    "            component_names.append(\"minutes_30d\")\n",
    "        \n",
    "        # Days since last (inverse, 20%)\n",
    "        days_candidates = [f\"days_since_last{suffix}\", f\"days_since_last_ff{suffix}\", f\"days_rest{suffix}\"]\n",
    "        days_col = next((c for c in days_candidates if c in df.columns), None)\n",
    "        if days_col:\n",
    "            fatigue_components.append((1 - (pl.col(days_col).fill_null(7) / 14).clip(0, 1)) * 0.20)\n",
    "            component_names.append(\"days_rest_inv\")\n",
    "        \n",
    "        # Fatigue qualifs (bonus)\n",
    "        fatigue_qualifs_col = f\"fatigue_qualifs{suffix}\"\n",
    "        if fatigue_qualifs_col in df.columns:\n",
    "            fatigue_components.append((pl.col(fatigue_qualifs_col).fill_null(0) / 300).clip(0, 1) * 0.10)\n",
    "            component_names.append(\"qualifs\")\n",
    "        \n",
    "        if fatigue_components:\n",
    "            fatigue_sum = fatigue_components[0]\n",
    "            for expr in fatigue_components[1:]:\n",
    "                fatigue_sum = fatigue_sum + expr\n",
    "            \n",
    "            df = df.with_columns([\n",
    "                fatigue_sum.clip(0, 1).cast(pl.Float32).alias(f\"cumulative_fatigue_index{suffix}\"),\n",
    "                (fatigue_sum > 0.6).cast(pl.Int8).alias(f\"is_high_fatigue{suffix}\"),\n",
    "                (fatigue_sum < 0.2).cast(pl.Int8).alias(f\"is_fresh{suffix}\"),\n",
    "            ])\n",
    "            print(f\"  ‚úÖ cumulative_fatigue_index{suffix} (components: {', '.join(component_names)})\")\n",
    "        else:\n",
    "            # Default neutral values\n",
    "            df = df.with_columns([\n",
    "                pl.lit(0.3).cast(pl.Float32).alias(f\"cumulative_fatigue_index{suffix}\"),\n",
    "                pl.lit(0).cast(pl.Int8).alias(f\"is_high_fatigue{suffix}\"),\n",
    "                pl.lit(0).cast(pl.Int8).alias(f\"is_fresh{suffix}\"),\n",
    "            ])\n",
    "            print(f\"  ‚ö†Ô∏è cumulative_fatigue_index{suffix} (no components found, defaulted)\")\n",
    "    \n",
    "    # Fatigue advantage\n",
    "    df = df.with_columns([\n",
    "        (pl.col(\"cumulative_fatigue_index_B\") - pl.col(\"cumulative_fatigue_index_A\"))\n",
    "        .cast(pl.Float32).alias(\"fatigue_advantage_A\")\n",
    "    ])\n",
    "    print(\"  ‚úÖ fatigue_advantage_A\")\n",
    "    \n",
    "    n_after = len(df.columns)\n",
    "    print(f\"\\n  Cellule B: {n_after - n_before} features added\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 2C: CELLULE C - Players Enrichment\n",
    "# ===============================================\n",
    "\n",
    "def add_cellule_c_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Cellule C: Enrichissement avec donn√©es joueurs (age, height, handed, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   CELLULE C: PLAYERS ENRICHMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    n_before = len(df.columns)\n",
    "    \n",
    "    # Try to load players master\n",
    "    players = None\n",
    "    \n",
    "    if PLAYERS_MASTER_PARQUET.exists():\n",
    "        print(f\"\\n  Loading players from {PLAYERS_MASTER_PARQUET}...\")\n",
    "        try:\n",
    "            players = pl.read_parquet(PLAYERS_MASTER_PARQUET)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error loading parquet: {e}\")\n",
    "    \n",
    "    if players is None and PLAYERS_MASTER_FILE.exists():\n",
    "        print(f\"\\n  Loading players from {PLAYERS_MASTER_FILE}...\")\n",
    "        try:\n",
    "            players = pl.read_csv(PLAYERS_MASTER_FILE, infer_schema_length=10000)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error loading CSV: {e}\")\n",
    "    \n",
    "    if players is not None:\n",
    "        print(f\"  Loaded {len(players)} players\")\n",
    "        print(f\"  Columns: {players.columns[:10]}...\")\n",
    "        \n",
    "        # Normalize players data\n",
    "        # Find ID column\n",
    "        id_col = None\n",
    "        for candidate in [\"atp_id_ref\", \"generated_id\", \"player_id\", \"id\"]:\n",
    "            if candidate in players.columns:\n",
    "                id_col = candidate\n",
    "                break\n",
    "        \n",
    "        if id_col:\n",
    "            print(f\"  Using ID column: {id_col}\")\n",
    "            \n",
    "            # Prepare player features\n",
    "            player_features = [id_col]\n",
    "            rename_map = {id_col: \"player_id_join\"}\n",
    "            \n",
    "            # Birth date -> for age calculation\n",
    "            if \"birth_date\" in players.columns:\n",
    "                players = players.with_columns([\n",
    "                    pl.col(\"birth_date\").str.to_datetime(\"%Y-%m-%d\", strict=False).alias(\"birth_date_dt\")\n",
    "                ])\n",
    "                player_features.append(\"birth_date_dt\")\n",
    "            \n",
    "            # Height\n",
    "            if \"height_cm\" in players.columns:\n",
    "                players = players.with_columns([\n",
    "                    pl.col(\"height_cm\").cast(pl.Float32)\n",
    "                ])\n",
    "                player_features.append(\"height_cm\")\n",
    "            \n",
    "            # Weight\n",
    "            if \"weight_kg\" in players.columns:\n",
    "                players = players.with_columns([\n",
    "                    pl.col(\"weight_kg\").cast(pl.Float32)\n",
    "                ])\n",
    "                player_features.append(\"weight_kg\")\n",
    "            \n",
    "            # Handed\n",
    "            if \"plays_hand\" in players.columns:\n",
    "                players = players.with_columns([\n",
    "                    pl.when(pl.col(\"plays_hand\").str.to_lowercase().str.contains(\"left\"))\n",
    "                    .then(pl.lit(\"L\"))\n",
    "                    .when(pl.col(\"plays_hand\").str.to_lowercase().str.contains(\"right\"))\n",
    "                    .then(pl.lit(\"R\"))\n",
    "                    .otherwise(pl.lit(None))\n",
    "                    .alias(\"handed\")\n",
    "                ])\n",
    "                player_features.append(\"handed\")\n",
    "            \n",
    "            # Backhand\n",
    "            if \"plays_backhand\" in players.columns:\n",
    "                players = players.with_columns([\n",
    "                    pl.when(pl.col(\"plays_backhand\").str.to_lowercase().str.contains(\"two|2\"))\n",
    "                    .then(pl.lit(\"2H\"))\n",
    "                    .when(pl.col(\"plays_backhand\").str.to_lowercase().str.contains(\"one|1\"))\n",
    "                    .then(pl.lit(\"1H\"))\n",
    "                    .otherwise(pl.lit(None))\n",
    "                    .alias(\"backhand\")\n",
    "                ])\n",
    "                player_features.append(\"backhand\")\n",
    "            \n",
    "            # Pro year\n",
    "            if \"pro_year\" in players.columns:\n",
    "                players = players.with_columns([\n",
    "                    pl.col(\"pro_year\").cast(pl.Float32)\n",
    "                ])\n",
    "                player_features.append(\"pro_year\")\n",
    "            \n",
    "            # Select and deduplicate\n",
    "            players_subset = players.select([c for c in player_features if c in players.columns]).unique(id_col)\n",
    "            players_subset = players_subset.rename({id_col: \"player_id_join\"})\n",
    "            \n",
    "            # Join for winner (A) and loser (B)\n",
    "            for suffix, id_source in [(\"_A\", \"winner_id\"), (\"_B\", \"loser_id\")]:\n",
    "                if id_source in df.columns:\n",
    "                    # Prepare join\n",
    "                    players_for_join = players_subset.clone()\n",
    "                    \n",
    "                    # Rename columns with suffix\n",
    "                    for col in players_for_join.columns:\n",
    "                        if col != \"player_id_join\":\n",
    "                            players_for_join = players_for_join.rename({col: f\"{col}{suffix}\"})\n",
    "                    \n",
    "                    # Join\n",
    "                    df = df.join(\n",
    "                        players_for_join,\n",
    "                        left_on=id_source,\n",
    "                        right_on=\"player_id_join\",\n",
    "                        how=\"left\"\n",
    "                    )\n",
    "            \n",
    "            print(\"  ‚úÖ Player features joined\")\n",
    "            \n",
    "            # Calculate derived features\n",
    "            if \"tourney_date_ta\" in df.columns:\n",
    "                # Get date expression\n",
    "                if df[\"tourney_date_ta\"].dtype in [pl.Int64, pl.Int32]:\n",
    "                    date_expr = pl.col(\"tourney_date_ta\").cast(pl.Utf8).str.to_datetime(\"%Y%m%d\")\n",
    "                    year_expr = (pl.col(\"tourney_date_ta\") // 10000).cast(pl.Float32)\n",
    "                else:\n",
    "                    date_expr = pl.col(\"tourney_date_ta\")\n",
    "                    year_expr = pl.col(\"tourney_date_ta\").dt.year().cast(pl.Float32)\n",
    "                \n",
    "                for suffix in [\"_A\", \"_B\"]:\n",
    "                    # Age at match\n",
    "                    birth_col = f\"birth_date_dt{suffix}\"\n",
    "                    if birth_col in df.columns:\n",
    "                        df = df.with_columns([\n",
    "                            ((date_expr - pl.col(birth_col)).dt.total_days() / 365.25)\n",
    "                            .clip(13, 55)\n",
    "                            .cast(pl.Float32)\n",
    "                            .alias(f\"age_at_match{suffix}\")\n",
    "                        ])\n",
    "                        print(f\"  ‚úÖ age_at_match{suffix}\")\n",
    "                    \n",
    "                    # Pro career length\n",
    "                    pro_year_col = f\"pro_year{suffix}\"\n",
    "                    if pro_year_col in df.columns:\n",
    "                        df = df.with_columns([\n",
    "                            (year_expr - pl.col(pro_year_col))\n",
    "                            .clip(0, 40)\n",
    "                            .cast(pl.Float32)\n",
    "                            .alias(f\"pro_career_len{suffix}\")\n",
    "                        ])\n",
    "                        print(f\"  ‚úÖ pro_career_len{suffix}\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è No valid ID column found in players file\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No players file found, creating default values\")\n",
    "        \n",
    "        # Create default values\n",
    "        for suffix in [\"_A\", \"_B\"]:\n",
    "            df = df.with_columns([\n",
    "                pl.lit(None).cast(pl.Float32).alias(f\"age_at_match{suffix}\"),\n",
    "                pl.lit(None).cast(pl.Float32).alias(f\"pro_career_len{suffix}\"),\n",
    "                pl.lit(None).cast(pl.Float32).alias(f\"height_cm{suffix}\"),\n",
    "                pl.lit(None).cast(pl.Float32).alias(f\"weight_kg{suffix}\"),\n",
    "                pl.lit(None).cast(pl.Utf8).alias(f\"handed{suffix}\"),\n",
    "                pl.lit(None).cast(pl.Utf8).alias(f\"backhand{suffix}\"),\n",
    "            ])\n",
    "    \n",
    "    n_after = len(df.columns)\n",
    "    print(f\"\\n  Cellule C: {n_after - n_before} features added\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 2D: CELLULE D - Tourney Speed Index\n",
    "# ===============================================\n",
    "\n",
    "def add_cellule_d_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Cellule D: Index de vitesse du tournoi bas√© sur l'exc√®s d'aces.\n",
    "    Note: Si les features SOTA existent d√©j√† (de Preprocess 2.1b), on les garde.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   CELLULE D: TOURNEY SPEED INDEX\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    n_before = len(df.columns)\n",
    "    \n",
    "    # Check if SOTA speed index already exists\n",
    "    if \"tourney_speed_index\" in df.columns:\n",
    "        coverage = df[\"tourney_speed_index\"].is_not_null().mean()\n",
    "        print(f\"\\n  ‚úÖ tourney_speed_index already exists (coverage: {coverage:.1%})\")\n",
    "        print(\"     Skipping recalculation (using Preprocess 2.1b SOTA version)\")\n",
    "    else:\n",
    "        # Check for precomputed speed index file\n",
    "        speed_index_dir = DATA_CLEAN / \"features\" / \"tourney_speed_index\"\n",
    "        \n",
    "        if speed_index_dir.exists():\n",
    "            print(f\"\\n  Loading precomputed speed index from {speed_index_dir}...\")\n",
    "            try:\n",
    "                speed_df = pl.read_parquet(speed_index_dir)\n",
    "                \n",
    "                # Find the right columns\n",
    "                speed_col = \"tourney_speed_index\" if \"tourney_speed_index\" in speed_df.columns else \"speed_prior3y\"\n",
    "                \n",
    "                if \"tourney_slug_ta\" in speed_df.columns and \"year\" in speed_df.columns:\n",
    "                    speed_df = speed_df.select([\n",
    "                        \"tourney_slug_ta\", \"year\",\n",
    "                        pl.col(speed_col).alias(\"tourney_speed_index\")\n",
    "                    ]).unique([\"tourney_slug_ta\", \"year\"])\n",
    "                    \n",
    "                    df = df.join(speed_df, on=[\"tourney_slug_ta\", \"year\"], how=\"left\")\n",
    "                    print(f\"  ‚úÖ tourney_speed_index loaded (coverage: {df['tourney_speed_index'].is_not_null().mean():.1%})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Error loading speed index: {e}\")\n",
    "        \n",
    "        # If still not loaded, approximate from surface\n",
    "        if \"tourney_speed_index\" not in df.columns:\n",
    "            print(\"\\n  Approximating speed index from surface...\")\n",
    "            \n",
    "            if \"tourney_surface_ta\" in df.columns:\n",
    "                df = df.with_columns([\n",
    "                    pl.when(pl.col(\"tourney_surface_ta\") == \"Grass\").then(pl.lit(0.70))\n",
    "                    .when(pl.col(\"tourney_surface_ta\") == \"Carpet\").then(pl.lit(0.65))\n",
    "                    .when(pl.col(\"tourney_surface_ta\") == \"Hard\").then(pl.lit(0.50))\n",
    "                    .when(pl.col(\"tourney_surface_ta\") == \"Clay\").then(pl.lit(0.30))\n",
    "                    .otherwise(pl.lit(0.50))\n",
    "                    .cast(pl.Float32).alias(\"tourney_speed_index\")\n",
    "                ])\n",
    "                print(\"  ‚ö†Ô∏è tourney_speed_index (from surface prior - less accurate)\")\n",
    "    \n",
    "    # Check if SSI features already exist\n",
    "    if \"pref_ssi_A\" in df.columns and \"pref_ssi_B\" in df.columns:\n",
    "        print(f\"\\n  ‚úÖ pref_ssi_A/B already exist (from Preprocess 2.1b)\")\n",
    "        print(\"     Skipping SSI recalculation\")\n",
    "    \n",
    "    # Calculate speed sensitivity per player (only if not already done)\n",
    "    print(\"\\n  Adding speed sensitivity...\")\n",
    "    \n",
    "    for suffix in [\"_A\", \"_B\"]:\n",
    "        if f\"speed_sensitivity{suffix}\" in df.columns:\n",
    "            print(f\"  ‚úÖ speed_sensitivity{suffix} already exists\")\n",
    "            continue\n",
    "            \n",
    "        # Find ace rate column\n",
    "        ace_candidates = [f\"r10_p_s_ace_p{suffix}\", f\"ace_rate_r10{suffix}\", \n",
    "                         f\"r10_ace_rate{suffix}\", f\"w_s_ace_p\" if suffix == \"_A\" else f\"l_s_ace_p\"]\n",
    "        ace_col = next((c for c in ace_candidates if c in df.columns), None)\n",
    "        \n",
    "        if ace_col:\n",
    "            # Speed sensitivity = deviation from mean ace rate\n",
    "            mean_ace = df.select(pl.col(ace_col).mean()).item()\n",
    "            df = df.with_columns([\n",
    "                (pl.col(ace_col).fill_null(mean_ace) - mean_ace)\n",
    "                .cast(pl.Float32).alias(f\"speed_sensitivity{suffix}\")\n",
    "            ])\n",
    "            print(f\"  ‚úÖ speed_sensitivity{suffix}\")\n",
    "        else:\n",
    "            df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(f\"speed_sensitivity{suffix}\")])\n",
    "            print(f\"  ‚ö†Ô∏è speed_sensitivity{suffix} (no ace rate found, defaulted to 0)\")\n",
    "    \n",
    "    # Speed matchup interaction\n",
    "    if \"inter__speed_matchup\" not in df.columns:\n",
    "        df = df.with_columns([\n",
    "            (pl.col(\"tourney_speed_index\") * \n",
    "             (pl.col(\"speed_sensitivity_A\") - pl.col(\"speed_sensitivity_B\")))\n",
    "            .cast(pl.Float32).alias(\"inter__speed_matchup\")\n",
    "        ])\n",
    "        print(\"  ‚úÖ inter__speed_matchup\")\n",
    "    \n",
    "    n_after = len(df.columns)\n",
    "    print(f\"\\n  Cellule D: {n_after - n_before} features added\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 2E: CELLULE E - Meta-features\n",
    "# ===============================================\n",
    "\n",
    "def add_cellule_e_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Cellule E: Meta-features (logit transforms, SSI mismatch, gaps).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   CELLULE E: META-FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    n_before = len(df.columns)\n",
    "    \n",
    "    # --- Logit transform helper ---\n",
    "    def safe_logit(col_expr):\n",
    "        clipped = col_expr.clip(0.001, 0.999)\n",
    "        return (clipped / (1 - clipped)).log()\n",
    "    \n",
    "    # --- BP conversion logit ---\n",
    "    print(\"\\n[E.1] Adding logit transform features...\")\n",
    "    \n",
    "    bp_candidates = [\n",
    "        (\"r10_p_bp_conv_p_A\", \"r10_p_bp_conv_p_B\"),\n",
    "        (\"A_r10_p_bp_conv_p\", \"B_r10_p_bp_conv_p\"),\n",
    "        (\"bp_conv_pct_A\", \"bp_conv_pct_B\"),\n",
    "    ]\n",
    "    for col_a, col_b in bp_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (safe_logit(pl.col(col_a)) - safe_logit(pl.col(col_b)))\n",
    "                .cast(pl.Float32).alias(\"diff__clutch_bpconv_logit\")\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__clutch_bpconv_logit\")\n",
    "            break\n",
    "    else:\n",
    "        df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(\"diff__clutch_bpconv_logit\")])\n",
    "        print(\"  ‚ö†Ô∏è diff__clutch_bpconv_logit (defaulted, no bp_conv columns)\")\n",
    "    \n",
    "    # --- First serve in logit ---\n",
    "    fs_candidates = [\n",
    "        (\"r10_p_s_1stIn_p_A\", \"r10_p_s_1stIn_p_B\"),\n",
    "        (\"A_r10_p_s_1stIn_p\", \"B_r10_p_s_1stIn_p\"),\n",
    "    ]\n",
    "    for col_a, col_b in fs_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (safe_logit(pl.col(col_a)) - safe_logit(pl.col(col_b)))\n",
    "                .cast(pl.Float32).alias(\"diff__first_in_consistency\")\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__first_in_consistency\")\n",
    "            break\n",
    "    else:\n",
    "        df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(\"diff__first_in_consistency\")])\n",
    "    \n",
    "    # --- DF pressure ---\n",
    "    df_candidates = [\n",
    "        (\"r10_p_s_df_p_A\", \"r10_p_s_df_p_B\"),\n",
    "        (\"A_r10_p_s_df_p\", \"B_r10_p_s_df_p\"),\n",
    "    ]\n",
    "    for col_a, col_b in df_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_b) - pl.col(col_a)).cast(pl.Float32).alias(\"diff__df_pressure\")\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__df_pressure\")\n",
    "            break\n",
    "    else:\n",
    "        df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(\"diff__df_pressure\")])\n",
    "    \n",
    "    # --- Rally mean & style ---\n",
    "    print(\"\\n[E.2] Adding charting meta-features...\")\n",
    "    \n",
    "    rally_candidates = [\n",
    "        (\"r10_chart_avg_rally_A\", \"r10_chart_avg_rally_B\"),  # NOUVEAU - du Preprocess 2.1E\n",
    "        (\"r10_ch_avg_rally_len_A\", \"r10_ch_avg_rally_len_B\"),  # Fallback ancien format\n",
    "        (\"A_r10_ch_avg_rally_len\", \"B_r10_ch_avg_rally_len\"),  # Fallback autre format\n",
    "    ]\n",
    "    for col_a, col_b in rally_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(\"diff__rally_mean\"),\n",
    "                ((pl.col(col_a) + pl.col(col_b)) / 2).cast(pl.Float32).alias(\"style__rally_match\"),\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__rally_mean, style__rally_match\")\n",
    "            break\n",
    "    else:\n",
    "        df = df.with_columns([\n",
    "            pl.lit(0.0).cast(pl.Float32).alias(\"diff__rally_mean\"),\n",
    "            pl.lit(6.0).cast(pl.Float32).alias(\"style__rally_match\"),  # ~6 shots per rally average\n",
    "        ])\n",
    "        print(\"  ‚ö†Ô∏è diff__rally_mean, style__rally_match (defaulted)\")\n",
    "    \n",
    "    # --- Net aggression ---\n",
    "    net_candidates = [\n",
    "        (\"r10_ch_net_won_pct_A\", \"r10_ch_net_won_pct_B\"),\n",
    "        (\"A_r10_ch_net_won_pct\", \"B_r10_ch_net_won_pct\"),\n",
    "    ]\n",
    "    for col_a, col_b in net_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(\"diff__net_aggression\")\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__net_aggression\")\n",
    "            break\n",
    "    else:\n",
    "        df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(\"diff__net_aggression\")])\n",
    "    # --- NOUVEAU: Features charting √©tendues ---\n",
    "    print(\"\\n[E.2b] Adding extended charting features...\")\n",
    "    \n",
    "    # Serve pattern gap\n",
    "    serve_pattern_candidates = [\n",
    "        (\"r10_chart_serve_wide_pct_A\", \"r10_chart_serve_wide_pct_B\"),\n",
    "    ]\n",
    "    for col_a, col_b in serve_pattern_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(\"diff__serve_wide_pattern\"),\n",
    "                (pl.col(\"r10_chart_serve_t_pct_A\") - pl.col(\"r10_chart_serve_t_pct_B\"))\n",
    "                .cast(pl.Float32).alias(\"diff__serve_t_pattern\"),\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__serve_wide_pattern, diff__serve_t_pattern\")\n",
    "            break\n",
    "    \n",
    "    # Shot quality gap\n",
    "    shot_candidates = [\n",
    "        (\"r10_chart_fh_winner_pct_A\", \"r10_chart_fh_winner_pct_B\"),\n",
    "    ]\n",
    "    for col_a, col_b in shot_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(\"diff__fh_winner\"),\n",
    "                (pl.col(\"r10_chart_bh_winner_pct_A\") - pl.col(\"r10_chart_bh_winner_pct_B\"))\n",
    "                .cast(pl.Float32).alias(\"diff__bh_winner\"),\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__fh_winner, diff__bh_winner\")\n",
    "            break\n",
    "    \n",
    "    # Pressure performance gap\n",
    "    pressure_candidates = [\n",
    "        (\"r10_chart_bp_ptsw_pct_A\", \"r10_chart_bp_ptsw_pct_B\"),\n",
    "    ]\n",
    "    for col_a, col_b in pressure_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(\"diff__bp_pressure\"),\n",
    "                (pl.col(\"r10_chart_gp_ptsw_pct_A\") - pl.col(\"r10_chart_gp_ptsw_pct_B\"))\n",
    "                .cast(pl.Float32).alias(\"diff__gp_pressure\"),\n",
    "            ])\n",
    "            print(\"  ‚úÖ diff__bp_pressure, diff__gp_pressure\")\n",
    "            break\n",
    "    # --- SSI Mismatch ---\n",
    "    print(\"\\n[E.3] Adding SSI mismatch...\")\n",
    "    \n",
    "    # Check if SOTA pref_ssi already exists (from Preprocess 2.1b)\n",
    "    if \"pref_ssi_A\" in df.columns and \"pref_ssi_B\" in df.columns:\n",
    "        print(\"  ‚úÖ pref_ssi_A/B already exist (SOTA from Preprocess 2.1b)\")\n",
    "        \n",
    "        # Just ensure diff_ssi_mismatch exists\n",
    "        if \"diff_ssi_mismatch\" not in df.columns:\n",
    "            if \"tourney_speed_index\" in df.columns:\n",
    "                df = df.with_columns([\n",
    "                    (\n",
    "                        (pl.col(\"tourney_speed_index\").fill_null(0) - pl.col(\"pref_ssi_A\").fill_null(0)).abs() -\n",
    "                        (pl.col(\"tourney_speed_index\").fill_null(0) - pl.col(\"pref_ssi_B\").fill_null(0)).abs()\n",
    "                    )\n",
    "                    .cast(pl.Float32).alias(\"diff_ssi_mismatch\")\n",
    "                ])\n",
    "                print(\"  ‚úÖ diff_ssi_mismatch (computed from SOTA pref_ssi)\")\n",
    "    else:\n",
    "        # Fallback: compute from Glicko surface ratings\n",
    "        for suffix in [\"_A\", \"_B\"]:\n",
    "            hard_col = f\"g2_hard_rating{suffix}\"\n",
    "            clay_col = f\"g2_clay_rating{suffix}\"\n",
    "            \n",
    "            if hard_col in df.columns and clay_col in df.columns:\n",
    "                mean_rating = (pl.col(hard_col) + pl.col(clay_col)) / 2\n",
    "                df = df.with_columns([\n",
    "                    ((pl.col(hard_col) - pl.col(clay_col)) / mean_rating.clip(1, None))\n",
    "                    .cast(pl.Float32).alias(f\"ssi_hard_vs_clay{suffix}\")\n",
    "                ])\n",
    "                \n",
    "                if \"tourney_surface_ta\" in df.columns:\n",
    "                    df = df.with_columns([\n",
    "                        pl.when(pl.col(\"tourney_surface_ta\") == \"Hard\")\n",
    "                        .then(-pl.col(f\"ssi_hard_vs_clay{suffix}\"))\n",
    "                        .when(pl.col(\"tourney_surface_ta\") == \"Clay\")\n",
    "                        .then(pl.col(f\"ssi_hard_vs_clay{suffix}\"))\n",
    "                        .otherwise(pl.lit(0.0))\n",
    "                        .abs()\n",
    "                        .cast(pl.Float32).alias(f\"ssi_mismatch{suffix}\")\n",
    "                    ])\n",
    "                \n",
    "                # Use as fallback pref_ssi\n",
    "                if f\"pref_ssi{suffix}\" not in df.columns:\n",
    "                    df = df.with_columns([\n",
    "                        pl.col(f\"ssi_hard_vs_clay{suffix}\").fill_null(0).cast(pl.Float32).alias(f\"pref_ssi{suffix}\")\n",
    "                    ])\n",
    "                print(f\"  ‚úÖ ssi_hard_vs_clay{suffix}, ssi_mismatch{suffix}, pref_ssi{suffix} (fallback)\")\n",
    "            else:\n",
    "                if f\"pref_ssi{suffix}\" not in df.columns:\n",
    "                    df = df.with_columns([\n",
    "                        pl.lit(0.0).cast(pl.Float32).alias(f\"ssi_hard_vs_clay{suffix}\"),\n",
    "                        pl.lit(0.0).cast(pl.Float32).alias(f\"ssi_mismatch{suffix}\"),\n",
    "                        pl.lit(0.0).cast(pl.Float32).alias(f\"pref_ssi{suffix}\"),\n",
    "                    ])\n",
    "        \n",
    "        # SSI mismatch diff\n",
    "        if \"diff_ssi_mismatch\" not in df.columns:\n",
    "            if \"ssi_mismatch_A\" in df.columns and \"ssi_mismatch_B\" in df.columns:\n",
    "                df = df.with_columns([\n",
    "                    (pl.col(\"ssi_mismatch_B\") - pl.col(\"ssi_mismatch_A\"))\n",
    "                    .cast(pl.Float32).alias(\"diff_ssi_mismatch\")\n",
    "                ])\n",
    "            else:\n",
    "                df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(\"diff_ssi_mismatch\")])\n",
    "        print(\"  ‚úÖ diff_ssi_mismatch\")\n",
    "    \n",
    "    # --- Rank interactions ---\n",
    "    print(\"\\n[E.4] Adding rank interactions...\")\n",
    "    \n",
    "    if \"winner_rank_ta\" in df.columns and \"loser_rank_ta\" in df.columns:\n",
    "        # SEULEMENT les features SYM√âTRIQUES ici (ne d√©pendent pas de qui gagne)\n",
    "        df = df.with_columns([\n",
    "            # both_top10: les deux joueurs sont top 10\n",
    "            ((pl.col(\"winner_rank_ta\").fill_null(9999) <= 10) &\n",
    "             (pl.col(\"loser_rank_ta\").fill_null(9999) <= 10))\n",
    "            .cast(pl.Int8).alias(\"both_top10\"),\n",
    "\n",
    "            # both_top50: les deux joueurs sont top 50\n",
    "            ((pl.col(\"winner_rank_ta\").fill_null(9999) <= 50) &\n",
    "             (pl.col(\"loser_rank_ta\").fill_null(9999) <= 50))\n",
    "            .cast(pl.Int8).alias(\"both_top50\"),\n",
    "\n",
    "            # rank_diff_abs: diff√©rence absolue (sym√©trique)\n",
    "            (pl.col(\"winner_rank_ta\").fill_null(100) - \n",
    "             pl.col(\"loser_rank_ta\").fill_null(100)).abs()\n",
    "            .cast(pl.Float32).alias(\"rank_diff_abs\"),\n",
    "        ])\n",
    "        print(\"  ‚úÖ both_top10, both_top50, rank_diff_abs (symmetric - safe)\")\n",
    "        print(\"  ‚è≥ diff_log_rank, is_underdog_A ‚Üí post-shuffle\")\n",
    "    \n",
    "    # --- Rest advantage ---\n",
    "    print(\"\\n[E.5] Adding rest advantage...\")\n",
    "    \n",
    "    rest_candidates = [\n",
    "        (\"days_since_last_A\", \"days_since_last_B\"),\n",
    "        (\"days_since_last_ff_A\", \"days_since_last_ff_B\"),\n",
    "    ]\n",
    "    for col_a, col_b in rest_candidates:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(\"rest_advantage_A\")\n",
    "            ])\n",
    "            print(\"  ‚úÖ rest_advantage_A\")\n",
    "            break\n",
    "    else:\n",
    "        df = df.with_columns([pl.lit(0.0).cast(pl.Float32).alias(\"rest_advantage_A\")])\n",
    "    \n",
    "    # --- Surface Glicko gaps ---\n",
    "    print(\"\\n[E.6] Adding Glicko surface gaps...\")\n",
    "    \n",
    "    for surface in [\"hard\", \"clay\", \"grass\"]:\n",
    "        col_a = f\"g2_{surface}_rating_A\"\n",
    "        col_b = f\"g2_{surface}_rating_B\"\n",
    "        \n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(f\"g2_{surface}_gap\")\n",
    "            ])\n",
    "            print(f\"  ‚úÖ g2_{surface}_gap\")\n",
    "    \n",
    "    # Current surface advantage\n",
    "    if \"tourney_surface_ta\" in df.columns:\n",
    "        gap_mapping = []\n",
    "        for surf, gap_col in [(\"Hard\", \"g2_hard_gap\"), (\"Clay\", \"g2_clay_gap\"), (\"Grass\", \"g2_grass_gap\")]:\n",
    "            if gap_col in df.columns:\n",
    "                gap_mapping.append((surf, gap_col))\n",
    "        \n",
    "        if gap_mapping:\n",
    "            expr = pl.lit(0.0)\n",
    "            for surf, gap_col in gap_mapping:\n",
    "                expr = pl.when(pl.col(\"tourney_surface_ta\") == surf).then(pl.col(gap_col)).otherwise(expr)\n",
    "            df = df.with_columns([expr.cast(pl.Float32).alias(\"current_surface_advantage\")])\n",
    "            print(\"  ‚úÖ current_surface_advantage\")\n",
    "    \n",
    "    # --- Mental/Clutch gaps ---\n",
    "    print(\"\\n[E.7] Adding mental/clutch gaps...\")\n",
    "    \n",
    "    gap_pairs = [\n",
    "        (\"mental_toughness_score_A\", \"mental_toughness_score_B\", \"mental_gap\"),\n",
    "        (\"clutch_score_A\", \"clutch_score_B\", \"clutch_gap\"),\n",
    "        (\"upset_rate_r20_A\", \"upset_rate_r20_B\", \"upset_rate_gap\"),\n",
    "        (\"comeback_rate_r20_A\", \"comeback_rate_r20_B\", \"comeback_rate_gap\"),\n",
    "        (\"win_streak_current_A\", \"win_streak_current_B\", \"win_streak_gap\"),\n",
    "    ]\n",
    "    for col_a, col_b, new_col in gap_pairs:\n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            df = df.with_columns([\n",
    "                (pl.col(col_a) - pl.col(col_b)).cast(pl.Float32).alias(new_col)\n",
    "            ])\n",
    "            print(f\"  ‚úÖ {new_col}\")\n",
    "    \n",
    "    n_after = len(df.columns)\n",
    "    print(f\"\\n  Cellule E: {n_after - n_before} features added\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 3: SHUFFLE A/B\n",
    "# ===============================================\n",
    "\n",
    "def shuffle_ab(df: pl.DataFrame, seed: int = 42) -> pl.DataFrame:\n",
    "    \"\"\"Shuffle A/B pour avoir target 50/50.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   SECTION 3: SHUFFLE A/B\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    n_rows = len(df)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    swap_array = np.random.rand(n_rows) < 0.5\n",
    "    n_swapped = swap_array.sum()\n",
    "    \n",
    "    print(f\"  Swapping {n_swapped:,} rows ({100*n_swapped/n_rows:.1f}%)\")\n",
    "    \n",
    "    df = df.with_columns([pl.Series(\"_swap_mask\", swap_array)])\n",
    "    \n",
    "    # Identify paired columns\n",
    "    cols_A = [c for c in df.columns if c.endswith(\"_A\")]\n",
    "    cols_B = [c for c in df.columns if c.endswith(\"_B\")]\n",
    "    \n",
    "    bases_A = {c[:-2] for c in cols_A}\n",
    "    bases_B = {c[:-2] for c in cols_B}\n",
    "    paired_bases = bases_A & bases_B\n",
    "    \n",
    "    print(f\"  Paired features to swap: {len(paired_bases)}\")\n",
    "    \n",
    "    # Build swap expressions\n",
    "    swap_exprs = []\n",
    "    \n",
    "    for base in paired_bases:\n",
    "        col_a = f\"{base}_A\"\n",
    "        col_b = f\"{base}_B\"\n",
    "        \n",
    "        if col_a in df.columns and col_b in df.columns:\n",
    "            swap_exprs.extend([\n",
    "                pl.when(pl.col(\"_swap_mask\")).then(pl.col(col_b)).otherwise(pl.col(col_a)).alias(f\"_new_{col_a}\"),\n",
    "                pl.when(pl.col(\"_swap_mask\")).then(pl.col(col_a)).otherwise(pl.col(col_b)).alias(f\"_new_{col_b}\"),\n",
    "            ])\n",
    "    \n",
    "    # Swap winner_id <-> loser_id\n",
    "    if \"winner_id\" in df.columns and \"loser_id\" in df.columns:\n",
    "        swap_exprs.extend([\n",
    "            pl.when(pl.col(\"_swap_mask\")).then(pl.col(\"loser_id\")).otherwise(pl.col(\"winner_id\")).alias(\"_new_winner_id\"),\n",
    "            pl.when(pl.col(\"_swap_mask\")).then(pl.col(\"winner_id\")).otherwise(pl.col(\"loser_id\")).alias(\"_new_loser_id\"),\n",
    "        ])\n",
    "    \n",
    "    if swap_exprs:\n",
    "        df = df.with_columns(swap_exprs)\n",
    "    \n",
    "    # Replace columns\n",
    "    rename_map = {}\n",
    "    drop_cols = []\n",
    "    \n",
    "    for base in paired_bases:\n",
    "        col_a = f\"{base}_A\"\n",
    "        col_b = f\"{base}_B\"\n",
    "        new_a = f\"_new_{col_a}\"\n",
    "        new_b = f\"_new_{col_b}\"\n",
    "        \n",
    "        if new_a in df.columns:\n",
    "            drop_cols.extend([col_a, col_b])\n",
    "            rename_map[new_a] = col_a\n",
    "            rename_map[new_b] = col_b\n",
    "    \n",
    "    if \"_new_winner_id\" in df.columns:\n",
    "        drop_cols.extend([\"winner_id\", \"loser_id\"])\n",
    "        rename_map[\"_new_winner_id\"] = \"winner_id\"\n",
    "        rename_map[\"_new_loser_id\"] = \"loser_id\"\n",
    "    \n",
    "    df = df.drop([c for c in drop_cols if c in df.columns])\n",
    "    df = df.rename(rename_map)\n",
    "    \n",
    "    # Invert diff columns\n",
    "    diff_cols = [c for c in df.columns if c.startswith(\"diff\")]\n",
    "    if diff_cols:\n",
    "        invert_exprs = [\n",
    "            pl.when(pl.col(\"_swap_mask\")).then(-pl.col(c)).otherwise(pl.col(c)).alias(c)\n",
    "            for c in diff_cols\n",
    "        ]\n",
    "        df = df.with_columns(invert_exprs)\n",
    "    \n",
    "    # Update target\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.col(\"_swap_mask\")).then(pl.lit(0)).otherwise(pl.lit(1))\n",
    "        .cast(pl.Int8).alias(\"target_A_wins\")\n",
    "    ])\n",
    "    \n",
    "    df = df.drop(\"_swap_mask\")\n",
    "    \n",
    "    # Verify balance\n",
    "    target_dist = df.group_by(\"target_A_wins\").len().sort(\"target_A_wins\")\n",
    "    print(f\"\\n  Target distribution:\")\n",
    "    for row in target_dist.iter_rows():\n",
    "        pct = 100 * row[1] / n_rows\n",
    "        print(f\"    {row[0]}: {row[1]:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# POST-SHUFFLE RANK FEATURES (NO LEAKAGE)\n",
    "# ==============================================================================\n",
    "\n",
    "def add_post_shuffle_rank_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcule les features de rang APR√àS le shuffle pour √©viter le leakage.\n",
    "    \n",
    "    Apr√®s shuffle_ab():\n",
    "    - target_A_wins = 1 ‚Üí A = winner original (pas de swap)\n",
    "    - target_A_wins = 0 ‚Üí A = loser original (swap effectu√©)\n",
    "    \n",
    "    Cette fonction cr√©e rank_A et rank_B en tenant compte du swap,\n",
    "    puis calcule diff_log_rank et is_underdog_A correctement.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   POST-SHUFFLE RANK FEATURES (NO LEAKAGE)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # V√©rifications\n",
    "    if \"winner_rank_ta\" not in df.columns or \"loser_rank_ta\" not in df.columns:\n",
    "        print(\"  ‚ö†Ô∏è winner_rank_ta/loser_rank_ta not found, skipping\")\n",
    "        return df\n",
    "    \n",
    "    if \"target_A_wins\" not in df.columns:\n",
    "        print(\"  ‚ö†Ô∏è target_A_wins not found (shuffle not done?), skipping\")\n",
    "        return df\n",
    "    \n",
    "    # Cr√©er rank_A et rank_B selon le swap\n",
    "    df = df.with_columns([\n",
    "        # rank_A: rang du joueur en position A\n",
    "        pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "          .then(pl.col(\"winner_rank_ta\"))  # Pas de swap: A = winner\n",
    "          .otherwise(pl.col(\"loser_rank_ta\"))  # Swap: A = loser\n",
    "          .alias(\"rank_A\"),\n",
    "        \n",
    "        # rank_B: rang du joueur en position B\n",
    "        pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "          .then(pl.col(\"loser_rank_ta\"))  # Pas de swap: B = loser\n",
    "          .otherwise(pl.col(\"winner_rank_ta\"))  # Swap: B = winner\n",
    "          .alias(\"rank_B\"),\n",
    "    ])\n",
    "    \n",
    "    # Calculer les features d√©riv√©es\n",
    "    df = df.with_columns([\n",
    "        # diff_log_rank: log(rank_B) - log(rank_A)\n",
    "        # Positif si A est mieux class√©\n",
    "        (pl.col(\"rank_B\").clip(1, 2000).log() - \n",
    "         pl.col(\"rank_A\").clip(1, 2000).log())\n",
    "        .cast(pl.Float32)\n",
    "        .alias(\"diff_log_rank\"),\n",
    "        \n",
    "        # is_underdog_A: 1 si A est un gros outsider\n",
    "        (pl.col(\"rank_A\").fill_null(9999) > \n",
    "         pl.col(\"rank_B\").fill_null(1) * 2)\n",
    "        .cast(pl.Int8)\n",
    "        .alias(\"is_underdog_A\"),\n",
    "        \n",
    "        # diff_rank_normalized: √©cart normalis√©\n",
    "        ((pl.col(\"rank_A\").fill_null(100) - \n",
    "          pl.col(\"rank_B\").fill_null(100)) / 100)\n",
    "        .clip(-5, 5)\n",
    "        .cast(pl.Float32)\n",
    "        .alias(\"diff_rank_normalized\"),\n",
    "    ])\n",
    "    \n",
    "    # Statistiques\n",
    "    print(f\"  ‚úÖ rank_A, rank_B created\")\n",
    "    print(f\"  ‚úÖ diff_log_rank created\")\n",
    "    print(f\"  ‚úÖ is_underdog_A created\")\n",
    "    print(f\"  ‚úÖ diff_rank_normalized created\")\n",
    "    \n",
    "    # Check anti-leakage\n",
    "    corr = df.select(pl.corr(\"target_A_wins\", \"diff_log_rank\")).item()\n",
    "    print(f\"\\n  üîç Anti-leakage: corr(target, diff_log_rank) = {corr:.4f}\")\n",
    "    \n",
    "    if abs(corr) > 0.5:\n",
    "        print(\"  ‚ö†Ô∏è WARNING: High correlation! Check for leakage.\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Correlation OK - no obvious leakage\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==============================================================================\n",
    "# POST-SHUFFLE ODDS CONVERSION (NO LEAKAGE)\n",
    "# ==============================================================================\n",
    "\n",
    "def convert_odds_to_AB(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Convertit les odds winner/loser ‚Üí A/B apr√®s le shuffle.\n",
    "    \n",
    "    DOIT √™tre appel√© APR√àS shuffle_ab() car on utilise target_A_wins\n",
    "    pour savoir qui est A et qui est B.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   POST-SHUFFLE ODDS CONVERSION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if \"odds_implied_prob_winner\" not in df.columns:\n",
    "        print(\"  ‚ö†Ô∏è No odds columns found, skipping\")\n",
    "        return df\n",
    "    \n",
    "    # Convertir winner/loser ‚Üí A/B selon le shuffle\n",
    "    # target_A_wins = 1 ‚Üí A = winner (pas de swap)\n",
    "    # target_A_wins = 0 ‚Üí A = loser (swap effectu√©)\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        # Probabilit√©s implicites\n",
    "        pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "          .then(pl.col(\"odds_implied_prob_winner\"))\n",
    "          .otherwise(pl.col(\"odds_implied_prob_loser\"))\n",
    "          .cast(pl.Float32)\n",
    "          .alias(\"odds_implied_prob_A\"),\n",
    "        \n",
    "        pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "          .then(pl.col(\"odds_implied_prob_loser\"))\n",
    "          .otherwise(pl.col(\"odds_implied_prob_winner\"))\n",
    "          .cast(pl.Float32)\n",
    "          .alias(\"odds_implied_prob_B\"),\n",
    "        \n",
    "        # Cotes brutes (si disponibles)\n",
    "        pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "          .then(pl.col(\"odds_winner\"))\n",
    "          .otherwise(pl.col(\"odds_loser\"))\n",
    "          .cast(pl.Float32)\n",
    "          .alias(\"odds_A\"),\n",
    "        \n",
    "        pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "          .then(pl.col(\"odds_loser\"))\n",
    "          .otherwise(pl.col(\"odds_winner\"))\n",
    "          .cast(pl.Float32)\n",
    "          .alias(\"odds_B\"),\n",
    "    ])\n",
    "    \n",
    "    # Features d√©riv√©es\n",
    "    df = df.with_columns([\n",
    "        # Diff√©rence de probabilit√© implicite\n",
    "        (pl.col(\"odds_implied_prob_A\") - pl.col(\"odds_implied_prob_B\"))\n",
    "        .cast(pl.Float32)\n",
    "        .alias(\"diff_odds_implied_prob\"),\n",
    "        \n",
    "        # Log odds ratio (utile pour le mod√®le)\n",
    "        (pl.col(\"odds_B\").log() - pl.col(\"odds_A\").log())\n",
    "        .cast(pl.Float32)\n",
    "        .alias(\"log_odds_ratio_AB\"),\n",
    "    ])\n",
    "    \n",
    "    # Supprimer les colonnes winner/loser (√©viter confusion)\n",
    "    cols_to_drop = [\"odds_winner\", \"odds_loser\", \n",
    "                    \"odds_implied_prob_winner\", \"odds_implied_prob_loser\"]\n",
    "    df = df.drop([c for c in cols_to_drop if c in df.columns])\n",
    "    \n",
    "    # Stats\n",
    "    n_with_odds = df[\"odds_implied_prob_A\"].is_not_null().sum()\n",
    "    print(f\"  ‚úÖ Odds converted to A/B format\")\n",
    "    print(f\"  üìä Matchs with odds: {n_with_odds:,}/{len(df):,} ({100*n_with_odds/len(df):.1f}%)\")\n",
    "    \n",
    "    # V√©rification anti-leakage\n",
    "    corr = df.select(pl.corr(\"target_A_wins\", \"odds_implied_prob_A\")).item()\n",
    "    print(f\"  üîç Anti-leakage: corr(target, odds_implied_prob_A) = {corr:.4f}\")\n",
    "    \n",
    "    if abs(corr) > 0.5:\n",
    "        print(\"  ‚ö†Ô∏è WARNING: High correlation detected!\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Correlation OK - no obvious leakage\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "# ===============================================\n",
    "# SECTION 4: TEMPORAL SPLIT\n",
    "# ===============================================\n",
    "\n",
    "def temporal_split(df: pl.DataFrame) -> tuple:\n",
    "    \"\"\"Split temporel strict.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   SECTION 4: TEMPORAL SPLIT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    train = df.filter(pl.col(\"year\") <= TRAIN_END_YEAR)\n",
    "    val = df.filter((pl.col(\"year\") > TRAIN_END_YEAR) & (pl.col(\"year\") <= VAL_END_YEAR))\n",
    "    test = df.filter(pl.col(\"year\") > VAL_END_YEAR)\n",
    "    \n",
    "    print(f\"\\n  Train: {len(train):,} rows (‚â§{TRAIN_END_YEAR})\")\n",
    "    print(f\"  Val:   {len(val):,} rows ({TRAIN_END_YEAR+1}-{VAL_END_YEAR})\")\n",
    "    print(f\"  Test:  {len(test):,} rows (>{VAL_END_YEAR})\")\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 5: FEATURE SELECTION\n",
    "# ===============================================\n",
    "\n",
    "def select_features(train: pl.DataFrame, val: pl.DataFrame, test: pl.DataFrame) -> tuple:\n",
    "    \"\"\"Feature selection with SOTA features protection.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   SECTION 5: FEATURE SELECTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    exclude_cols = [\"target_A_wins\", \"year\", \"tourney_date_ta\", \"gender\", \n",
    "                    \"tourney_surface_ta\", \"tourney_level_ta\", \"round_ta\",\n",
    "                    \"best_of_ta\", \"is_indoor\", \"match_status\"]\n",
    "    exclude_cols += ID_COLS\n",
    "    \n",
    "    numeric_types = [pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64]\n",
    "    numeric_cols = [c for c in train.columns \n",
    "                    if c not in exclude_cols\n",
    "                    and train[c].dtype in numeric_types]\n",
    "    \n",
    "    print(f\"\\n[5.1] Initial numeric features: {len(numeric_cols)}\")\n",
    "    \n",
    "    # Identify protected SOTA features that exist in data\n",
    "    protected_in_data = [c for c in PROTECTED_SOTA_FEATURES if c in train.columns]\n",
    "    print(f\"[5.1b] Protected SOTA features found: {len(protected_in_data)}\")\n",
    "    for f in protected_in_data:\n",
    "        coverage = train[f].is_not_null().mean()\n",
    "        print(f\"       {f}: {coverage:.1%} coverage\")\n",
    "    \n",
    "    # Separate protected from regular features\n",
    "    regular_cols = [c for c in numeric_cols if c not in protected_in_data]\n",
    "    \n",
    "    # Remove high null rate (only for non-protected features)\n",
    "    null_exprs = [pl.col(c).is_null().mean().alias(c) for c in regular_cols]\n",
    "    null_rates_df = train.select(null_exprs)\n",
    "    null_rates = {c: null_rates_df[c][0] for c in regular_cols}\n",
    "    \n",
    "    keep_cols = [c for c in regular_cols if (null_rates[c] or 0) <= MAX_NULL_RATE]\n",
    "    print(f\"[5.2] After null filter (regular): {len(keep_cols)}\")\n",
    "    \n",
    "    # Remove near-constant (only for non-protected features)\n",
    "    var_exprs = [pl.col(c).var().alias(c) for c in keep_cols]\n",
    "    var_df = train.select(var_exprs)\n",
    "    variances = {c: var_df[c][0] for c in keep_cols}\n",
    "    \n",
    "    keep_cols = [c for c in keep_cols if (variances[c] or 0) >= MIN_VARIANCE_THRESHOLD]\n",
    "    print(f\"[5.3] After variance filter (regular): {len(keep_cols)}\")\n",
    "    \n",
    "    # Feature importance via correlation\n",
    "    correlations = {}\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(keep_cols), batch_size):\n",
    "        batch_cols = keep_cols[i:i+batch_size]\n",
    "        corr_exprs = [pl.corr(\"target_A_wins\", c).alias(c) for c in batch_cols]\n",
    "        corr_df = train.select(corr_exprs)\n",
    "        for c in batch_cols:\n",
    "            corr_val = corr_df[c][0]  # Renamed to avoid shadowing 'val' DataFrame\n",
    "            correlations[c] = abs(corr_val) if corr_val is not None and not np.isnan(corr_val) else 0.0\n",
    "    \n",
    "    # Also compute correlations for protected features (for reporting)\n",
    "    for c in protected_in_data:\n",
    "        try:\n",
    "            corr_val = train.select(pl.corr(\"target_A_wins\", c))[0, 0]\n",
    "            correlations[c] = abs(corr_val) if corr_val is not None and not np.isnan(corr_val) else 0.0\n",
    "        except:\n",
    "            correlations[c] = 0.0\n",
    "    \n",
    "    sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\n  Top 10 by correlation:\")\n",
    "    for feat, corr in sorted_features[:10]:\n",
    "        protected_mark = \" [SOTA]\" if feat in protected_in_data else \"\"\n",
    "        print(f\"    {feat}: {corr:.4f}{protected_mark}\")\n",
    "    \n",
    "    # Select top N regular features + ALL protected SOTA features\n",
    "    n_regular = MAX_FEATURES - len(protected_in_data)\n",
    "    regular_selected = [f for f, _ in sorted_features if f not in protected_in_data][:n_regular]\n",
    "    \n",
    "    selected_features = protected_in_data + regular_selected\n",
    "    print(f\"\\n  Selected: {len(selected_features)} features\")\n",
    "    print(f\"    - Protected SOTA: {len(protected_in_data)}\")\n",
    "    print(f\"    - Regular (top by corr): {len(regular_selected)}\")\n",
    "    \n",
    "    # Apply selection\n",
    "    keep_all = [\"target_A_wins\", \"year\"] + [c for c in ID_COLS if c in train.columns] + selected_features\n",
    "    keep_all = list(dict.fromkeys([c for c in keep_all if c in train.columns]))\n",
    "    \n",
    "    train_sel = train.select(keep_all)\n",
    "    val_sel = val.select([c for c in keep_all if c in val.columns])\n",
    "    test_sel = test.select([c for c in keep_all if c in test.columns])\n",
    "    \n",
    "    return train_sel, val_sel, test_sel, selected_features\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 6: SCALING\n",
    "# ===============================================\n",
    "\n",
    "def apply_scaling(train: pl.DataFrame, val: pl.DataFrame, test: pl.DataFrame, \n",
    "                  feature_cols: list) -> tuple:\n",
    "    \"\"\"Applique QuantileTransformer avec imputation sp√©ciale pour SOTA features.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   SECTION 6: QUANTILE SCALING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    from sklearn.preprocessing import QuantileTransformer\n",
    "    import joblib\n",
    "    \n",
    "    numeric_types = [pl.Float32, pl.Float64, pl.Int32, pl.Int64, pl.Int16, pl.Int8]\n",
    "    scale_cols = [c for c in feature_cols \n",
    "                  if c not in ID_COLS + [\"target_A_wins\", \"year\"]\n",
    "                  and c in train.columns\n",
    "                  and train[c].dtype in numeric_types]\n",
    "    \n",
    "    print(f\"\\n  Scaling {len(scale_cols)} features\")\n",
    "    \n",
    "    # Compute medians for regular imputation\n",
    "    median_exprs = [pl.col(c).median().alias(c) for c in scale_cols]\n",
    "    medians_df = train.select(median_exprs)\n",
    "    medians = {c: medians_df[c][0] for c in scale_cols}\n",
    "    \n",
    "    # Special handling for SOTA features with high null rates\n",
    "    # Use sensible defaults instead of median (which might be null)\n",
    "    sota_defaults = {\n",
    "        \"r20_win_rate_vs_top10_A\": 0.3,  # Below average (most players lose vs top10)\n",
    "        \"r20_win_rate_vs_top10_B\": 0.3,\n",
    "        \"r20_win_rate_vs_top50_A\": 0.4,\n",
    "        \"r20_win_rate_vs_top50_B\": 0.4,\n",
    "        \"r20_upset_rate_A\": 0.3,\n",
    "        \"r20_upset_rate_B\": 0.3,\n",
    "        \"pref_ssi_A\": 0.0,  # Neutral preference\n",
    "        \"pref_ssi_B\": 0.0,\n",
    "        \"diff_pref_ssi\": 0.0,\n",
    "        \"diff_ssi_mismatch\": 0.0,\n",
    "        \"tourney_speed_index\": 0.0,  # Neutral speed\n",
    "    }\n",
    "    \n",
    "    for col, default in sota_defaults.items():\n",
    "        if col in medians:\n",
    "            if medians[col] is None or np.isnan(medians[col]):\n",
    "                medians[col] = default\n",
    "                print(f\"  ‚ö†Ô∏è {col}: using default {default} (high null rate)\")\n",
    "    \n",
    "    # Impute with combined medians/defaults\n",
    "    impute_exprs = [pl.col(c).fill_null(medians.get(c, 0.0)).alias(c) for c in scale_cols]\n",
    "    train = train.with_columns(impute_exprs)\n",
    "    val = val.with_columns([pl.col(c).fill_null(medians.get(c, 0.0)).alias(c) for c in scale_cols if c in val.columns])\n",
    "    test = test.with_columns([pl.col(c).fill_null(medians.get(c, 0.0)).alias(c) for c in scale_cols if c in test.columns])\n",
    "    \n",
    "    # Scale\n",
    "    train_np = train.select(scale_cols).to_numpy()\n",
    "    \n",
    "    scaler = QuantileTransformer(\n",
    "        n_quantiles=min(1000, len(train)),\n",
    "        output_distribution='normal',\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    train_scaled = scaler.fit_transform(train_np)\n",
    "    del train_np\n",
    "    \n",
    "    for i, col in enumerate(scale_cols):\n",
    "        train = train.with_columns([pl.Series(col, train_scaled[:, i].astype(np.float32))])\n",
    "    del train_scaled\n",
    "    \n",
    "    # Transform val/test\n",
    "    for df_name, df_ref in [(\"val\", val), (\"test\", test)]:\n",
    "        df_cols = [c for c in scale_cols if c in df_ref.columns]\n",
    "        if df_cols:\n",
    "            df_np = df_ref.select(df_cols).to_numpy()\n",
    "            df_full = np.zeros((len(df_ref), len(scale_cols)), dtype=np.float32)\n",
    "            for i, col in enumerate(scale_cols):\n",
    "                if col in df_cols:\n",
    "                    df_full[:, i] = df_np[:, df_cols.index(col)]\n",
    "            \n",
    "            df_scaled = scaler.transform(df_full)\n",
    "            del df_np, df_full\n",
    "            \n",
    "            for i, col in enumerate(scale_cols):\n",
    "                if col in df_ref.columns:\n",
    "                    if df_name == \"val\":\n",
    "                        val = val.with_columns([pl.Series(col, df_scaled[:, i].astype(np.float32))])\n",
    "                    else:\n",
    "                        test = test.with_columns([pl.Series(col, df_scaled[:, i].astype(np.float32))])\n",
    "            del df_scaled\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_path = OUTPUT_DIR / \"quantile_scaler.joblib\"\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"  Scaler saved: {scaler_path}\")\n",
    "    \n",
    "    # Save medians\n",
    "    medians_path = OUTPUT_DIR / \"imputation_medians.json\"\n",
    "    medians_serializable = {k: float(v) if v is not None else 0.0 for k, v in medians.items()}\n",
    "    medians_path.write_text(json.dumps(medians_serializable, indent=2))\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# SECTION 7: EXPORT\n",
    "# ===============================================\n",
    "\n",
    "def export_datasets(train: pl.DataFrame, val: pl.DataFrame, test: pl.DataFrame, feature_cols: list):\n",
    "    \"\"\"Export final datasets.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   SECTION 7: EXPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    train.write_parquet(OUTPUT_DIR / \"train.parquet\")\n",
    "    val.write_parquet(OUTPUT_DIR / \"val.parquet\")\n",
    "    test.write_parquet(OUTPUT_DIR / \"test.parquet\")\n",
    "    \n",
    "    (OUTPUT_DIR / \"feature_list.json\").write_text(json.dumps(feature_cols, indent=2))\n",
    "    \n",
    "    metadata = {\n",
    "        \"created\": datetime.now().isoformat(),\n",
    "        \"train_rows\": len(train),\n",
    "        \"val_rows\": len(val),\n",
    "        \"test_rows\": len(test),\n",
    "        \"n_features\": len(feature_cols),\n",
    "        \"train_end_year\": TRAIN_END_YEAR,\n",
    "        \"val_end_year\": VAL_END_YEAR,\n",
    "        \"pipeline\": \"preprocess3_ULTRA_GODMODE_SOTA2026\",\n",
    "    }\n",
    "    (OUTPUT_DIR / \"metadata.json\").write_text(json.dumps(metadata, indent=2))\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Exported to {OUTPUT_DIR}\")\n",
    "    print(f\"     train.parquet: {len(train):,} rows\")\n",
    "    print(f\"     val.parquet: {len(val):,} rows\")\n",
    "    print(f\"     test.parquet: {len(test):,} rows\")\n",
    "\n",
    "def convert_new_features_to_AB(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Convertit les nouvelles features GOD (PP_12-15) de winner/loser vers A/B.\n",
    "    IMPORTANT: √Ä appeler APR√àS le shuffle (quand target_A_wins existe).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   CONVERSION GOD FEATURES: winner/loser ‚Üí A/B\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    converted_count = 0\n",
    "    \n",
    "    # Features avec paires winner/loser\n",
    "    winner_cols = [c for c in df.columns if c.endswith(\"_winner\")]\n",
    "    \n",
    "    for winner_col in winner_cols:\n",
    "        loser_col = winner_col.replace(\"_winner\", \"_loser\")\n",
    "        \n",
    "        if loser_col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        a_col = winner_col.replace(\"_winner\", \"_A\")\n",
    "        b_col = winner_col.replace(\"_winner\", \"_B\")\n",
    "        \n",
    "        if a_col in df.columns:\n",
    "            continue\n",
    "        \n",
    "        df = df.with_columns([\n",
    "            pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "                .then(pl.col(winner_col))\n",
    "                .otherwise(pl.col(loser_col))\n",
    "                .alias(a_col),\n",
    "            \n",
    "            pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "                .then(pl.col(loser_col))\n",
    "                .otherwise(pl.col(winner_col))\n",
    "                .alias(b_col),\n",
    "        ])\n",
    "        converted_count += 2\n",
    "    \n",
    "    # Features \"diff\" √† inverser si swap\n",
    "    diff_features = [\"bt_diff\", \"bt_recent_diff\", \"bt_surface_diff\",\n",
    "                     \"travel_advantage_winner\", \"timezone_advantage_winner\", \n",
    "                     \"rest_advantage_winner\"]\n",
    "    \n",
    "    for col in diff_features:\n",
    "        if col in df.columns:\n",
    "            new_col = col.replace(\"_winner\", \"\") + \"_A\" if \"_winner\" in col else col\n",
    "            df = df.with_columns([\n",
    "                pl.when(pl.col(\"target_A_wins\") == 1)\n",
    "                    .then(pl.col(col))\n",
    "                    .otherwise(-pl.col(col))\n",
    "                    .alias(new_col)\n",
    "            ])\n",
    "            converted_count += 1\n",
    "    \n",
    "    print(f\"  ‚úÖ Converted {converted_count} features from winner/loser to A/B\")\n",
    "    \n",
    "    return df\n",
    "# ===============================================\n",
    "# MAIN\n",
    "# ===============================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline complet.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   PREPROCESS 9 - ULTRA GOD MODE SOTA 2026\")\n",
    "    print(\"   TennisTitan - COMPLETE Feature Engineering\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    # 1. Load\n",
    "    df = load_dataset()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 2. Add ALL missing features (Cellules A-E)\n",
    "    df = add_cellule_a_features(df)\n",
    "    gc.collect()\n",
    "    \n",
    "    df = add_cellule_b_features(df)\n",
    "    gc.collect()\n",
    "    \n",
    "    df = add_cellule_c_features(df)\n",
    "    gc.collect()\n",
    "    \n",
    "    df = add_cellule_d_features(df)\n",
    "    gc.collect()\n",
    "    \n",
    "    df = add_cellule_e_features(df)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print feature summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   FEATURE SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"  Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # 3. Shuffle\n",
    "    df = shuffle_ab(df, seed=RANDOM_SEED)\n",
    "    gc.collect()\n",
    "\n",
    "    # ========== NOUVEAU: CONVERSION GOD FEATURES ==========\n",
    "    df = convert_new_features_to_AB(df)\n",
    "    gc.collect()\n",
    "    # ======================================================\n",
    "\n",
    "    # ========== NOUVEAU: POST-SHUFFLE RANK FEATURES ==========\n",
    "    df = add_post_shuffle_rank_features(df)\n",
    "    gc.collect()\n",
    "    # =========================================================\n",
    "\n",
    "    # ========== NOUVEAU: CONVERT ODDS WINNER/LOSER ‚Üí A/B ==========\n",
    "    df = convert_odds_to_AB(df)\n",
    "    gc.collect()\n",
    "    # ==============================================================\n",
    "    \n",
    "    # Split temporel\n",
    "    train, val, test = temporal_split(df)\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # 5. Select\n",
    "    train, val, test, feature_cols = select_features(train, val, test)\n",
    "    gc.collect()\n",
    "    \n",
    "    # 6. Scale\n",
    "    train, val, test = apply_scaling(train, val, test, feature_cols)\n",
    "    gc.collect()\n",
    "    \n",
    "    # 7. Export\n",
    "    export_datasets(train, val, test, feature_cols)\n",
    "    \n",
    "    elapsed = time.perf_counter() - t0\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"   COMPLETE! Total time: {elapsed:.1f}s\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nüìä FINAL SUMMARY:\")\n",
    "    print(f\"   Train: {len(train):,} rows\")\n",
    "    print(f\"   Val: {len(val):,} rows\")\n",
    "    print(f\"   Test: {len(test):,} rows\")\n",
    "    print(f\"   Features: {len(feature_cols)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
