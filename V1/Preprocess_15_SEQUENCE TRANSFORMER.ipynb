{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b809f18-c9f2-40df-a1dd-1a9302baf362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "   PP_15 - SEQUENCE TRANSFORMER (GOD SOTA 2026 v4 FINAL)\n",
      "======================================================================\n",
      "   2025-12-17 08:41:19\n",
      "   Device: cuda\n",
      "   TF32: True\n",
      "   Feature dim: 13\n",
      "======================================================================\n",
      "\n",
      "[1/5] Loading matches...\n",
      "  Matches: 544,245\n",
      "\n",
      "[2/5] Building dataset...\n",
      "\n",
      "[Dataset] Building dataset (single pass, pre-filtered)...\n",
      "  Cutoff date: 2024-01-22 (matches on this date ignored)\n",
      "  Valid matches: 544,245\n",
      "  Train: 489,820 | Val: 54,425\n",
      "  Train period: 1942-08-01 ‚Üí 2024-01-22\n",
      "  Val period: 2024-01-22 ‚Üí 2025-09-01\n",
      "  Train dataset: 873,736\n",
      "  Val dataset: 100,378\n",
      "\n",
      "[3/5] Training transformer...\n",
      "\n",
      "==================================================\n",
      "  TRAINING SEQUENCE TRANSFORMER (v4 FINAL)\n",
      "==================================================\n",
      "  Params: 79,969\n",
      "  AMP: True (dtype=torch.bfloat16)\n",
      "  Epoch   5: loss=0.6121, AUC=0.7253 | val_loss=0.5888, val_acc=0.6928, val_AUC=0.7581\n",
      "  Epoch  10: loss=0.5023, AUC=0.8343 | val_loss=0.4600, val_acc=0.7920, val_AUC=0.8775\n",
      "  Epoch  15: loss=0.4576, AUC=0.8653 | val_loss=0.4126, val_acc=0.8082, val_AUC=0.8967\n",
      "  Epoch  20: loss=0.4468, AUC=0.8719 | val_loss=0.4180, val_acc=0.8080, val_AUC=0.8968\n",
      "  Epoch  25: loss=0.4415, AUC=0.8750 | val_loss=0.4122, val_acc=0.8092, val_AUC=0.8980\n",
      "  Epoch  30: loss=0.4381, AUC=0.8770 | val_loss=0.4149, val_acc=0.8102, val_AUC=0.8982\n",
      "  Epoch  35: loss=0.4355, AUC=0.8786 | val_loss=0.4146, val_acc=0.8100, val_AUC=0.8987\n",
      "  Epoch  40: loss=0.4338, AUC=0.8795 | val_loss=0.4002, val_acc=0.8118, val_AUC=0.9016\n",
      "  Epoch  45: loss=0.4322, AUC=0.8804 | val_loss=0.4130, val_acc=0.8102, val_AUC=0.8988\n",
      "  Epoch  50: loss=0.4308, AUC=0.8811 | val_loss=0.4007, val_acc=0.8123, val_AUC=0.9023\n",
      "  Epoch  55: loss=0.4304, AUC=0.8814 | val_loss=0.3988, val_acc=0.8134, val_AUC=0.9030\n",
      "  Epoch  60: loss=0.4291, AUC=0.8821 | val_loss=0.3974, val_acc=0.8134, val_AUC=0.9035\n",
      "  Epoch  65: loss=0.4281, AUC=0.8827 | val_loss=0.4035, val_acc=0.8132, val_AUC=0.9024\n",
      "  Epoch  70: loss=0.4275, AUC=0.8829 | val_loss=0.4039, val_acc=0.8137, val_AUC=0.9030\n",
      "  Epoch  75: loss=0.4263, AUC=0.8835 | val_loss=0.3980, val_acc=0.8142, val_AUC=0.9036\n",
      "  Epoch  80: loss=0.4260, AUC=0.8837 | val_loss=0.4135, val_acc=0.8127, val_AUC=0.9005\n",
      "  Early stopping at epoch 80\n",
      "\n",
      "  ‚úÖ Best val AUC: 0.9035 (epoch 60)\n",
      "\n",
      "[4/5] Extracting features...\n",
      "\n",
      "[Features] Extracting sequence features (batched)...\n",
      "  Extracted: 486,704 matches\n",
      "  Coverage: 89.5%\n",
      "\n",
      "[5/5] Saving...\n",
      "  ‚úÖ Saved: C:\\Users\\Administrateur\\Tennis POLAR v2\\data_clean\\features\\sequence_transformer\\sequence_features.parquet\n",
      "\n",
      "======================================================================\n",
      "   ‚úÖ PP_15 SEQUENCE TRANSFORMER GOD SOTA v4 COMPLETE!\n",
      "======================================================================\n",
      "   ‚è±Ô∏è  Time: 8695.1s\n",
      "   üìä Train: 873,736 | Val: 100,378\n",
      "\n",
      "üìã v4 FINAL CORRECTIONS:\n",
      "   ‚úÖ Assert len == FEATURE_DIM\n",
      "   ‚úÖ Split sur liste PR√â-FILTR√âE\n",
      "   ‚úÖ AMP dtype explicite (bf16/fp16)\n",
      "   ‚úÖ match_id s√©curis√© str()\n",
      "   ‚úÖ Dropout coh√©rent (self.dropout_p)\n",
      "   ‚úÖ TF32 enabled\n",
      "   ‚úÖ Single pass chronologique\n",
      "   ‚úÖ Mask dans extraction\n",
      "   ‚úÖ Extraction batch√©e\n",
      "   ‚úÖ LayerNorm\n",
      "\n",
      "üîÑ NEXT: PP_16 (Merge GOD Features)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ===============================================\n",
    "# PP_15 - SEQUENCE TRANSFORMER (GOD SOTA 2026 v4)\n",
    "# VERSION FINALE BULLETPROOF\n",
    "# ===============================================\n",
    "#\n",
    "# CORRECTIONS v4:\n",
    "# ‚úÖ Assert len == FEATURE_DIM\n",
    "# ‚úÖ Split sur liste PR√â-FILTR√âE\n",
    "# ‚úÖ AMP avec dtype explicite\n",
    "# ‚úÖ match_id s√©curis√© str()\n",
    "# ‚úÖ Dropout coh√©rent (self.dropout_p)\n",
    "# ‚úÖ TF32 enabled\n",
    "# + Toutes les corrections v3\n",
    "#\n",
    "# Output: features/sequence_transformer/\n",
    "# ===============================================\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    HAS_SKLEARN = True\n",
    "except ImportError:\n",
    "    HAS_SKLEARN = False\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURATION\n",
    "# ===============================================\n",
    "ROOT = Path(r\"C:\\Users\\Administrateur\\Tennis POLAR v2\")\n",
    "DATA_CLEAN = ROOT / \"data_clean\"\n",
    "MATCHES_BASE = DATA_CLEAN / \"matches_base\"\n",
    "OUTPUT_DIR = DATA_CLEAN / \"features\" / \"sequence_transformer\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEQ_LENGTH = 20\n",
    "FEATURE_DIM = 13  # 8 stats + 4 surface + 1 result + 1 opp_rank\n",
    "\n",
    "D_MODEL = 64\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "D_FF = 128\n",
    "DROPOUT = 0.2\n",
    "EMBEDDING_DIM = 32\n",
    "\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 512\n",
    "WEIGHT_DECAY = 1e-5\n",
    "PATIENCE = 20\n",
    "WARMUP_EPOCHS = 10\n",
    "\n",
    "TRAIN_RATIO = 0.9\n",
    "USE_AMP = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ‚úÖ TF32 pour boost perf\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"   PP_15 - SEQUENCE TRANSFORMER (GOD SOTA 2026 v4 FINAL)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   TF32: {DEVICE.type == 'cuda'}\")\n",
    "print(f\"   Feature dim: {FEATURE_DIM}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# TRANSFORMER MODEL\n",
    "# ===============================================\n",
    "\n",
    "class TemporalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_days: int = 365):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(SEQ_LENGTH + 1, d_model)\n",
    "        self.days_proj = nn.Linear(1, d_model)\n",
    "        self.max_days = max_days\n",
    "        \n",
    "        nn.init.normal_(self.pos_embedding.weight, std=0.01)\n",
    "        nn.init.xavier_uniform_(self.days_proj.weight, gain=0.5)\n",
    "    \n",
    "    def forward(self, x, days_ago):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_enc = self.pos_embedding(positions)\n",
    "        \n",
    "        days_clamped = torch.clamp(days_ago, 0, self.max_days * 3)\n",
    "        days_normalized = days_clamped.float().unsqueeze(-1) / self.max_days\n",
    "        days_enc = self.days_proj(days_normalized)\n",
    "        \n",
    "        return x + pos_enc + days_enc\n",
    "\n",
    "\n",
    "class SequenceTransformer(nn.Module):\n",
    "    def __init__(self, input_dim: int, d_model: int, n_heads: int,\n",
    "                 n_layers: int, d_ff: int, dropout: float, output_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout_p = dropout  # ‚úÖ Stock√© pour utilisation coh√©rente\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.input_norm = nn.LayerNorm(d_model)\n",
    "        self.pos_encoding = TemporalPositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model) * 0.01)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.input_proj.weight, gain=0.5)\n",
    "        for m in self.output_proj:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.5)\n",
    "    \n",
    "    def forward(self, x, days_ago, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        if torch.isnan(x).any():\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        x = self.input_norm(x)\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training)  # ‚úÖ Dropout coh√©rent\n",
    "        \n",
    "        x = self.pos_encoding(x, days_ago)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            cls_mask = torch.zeros(batch_size, 1, dtype=torch.bool, device=x.device)\n",
    "            mask = torch.cat([cls_mask, mask], dim=1)\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        cls_output = x[:, 0, :]\n",
    "        embedding = self.output_proj(cls_output)\n",
    "        embedding = torch.clamp(embedding, -10, 10)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class RankingPredictor(nn.Module):\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.score_net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        for m in self.score_net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.5)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, emb_a, emb_b):\n",
    "        score_a = self.score_net(emb_a).squeeze(-1)\n",
    "        score_b = self.score_net(emb_b).squeeze(-1)\n",
    "        return torch.clamp(score_a - score_b, -20, 20)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# DATA - Construction single pass\n",
    "# ===============================================\n",
    "\n",
    "def build_dataset_single_pass(matches_df: pl.DataFrame):\n",
    "    \"\"\"Single pass chronologique avec liste PR√â-FILTR√âE.\"\"\"\n",
    "    \n",
    "    print(\"\\n[Dataset] Building dataset (single pass, pre-filtered)...\")\n",
    "    \n",
    "    # ‚úÖ PR√â-FILTRER avant split pour avoir un cutoff exact\n",
    "    matches_df = matches_df.sort(\"tourney_date_ta\")\n",
    "    all_matches = matches_df.to_dicts()\n",
    "    \n",
    "    matches_list = [\n",
    "        r for r in all_matches\n",
    "        if r[\"winner_id\"] and r[\"loser_id\"] and r[\"tourney_date_ta\"]\n",
    "    ]\n",
    "    \n",
    "    # ‚úÖ Split sur liste filtr√©e\n",
    "    split_idx = int(len(matches_list) * TRAIN_RATIO)\n",
    "    cutoff_date = matches_list[split_idx][\"tourney_date_ta\"]\n",
    "    print(f\"  Cutoff date: {cutoff_date} (matches on this date ignored)\")\n",
    "    print(f\"  Valid matches: {len(matches_list):,}\")\n",
    "    print(f\"  Train: {split_idx:,} | Val: {len(matches_list) - split_idx:,}\")\n",
    "    \n",
    "    if len(matches_list) > 0 and split_idx > 0 and split_idx < len(matches_list):\n",
    "        print(f\"  Train period: {matches_list[0]['tourney_date_ta']} ‚Üí {matches_list[split_idx-1]['tourney_date_ta']}\")\n",
    "        print(f\"  Val period: {matches_list[split_idx]['tourney_date_ta']} ‚Üí {matches_list[-1]['tourney_date_ta']}\")\n",
    "    \n",
    "    winner_features = [\n",
    "        \"w_s_ace_p\", \"w_s_df_p\", \"w_s_1stIn_p\", \"w_s_1stWon_p\", \n",
    "        \"w_s_2ndWon_p\", \"w_ret_1stWon_p\", \"w_ret_2ndWon_p\", \"w_bp_conv_p\"\n",
    "    ]\n",
    "    loser_features = [\n",
    "        \"l_s_ace_p\", \"l_s_df_p\", \"l_s_1stIn_p\", \"l_s_1stWon_p\",\n",
    "        \"l_s_2ndWon_p\", \"l_ret_1stWon_p\", \"l_ret_2ndWon_p\", \"l_bp_conv_p\"\n",
    "    ]\n",
    "    \n",
    "    player_history = defaultdict(list)\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    for idx, row in enumerate(matches_list):\n",
    "        match_id = row[\"custom_match_id\"]\n",
    "        winner_id = row[\"winner_id\"]\n",
    "        loser_id = row[\"loser_id\"]\n",
    "        match_date = row[\"tourney_date_ta\"]\n",
    "        \n",
    "        surface = row.get(\"tourney_surface_ta\") or row.get(\"surface\", \"Hard\")\n",
    "        surface_enc = {\n",
    "            \"Hard\": [1, 0, 0, 0],\n",
    "            \"Clay\": [0, 1, 0, 0],\n",
    "            \"Grass\": [0, 0, 1, 0],\n",
    "            \"Carpet\": [0, 0, 0, 1]\n",
    "        }.get(surface, [0.25, 0.25, 0.25, 0.25])\n",
    "        \n",
    "        winner_rank = row.get(\"winner_rank_ta\", 500)\n",
    "        loser_rank = row.get(\"loser_rank_ta\", 500)\n",
    "        if winner_rank is None or np.isnan(winner_rank): winner_rank = 500\n",
    "        if loser_rank is None or np.isnan(loser_rank): loser_rank = 500\n",
    "        \n",
    "        # R√©cup√©rer s√©quences AVANT ce match\n",
    "        w_past = player_history[winner_id][-SEQ_LENGTH:]\n",
    "        l_past = player_history[loser_id][-SEQ_LENGTH:]\n",
    "        \n",
    "        if len(w_past) >= 5 and len(l_past) >= 5:\n",
    "            w_feats = np.array([x[1] for x in w_past], dtype=np.float32)\n",
    "            l_feats = np.array([x[1] for x in l_past], dtype=np.float32)\n",
    "            \n",
    "            w_days = np.array([\n",
    "                min((match_date - x[0]).days, 1000) if x[0] else 30 \n",
    "                for x in w_past\n",
    "            ], dtype=np.float32)\n",
    "            l_days = np.array([\n",
    "                min((match_date - x[0]).days, 1000) if x[0] else 30 \n",
    "                for x in l_past\n",
    "            ], dtype=np.float32)\n",
    "            \n",
    "            item_forward = {\n",
    "                \"match_id\": match_id,\n",
    "                \"seq_a\": w_feats,\n",
    "                \"seq_b\": l_feats,\n",
    "                \"days_a\": w_days,\n",
    "                \"days_b\": l_days,\n",
    "                \"len_a\": len(w_past),\n",
    "                \"len_b\": len(l_past),\n",
    "                \"label\": 1,\n",
    "            }\n",
    "            item_reverse = {\n",
    "                \"match_id\": match_id,\n",
    "                \"seq_a\": l_feats,\n",
    "                \"seq_b\": w_feats,\n",
    "                \"days_a\": l_days,\n",
    "                \"days_b\": w_days,\n",
    "                \"len_a\": len(l_past),\n",
    "                \"len_b\": len(w_past),\n",
    "                \"label\": 0,\n",
    "            }\n",
    "            \n",
    "            if match_date < cutoff_date:\n",
    "                train_data.append(item_forward)\n",
    "                train_data.append(item_reverse)\n",
    "            elif match_date > cutoff_date:\n",
    "                val_data.append(item_forward)\n",
    "                val_data.append(item_reverse)\n",
    "        \n",
    "        # Construire features pour CE match\n",
    "        w_match_feats = []\n",
    "        for f in winner_features:\n",
    "            val = row.get(f, None)\n",
    "            if val is not None and not np.isnan(val):\n",
    "                val = np.clip(val, 0, 1)\n",
    "            else:\n",
    "                val = 0.5\n",
    "            w_match_feats.append(val)\n",
    "        w_match_feats.extend(surface_enc)\n",
    "        w_match_feats.append(np.log1p(loser_rank) / np.log1p(2000))\n",
    "        \n",
    "        l_match_feats = []\n",
    "        for f in loser_features:\n",
    "            val = row.get(f, None)\n",
    "            if val is not None and not np.isnan(val):\n",
    "                val = np.clip(val, 0, 1)\n",
    "            else:\n",
    "                val = 0.5\n",
    "            l_match_feats.append(val)\n",
    "        l_match_feats.extend(surface_enc)\n",
    "        l_match_feats.append(np.log1p(winner_rank) / np.log1p(2000))\n",
    "\n",
    "        \n",
    "        # ‚úÖ ASSERT pour v√©rifier la taille\n",
    "        assert len(w_match_feats) == FEATURE_DIM, f\"w_match_feats={len(w_match_feats)}, expected {FEATURE_DIM}\"\n",
    "        assert len(l_match_feats) == FEATURE_DIM, f\"l_match_feats={len(l_match_feats)}, expected {FEATURE_DIM}\"\n",
    "        \n",
    "        player_history[winner_id].append((match_date, w_match_feats))\n",
    "        player_history[loser_id].append((match_date, l_match_feats))\n",
    "    \n",
    "    print(f\"  Train dataset: {len(train_data):,}\")\n",
    "    print(f\"  Val dataset: {len(val_data):,}\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data: list, seq_length: int, feature_dim: int, return_match_id: bool = False):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.return_match_id = return_match_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        seq_a = self._pad_sequence(item[\"seq_a\"])\n",
    "        seq_b = self._pad_sequence(item[\"seq_b\"])\n",
    "        days_a = self._pad_days(item[\"days_a\"])\n",
    "        days_b = self._pad_days(item[\"days_b\"])\n",
    "        \n",
    "        len_a = min(item[\"len_a\"], self.seq_length)\n",
    "        len_b = min(item[\"len_b\"], self.seq_length)\n",
    "        pad_len_a = max(0, self.seq_length - len_a)\n",
    "        pad_len_b = max(0, self.seq_length - len_b)\n",
    "        \n",
    "        mask_a = np.zeros(self.seq_length, dtype=bool)\n",
    "        mask_a[:pad_len_a] = True\n",
    "        \n",
    "        mask_b = np.zeros(self.seq_length, dtype=bool)\n",
    "        mask_b[:pad_len_b] = True\n",
    "        \n",
    "        result = {\n",
    "            \"seq_a\": torch.tensor(seq_a, dtype=torch.float),\n",
    "            \"seq_b\": torch.tensor(seq_b, dtype=torch.float),\n",
    "            \"days_a\": torch.tensor(days_a, dtype=torch.float),\n",
    "            \"days_b\": torch.tensor(days_b, dtype=torch.float),\n",
    "            \"mask_a\": torch.tensor(mask_a, dtype=torch.bool),\n",
    "            \"mask_b\": torch.tensor(mask_b, dtype=torch.bool),\n",
    "            \"label\": torch.tensor(item[\"label\"], dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        if self.return_match_id:\n",
    "            result[\"match_id\"] = item[\"match_id\"]\n",
    "            result[\"is_forward\"] = item[\"label\"]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _pad_sequence(self, seq):\n",
    "        if len(seq) >= self.seq_length:\n",
    "            return seq[-self.seq_length:]\n",
    "        else:\n",
    "            pad = np.zeros((self.seq_length - len(seq), self.feature_dim), dtype=np.float32)\n",
    "            return np.concatenate([pad, seq], axis=0)\n",
    "    \n",
    "    def _pad_days(self, days):\n",
    "        if len(days) >= self.seq_length:\n",
    "            return days[-self.seq_length:]\n",
    "        else:\n",
    "            pad = np.full(self.seq_length - len(days), 365, dtype=np.float32)\n",
    "            return np.concatenate([pad, days], axis=0)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# TRAINING\n",
    "# ===============================================\n",
    "\n",
    "def compute_auc(y_true, y_proba):\n",
    "    if HAS_SKLEARN and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_proba)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return 0.5\n",
    "\n",
    "\n",
    "def get_amp_dtype():\n",
    "    \"\"\"‚úÖ Dtype explicite pour AMP.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            return torch.bfloat16\n",
    "        return torch.float16\n",
    "    return torch.float32\n",
    "\n",
    "\n",
    "def train_transformer(train_loader, val_loader):\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"  TRAINING SEQUENCE TRANSFORMER (v4 FINAL)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    transformer = SequenceTransformer(\n",
    "        input_dim=FEATURE_DIM,\n",
    "        d_model=D_MODEL,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        dropout=DROPOUT,\n",
    "        output_dim=EMBEDDING_DIM\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    predictor = RankingPredictor(EMBEDDING_DIM).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    params = list(transformer.parameters()) + list(predictor.parameters())\n",
    "    optimizer = torch.optim.AdamW(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < WARMUP_EPOCHS:\n",
    "            return (epoch + 1) / WARMUP_EPOCHS\n",
    "        return 1.0\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # ‚úÖ AMP avec dtype explicite\n",
    "    use_amp = USE_AMP and DEVICE.type == 'cuda'\n",
    "    amp_dtype = get_amp_dtype() if use_amp else torch.float32\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and amp_dtype == torch.float16))\n",
    "    \n",
    "    print(f\"  Params: {sum(p.numel() for p in params):,}\")\n",
    "    print(f\"  AMP: {use_amp} (dtype={amp_dtype})\")\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        transformer.train()\n",
    "        predictor.train()\n",
    "        \n",
    "        train_losses = []\n",
    "        train_preds_all = []\n",
    "        train_labels_all = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            seq_a = batch[\"seq_a\"].to(DEVICE)\n",
    "            seq_b = batch[\"seq_b\"].to(DEVICE)\n",
    "            days_a = batch[\"days_a\"].to(DEVICE)\n",
    "            days_b = batch[\"days_b\"].to(DEVICE)\n",
    "            mask_a = batch[\"mask_a\"].to(DEVICE)\n",
    "            mask_b = batch[\"mask_b\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                # ‚úÖ AMP avec dtype explicite\n",
    "                with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                    emb_a = transformer(seq_a, days_a, mask_a)\n",
    "                    emb_b = transformer(seq_b, days_b, mask_b)\n",
    "                    logits = predictor(emb_a, emb_b)\n",
    "                    loss = criterion(logits, labels)\n",
    "                \n",
    "                if torch.isnan(loss):\n",
    "                    continue\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(params, 0.5)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                emb_a = transformer(seq_a, days_a, mask_a)\n",
    "                emb_b = transformer(seq_b, days_b, mask_b)\n",
    "                \n",
    "                if torch.isnan(emb_a).any() or torch.isnan(emb_b).any():\n",
    "                    continue\n",
    "                \n",
    "                logits = predictor(emb_a, emb_b)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                if torch.isnan(loss):\n",
    "                    continue\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(params, 0.5)\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            with torch.no_grad():\n",
    "                train_preds_all.extend(torch.sigmoid(logits).float().cpu().numpy())\n",
    "            train_labels_all.extend(labels.float().cpu().numpy())\n",
    "        \n",
    "        if len(train_losses) == 0:\n",
    "            continue\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_auc = compute_auc(np.array(train_labels_all), np.array(train_preds_all))\n",
    "        \n",
    "        # Validation\n",
    "        transformer.eval()\n",
    "        predictor.eval()\n",
    "        \n",
    "        val_losses = []\n",
    "        val_preds_all = []\n",
    "        val_labels_all = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                seq_a = batch[\"seq_a\"].to(DEVICE)\n",
    "                seq_b = batch[\"seq_b\"].to(DEVICE)\n",
    "                days_a = batch[\"days_a\"].to(DEVICE)\n",
    "                days_b = batch[\"days_b\"].to(DEVICE)\n",
    "                mask_a = batch[\"mask_a\"].to(DEVICE)\n",
    "                mask_b = batch[\"mask_b\"].to(DEVICE)\n",
    "                labels = batch[\"label\"].to(DEVICE)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                        emb_a = transformer(seq_a, days_a, mask_a)\n",
    "                        emb_b = transformer(seq_b, days_b, mask_b)\n",
    "                        logits = predictor(emb_a, emb_b)\n",
    "                        loss = criterion(logits, labels)\n",
    "                else:\n",
    "                    emb_a = transformer(seq_a, days_a, mask_a)\n",
    "                    emb_b = transformer(seq_b, days_b, mask_b)\n",
    "                    logits = predictor(emb_a, emb_b)\n",
    "                    loss = criterion(logits, labels)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                val_preds_all.extend(torch.sigmoid(logits).float().cpu().numpy())\n",
    "                val_labels_all.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_preds_arr = np.array(val_preds_all)\n",
    "        val_labels_arr = np.array(val_labels_all)\n",
    "        val_auc = compute_auc(val_labels_arr, val_preds_arr)\n",
    "        val_acc = ((val_preds_arr > 0.5) == val_labels_arr).mean()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1:3d}: loss={train_loss:.4f}, AUC={train_auc:.4f} | val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, val_AUC={val_auc:.4f}\")\n",
    "        \n",
    "        if val_auc > best_val_auc + 0.001:\n",
    "            best_val_auc = val_auc\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            best_state = {\n",
    "                'transformer': transformer.state_dict(),\n",
    "                'predictor': predictor.state_dict()\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_state:\n",
    "        transformer.load_state_dict(best_state['transformer'])\n",
    "        predictor.load_state_dict(best_state['predictor'])\n",
    "    \n",
    "    print(f\"\\n  ‚úÖ Best val AUC: {best_val_auc:.4f} (epoch {best_epoch})\")\n",
    "    \n",
    "    return transformer, predictor\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# FEATURE EXTRACTION (batch√©e avec mask)\n",
    "# ===============================================\n",
    "\n",
    "def extract_sequence_features_batched(transformer, all_data, matches_df):\n",
    "    print(\"\\n[Features] Extracting sequence features (batched)...\")\n",
    "    \n",
    "    transformer.eval()\n",
    "    \n",
    "    forward_data = [item for item in all_data if item[\"label\"] == 1]\n",
    "    \n",
    "    if len(forward_data) == 0:\n",
    "        print(\"  ‚ö†Ô∏è No data to extract\")\n",
    "        return pl.DataFrame()\n",
    "    \n",
    "    extract_dataset = SequenceDataset(forward_data, SEQ_LENGTH, FEATURE_DIM, return_match_id=True)\n",
    "    extract_loader = DataLoader(extract_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    match_embeddings = {}\n",
    "    \n",
    "    use_amp = USE_AMP and DEVICE.type == 'cuda'\n",
    "    amp_dtype = get_amp_dtype() if use_amp else torch.float32\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in extract_loader:\n",
    "            seq_a = batch[\"seq_a\"].to(DEVICE)\n",
    "            seq_b = batch[\"seq_b\"].to(DEVICE)\n",
    "            days_a = batch[\"days_a\"].to(DEVICE)\n",
    "            days_b = batch[\"days_b\"].to(DEVICE)\n",
    "            mask_a = batch[\"mask_a\"].to(DEVICE)\n",
    "            mask_b = batch[\"mask_b\"].to(DEVICE)\n",
    "            match_ids = batch[\"match_id\"]\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast(dtype=amp_dtype):\n",
    "                    emb_a = transformer(seq_a, days_a, mask_a)\n",
    "                    emb_b = transformer(seq_b, days_b, mask_b)\n",
    "            else:\n",
    "                emb_a = transformer(seq_a, days_a, mask_a)\n",
    "                emb_b = transformer(seq_b, days_b, mask_b)\n",
    "            \n",
    "            emb_a_np = emb_a.float().cpu().numpy()  # Convert to float32 for numpy\n",
    "            emb_b_np = emb_b.float().cpu().numpy()\n",
    "            \n",
    "            for i, mid in enumerate(match_ids):\n",
    "                # ‚úÖ S√©curiser match_id en str\n",
    "                mid_str = str(mid)\n",
    "                emb_w = np.nan_to_num(emb_a_np[i], nan=0.0)\n",
    "                emb_l = np.nan_to_num(emb_b_np[i], nan=0.0)\n",
    "                match_embeddings[mid_str] = (emb_w, emb_l)\n",
    "    \n",
    "    print(f\"  Extracted: {len(match_embeddings):,} matches\")\n",
    "    \n",
    "    # Cr√©er features\n",
    "    results = []\n",
    "    \n",
    "    for row in matches_df.iter_rows(named=True):\n",
    "        match_id = str(row[\"custom_match_id\"])  # ‚úÖ S√©curiser\n",
    "        result = {\"custom_match_id\": row[\"custom_match_id\"]}\n",
    "        \n",
    "        if match_id in match_embeddings:\n",
    "            emb_w, emb_l = match_embeddings[match_id]\n",
    "            \n",
    "            norm_w = np.linalg.norm(emb_w)\n",
    "            norm_l = np.linalg.norm(emb_l)\n",
    "            \n",
    "            if norm_w > 1e-6 and norm_l > 1e-6:\n",
    "                result[\"seq_cosine_sim\"] = float(np.dot(emb_w, emb_l) / (norm_w * norm_l))\n",
    "            else:\n",
    "                result[\"seq_cosine_sim\"] = 0.0\n",
    "            \n",
    "            result[\"seq_l2_distance\"] = float(np.linalg.norm(emb_w - emb_l))\n",
    "            \n",
    "            emb_diff = emb_w - emb_l\n",
    "            for k in range(min(8, EMBEDDING_DIM)):\n",
    "                result[f\"seq_diff_{k}\"] = float(emb_diff[k])\n",
    "            \n",
    "            result[\"seq_norm_winner\"] = float(norm_w)\n",
    "            result[\"seq_norm_loser\"] = float(norm_l)\n",
    "            result[\"has_sequence\"] = 1\n",
    "        else:\n",
    "            result[\"seq_cosine_sim\"] = None\n",
    "            result[\"seq_l2_distance\"] = None\n",
    "            for k in range(min(8, EMBEDDING_DIM)):\n",
    "                result[f\"seq_diff_{k}\"] = None\n",
    "            result[\"seq_norm_winner\"] = None\n",
    "            result[\"seq_norm_loser\"] = None\n",
    "            result[\"has_sequence\"] = 0\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    features_df = pl.DataFrame(results, infer_schema_length=None)\n",
    "    coverage = features_df[\"has_sequence\"].mean()\n",
    "    print(f\"  Coverage: {coverage:.1%}\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# MAIN\n",
    "# ===============================================\n",
    "\n",
    "def main():\n",
    "    t0 = datetime.now()\n",
    "    \n",
    "    print(\"\\n[1/5] Loading matches...\")\n",
    "    matches_df = pl.read_parquet(MATCHES_BASE)\n",
    "    print(f\"  Matches: {len(matches_df):,}\")\n",
    "    \n",
    "    print(\"\\n[2/5] Building dataset...\")\n",
    "    train_data, val_data = build_dataset_single_pass(matches_df)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        SequenceDataset(train_data, SEQ_LENGTH, FEATURE_DIM),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        SequenceDataset(val_data, SEQ_LENGTH, FEATURE_DIM),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[3/5] Training transformer...\")\n",
    "    transformer, predictor = train_transformer(train_loader, val_loader)\n",
    "    \n",
    "    print(\"\\n[4/5] Extracting features...\")\n",
    "    all_data = train_data + val_data\n",
    "    features_df = extract_sequence_features_batched(transformer, all_data, matches_df)\n",
    "    \n",
    "    print(\"\\n[5/5] Saving...\")\n",
    "    output_path = OUTPUT_DIR / \"sequence_features.parquet\"\n",
    "    features_df.write_parquet(output_path)\n",
    "    print(f\"  ‚úÖ Saved: {output_path}\")\n",
    "    \n",
    "    torch.save({\n",
    "        'transformer': transformer.state_dict(),\n",
    "        'predictor': predictor.state_dict(),\n",
    "        'config': {\n",
    "            'd_model': D_MODEL,\n",
    "            'n_heads': N_HEADS,\n",
    "            'n_layers': N_LAYERS,\n",
    "            'embedding_dim': EMBEDDING_DIM,\n",
    "            'seq_length': SEQ_LENGTH,\n",
    "            'feature_dim': FEATURE_DIM,\n",
    "        }\n",
    "    }, OUTPUT_DIR / \"transformer_model.pt\")\n",
    "    \n",
    "    elapsed = (datetime.now() - t0).total_seconds()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"   ‚úÖ PP_15 SEQUENCE TRANSFORMER GOD SOTA v4 COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"   ‚è±Ô∏è  Time: {elapsed:.1f}s\")\n",
    "    print(f\"   üìä Train: {len(train_data):,} | Val: {len(val_data):,}\")\n",
    "    print(\"\"\"\n",
    "üìã v4 FINAL CORRECTIONS:\n",
    "   ‚úÖ Assert len == FEATURE_DIM\n",
    "   ‚úÖ Split sur liste PR√â-FILTR√âE\n",
    "   ‚úÖ AMP dtype explicite (bf16/fp16)\n",
    "   ‚úÖ match_id s√©curis√© str()\n",
    "   ‚úÖ Dropout coh√©rent (self.dropout_p)\n",
    "   ‚úÖ TF32 enabled\n",
    "   ‚úÖ Single pass chronologique\n",
    "   ‚úÖ Mask dans extraction\n",
    "   ‚úÖ Extraction batch√©e\n",
    "   ‚úÖ LayerNorm\n",
    "\n",
    "üîÑ NEXT: PP_16 (Merge GOD Features)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd72cfb0-4e17-4060-ac98-c90e8d4c69a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
