#!/usr/bin/env python3
# ===============================================
# PP_10 - TRAIN STACK SOTA 2026 - NASA GOD MODE v6
# TennisTitan - OOF META + CAPPED BLEND + ALL FIXES
# ===============================================
#
# FIXES v6 (NASA GOD MODE):
#   ‚úÖ OOF-based meta training (not val-based) - TRUE NASA
#   ‚úÖ Optimize Œ± on logloss (not hit-rate)
#   ‚úÖ player_A_id/player_B_id in ID_COLS
#   ‚úÖ Temporal sort by tourney_date_ta
#   ‚úÖ Use feature_list.json
#   ‚úÖ XGBoost callbacks for early stopping
#   ‚úÖ Blend conditionnel on has_odds
#   ‚úÖ Cap Œ± ‚â§ 0.40
#   ‚úÖ Re-binarize has_odds if quantile-scaled
#
# ===============================================

import warnings
warnings.filterwarnings('ignore', message='Mean of empty slice')

import numpy as np
import pandas as pd
import polars as pl
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import TimeSeriesSplit
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss, accuracy_score
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
import joblib
import json
import time
import gc

# ===============================================
# CONFIGURATION
# ===============================================
ROOT = Path.cwd()
DATA_CLEAN = ROOT / "data_clean"

# Two datasets
ML_FINAL_NO_ODDS = DATA_CLEAN / "ml_final_no_odds"
ML_FINAL_WITH_ODDS = DATA_CLEAN / "ml_final_with_odds"

MODELS_DIR = ROOT / "models" / "stack_blend_v6_nasa"
MODELS_DIR.mkdir(parents=True, exist_ok=True)

# Multi-seed configuration
SEEDS = [42, 123, 456, 789, 2024]
N_FOLDS = 5

# Clip bounds
CLIP_MIN = 0.02
CLIP_MAX = 0.98

# Blend configuration
ALPHA_MAX = 0.40  # Maximum weight for odds model
ALPHA_SEARCH = np.arange(0.0, ALPHA_MAX + 0.05, 0.05)  # [0.0, 0.05, ..., 0.40]

# Target column
TARGET_COL = "target_A_wins"

# Colonnes √† exclure
ID_COLS = [
    "custom_match_id", "match_id_ta_dedup", "match_id_ta_source",
    "winner_id", "loser_id",              # Avant shuffle
    "player_A_id", "player_B_id",         # Apr√®s shuffle
    "tourney_name_ta", "tourney_slug_ta",
    "year", "tourney_date_ta",
    TARGET_COL
]

# ===============================================
# METRICS
# ===============================================

def compute_ece(y_true, y_pred, n_bins=20):
    """Expected Calibration Error."""
    bins = np.linspace(0, 1, n_bins + 1)
    idx = np.digitize(y_pred, bins) - 1
    idx = np.clip(idx, 0, n_bins - 1)
    
    ece = 0.0
    for b in range(n_bins):
        mask = idx == b
        if mask.any():
            conf = y_pred[mask].mean()
            acc = y_true[mask].mean()
            w = mask.mean()
            ece += w * abs(acc - conf)
    
    return float(ece)


def compute_hit_rate(y_true, y_pred, threshold=0.5):
    """Hit rate (accuracy) at given threshold."""
    return accuracy_score(y_true, (y_pred >= threshold).astype(int))

def get_date_grouped_cv_splits(dates, n_splits=5, min_train_dates=30):
    """
    CV temporel group√© par DATE (pas par ligne).
    - dates: array tri√© (m√™me ordre que X_train)
    - chaque fold valide un bloc contigu de dates
    """
    dates = np.asarray(dates)
    unique_dates = np.unique(dates)
    unique_dates.sort()

    n_dates = len(unique_dates)
    if n_dates < (n_splits + 1) * 2:
        raise RuntimeError(f"Not enough unique dates for {n_splits} splits: {n_dates}")

    fold_size = n_dates // (n_splits + 1)

    for fold in range(n_splits):
        train_end = (fold + 1) * fold_size
        val_start = train_end
        val_end = min(val_start + fold_size, n_dates)

        if train_end < min_train_dates or val_end <= val_start:
            continue

        train_date_max = unique_dates[train_end - 1]
        val_date_min = unique_dates[val_start]
        val_date_max = unique_dates[val_end - 1]

        train_idx = np.where(dates <= train_date_max)[0]
        val_mask = (dates >= val_date_min) & (dates <= val_date_max)
        val_idx = np.where(val_mask)[0]

        if len(train_idx) == 0 or len(val_idx) == 0:
            continue

        tr_max = dates[train_idx].max()
        va_min = dates[val_idx].min()
        print(f"    Fold {fold+1}: train_max={tr_max} | val_min={va_min} | val_max={dates[val_idx].max()}")
        if tr_max >= va_min:
            raise RuntimeError(f"Temporal leak fold {fold+1}: train_max >= val_min")

        yield train_idx, val_idx
# ===============================================
# LOAD DATA
# ===============================================

def load_feature_list(path: Path) -> list:
    """Load feature list from JSON."""
    feature_file = path / "feature_list.json"
    if feature_file.exists():
        with open(feature_file) as f:
            return json.load(f)
    else:
        raise FileNotFoundError(f"feature_list.json not found in {path}")


def load_data_dual():
    """Charge les deux datasets avec tous les fixes."""
    print("\n" + "="*70)
    print("   LOAD DUAL DATASETS (NASA GOD MODE)")
    print("="*70)
    
    # Load feature lists from JSON
    print("\n  üìã Loading feature lists from JSON...")
    feature_cols_no = load_feature_list(ML_FINAL_NO_ODDS)
    feature_cols_odds = load_feature_list(ML_FINAL_WITH_ODDS)
    print(f"     NO_ODDS features: {len(feature_cols_no)}")
    print(f"     WITH_ODDS features: {len(feature_cols_odds)}")
    
    # Load datasets
    print("\n  üìÅ Loading NO_ODDS dataset...")
    train_no = pl.read_parquet(ML_FINAL_NO_ODDS / "train.parquet")
    val_no = pl.read_parquet(ML_FINAL_NO_ODDS / "val.parquet")
    test_no = pl.read_parquet(ML_FINAL_NO_ODDS / "test.parquet")
    print(f"     Train: {train_no.shape}, Val: {val_no.shape}, Test: {test_no.shape}")
    
    print("\n  üìÅ Loading WITH_ODDS dataset...")
    train_odds = pl.read_parquet(ML_FINAL_WITH_ODDS / "train.parquet")
    val_odds = pl.read_parquet(ML_FINAL_WITH_ODDS / "val.parquet")
    test_odds = pl.read_parquet(ML_FINAL_WITH_ODDS / "test.parquet")
    print(f"     Train: {train_odds.shape}, Val: {val_odds.shape}, Test: {test_odds.shape}")
    
    # Sort by tourney_date_ta
    # ‚úÖ FIX CRITICAL: Sort by date BEFORE TimeSeriesSplit
    print("\n  üïê Sorting by tourney_date_ta (CRITICAL for TimeSeriesSplit)...")
    
    # Use multiple sort keys for deterministic ordering
    sort_cols = ["tourney_date_ta"]
    if "custom_match_id" in train_no.columns:
        sort_cols = ["tourney_date_ta", "custom_match_id"]
    
    train_no = train_no.sort(sort_cols)
    val_no = val_no.sort(sort_cols)
    test_no = test_no.sort(sort_cols)
    train_odds = train_odds.sort(sort_cols)
    val_odds = val_odds.sort(sort_cols)
    test_odds = test_odds.sort(sort_cols)
    
    # ‚úÖ VERIFY SORT (guard rail)
    dates = train_no["tourney_date_ta"].to_list()
    is_sorted = all(dates[i] <= dates[i+1] for i in range(len(dates)-1))
    if is_sorted:
        print(f"     ‚úÖ Verified: train is correctly sorted by {sort_cols}")
    else:
        raise RuntimeError("üî¥ CRITICAL: train is NOT sorted after sort()! Check Polars version.")
    
    print(f"     Date range: {dates[0]} ‚Üí {dates[-1]}")
    
    # Extract has_odds with validation
    print("\n  üìä has_odds extraction...")
    if "has_odds" not in train_odds.columns:
        print("     ‚ö†Ô∏è has_odds not found in WITH_ODDS dataset!")
        print("     ‚Üí Creating default has_odds = 0")
        has_odds_train = np.zeros(len(train_odds), dtype=np.int8)
        has_odds_val = np.zeros(len(val_odds), dtype=np.int8)
        has_odds_test = np.zeros(len(test_odds), dtype=np.int8)
    else:
        # V√©rifier le dtype et les valeurs
        raw_has_odds = train_odds["has_odds"].to_numpy()
        print(f"     has_odds dtype: {train_odds['has_odds'].dtype}")
        print(f"     has_odds min/max: {raw_has_odds.min():.4f} / {raw_has_odds.max():.4f}")
        print(f"     has_odds sample: {raw_has_odds[:5]}")
        
        # ‚úÖ FIX: Si has_odds a √©t√© quantile-scal√© (valeurs hors [0,1])
        # ‚Üí Re-binariser avec seuil 0 (apr√®s quantile->normal, 0 original ‚Üí n√©gatif, 1 original ‚Üí positif)
        if raw_has_odds.min() < -0.5 or raw_has_odds.max() > 1.5:
            print("     üî¥ has_odds semble avoir √©t√© quantile-scal√©!")
            print("     ‚Üí Re-binarisation avec seuil > 0")
            has_odds_train = (train_odds["has_odds"].to_numpy() > 0).astype(np.int8)
            has_odds_val = (val_odds["has_odds"].to_numpy() > 0).astype(np.int8)
            has_odds_test = (test_odds["has_odds"].to_numpy() > 0).astype(np.int8)
        else:
            # has_odds normal (0/1)
            has_odds_train = train_odds["has_odds"].to_numpy().astype(np.int8)
            has_odds_val = val_odds["has_odds"].to_numpy().astype(np.int8)
            has_odds_test = test_odds["has_odds"].to_numpy().astype(np.int8)
    
    print(f"\n  üìä has_odds distribution:")
    print(f"     Train: {has_odds_train.mean()*100:.1f}% with real odds ({has_odds_train.sum():,}/{len(has_odds_train):,})")
    print(f"     Val:   {has_odds_val.mean()*100:.1f}% with real odds ({has_odds_val.sum():,}/{len(has_odds_val):,})")
    print(f"     Test:  {has_odds_test.mean()*100:.1f}% with real odds ({has_odds_test.sum():,}/{len(has_odds_test):,})")
    
    # Clean feature lists (remove any IDs)
    feature_cols_no = [c for c in feature_cols_no if c not in ID_COLS and not c.endswith("_id")]
    feature_cols_odds = [c for c in feature_cols_odds if c not in ID_COLS and not c.endswith("_id")]
    
    # Convert to numpy
    X_train_no = train_no.select(feature_cols_no).to_numpy().astype(np.float32)
    X_val_no = val_no.select(feature_cols_no).to_numpy().astype(np.float32)
    X_test_no = test_no.select(feature_cols_no).to_numpy().astype(np.float32)
    
    X_train_odds = train_odds.select(feature_cols_odds).to_numpy().astype(np.float32)
    X_val_odds = val_odds.select(feature_cols_odds).to_numpy().astype(np.float32)
    X_test_odds = test_odds.select(feature_cols_odds).to_numpy().astype(np.float32)
    
    y_train = train_no[TARGET_COL].to_numpy().astype(np.int32)
    y_val = val_no[TARGET_COL].to_numpy().astype(np.int32)
    y_test = test_no[TARGET_COL].to_numpy().astype(np.int32)
    
    # Leakage check
    print("\n  üîç Checking for leakage...")
    check_leakage(X_train_no, y_train, feature_cols_no, "NO_ODDS")
    check_leakage(X_train_odds, y_train, feature_cols_odds, "WITH_ODDS")
    
    # ‚úÖ FIX: Extract dates BEFORE deleting dataframes
    dates_train = train_no["tourney_date_ta"].to_numpy()
    
    del train_no, val_no, test_no, train_odds, val_odds, test_odds
    gc.collect()
    
    return {
        "no_odds": {
            "X_train": X_train_no, "X_val": X_val_no, "X_test": X_test_no,
            "feature_cols": feature_cols_no
        },
        "with_odds": {
            "X_train": X_train_odds, "X_val": X_val_odds, "X_test": X_test_odds,
            "feature_cols": feature_cols_odds
        },
        "y_train": y_train, "y_val": y_val, "y_test": y_test,
        "has_odds_train": has_odds_train, "has_odds_val": has_odds_val, "has_odds_test": has_odds_test,
        "dates_train": dates_train,  # ‚Üê AJOUTER
    }


def check_leakage(X, y, feature_cols, name):
    """Check for suspicious correlations."""
    suspicious = []
    for i, col in enumerate(feature_cols):
        valid_mask = ~np.isnan(X[:, i])
        if valid_mask.sum() > 100:
            corr = np.corrcoef(X[valid_mask, i], y[valid_mask])[0, 1]
            if not np.isnan(corr) and abs(corr) > 0.45:
                suspicious.append((col, corr))
    
    if suspicious:
        suspicious.sort(key=lambda x: abs(x[1]), reverse=True)
        print(f"     ‚ö†Ô∏è {name} - Suspicious (|corr| > 0.45):")
        for col, corr in suspicious[:5]:
            print(f"        {'üî¥' if abs(corr) > 0.7 else '‚ö†Ô∏è'} {col}: {corr:.4f}")
    else:
        print(f"     ‚úÖ {name} - No suspicious correlations")


# ===============================================
# MODEL DEFINITIONS
# ===============================================

def get_lgbm_model(seed=42):
    return lgb.LGBMClassifier(
        objective="binary",
        n_estimators=5000,
        learning_rate=0.02,
        num_leaves=31,
        max_depth=7,
        min_data_in_leaf=500,
        min_sum_hessian_in_leaf=10.0,
        lambda_l2=10.0,
        colsample_bytree=0.7,
        subsample=0.8,
        subsample_freq=1,
        max_bin=127,
        random_state=seed,
        verbose=-1,
        force_row_wise=True,
        n_jobs=-1
    )


def get_xgb_model(seed=42):
    # Note: early_stopping sera g√©r√© via callbacks dans .fit()
    return xgb.XGBClassifier(
        objective="binary:logistic",
        n_estimators=5000,
        learning_rate=0.02,
        max_depth=7,
        min_child_weight=5,
        subsample=0.8,
        colsample_bytree=0.7,
        reg_lambda=8.0,
        tree_method="hist",
        random_state=seed,
        n_jobs=-1,
        verbosity=0,
        eval_metric='logloss'
    )


def get_cat_model(seed=42):
    return CatBoostClassifier(
        loss_function="Logloss",
        iterations=5000,
        learning_rate=0.03,
        depth=7,
        l2_leaf_reg=8.0,
        random_seed=seed,
        verbose=0
    )


# ===============================================
# OOF PREDICTIONS - NASA GOD MODE
# ===============================================

def compute_oof_single_seed(X_train, y_train, dates_train, model_fn, model_name, seed=42):
    """
    OOF avec GroupTimeSeriesSplit par DATE.
    ‚úÖ NASA: Split par date unique, pas par ligne.
    """
    oof_preds = np.full(len(X_train), np.nan)
    
    # ‚úÖ FIX: Use date-grouped CV instead of TimeSeriesSplit
    for fold_idx, (train_idx, val_idx) in enumerate(get_date_grouped_cv_splits(dates_train, N_FOLDS)):
        X_tr, X_va = X_train[train_idx], X_train[val_idx]
        y_tr, y_va = y_train[train_idx], y_train[val_idx]
        
        model = model_fn(seed)
        
        if "lgbm" in model_name.lower():
            model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],
                     callbacks=[lgb.early_stopping(200, verbose=False)])
        elif "xgb" in model_name.lower():
            try:
                xgb_callbacks = [xgb.callback.EarlyStopping(rounds=200, save_best=True)]
                model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],
                         callbacks=xgb_callbacks, verbose=False)
            except TypeError:
                try:
                    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],
                             early_stopping_rounds=200, verbose=False)
                except TypeError:
                    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)
        elif "cat" in model_name.lower():
            model.fit(X_tr, y_tr, eval_set=(X_va, y_va),
                     early_stopping_rounds=200, verbose=False)
        else:
            model.fit(X_tr, y_tr)
        
        oof_preds[val_idx] = np.clip(model.predict_proba(X_va)[:, 1], CLIP_MIN, CLIP_MAX)
    
    return oof_preds


def compute_oof_multiseed(X_train, y_train, dates_train, model_fn, model_name):
    """OOF multi-seed."""
    print(f"    Computing OOF for {model_name}...")
    
    all_oof = []
    for seed in SEEDS:
        oof = compute_oof_single_seed(X_train, y_train, dates_train, model_fn, model_name, seed)
        all_oof.append(oof)
    
    # Moyenne (NaN reste NaN)
    oof_stack = np.stack(all_oof)
    oof_mean = np.nanmean(oof_stack, axis=0)
    oof_mean = np.where(~np.isnan(oof_mean), np.clip(oof_mean, CLIP_MIN, CLIP_MAX), np.nan)
    
    # M√©triques sur valides
    valid_mask = ~np.isnan(oof_mean)
    valid_y = y_train[valid_mask]
    valid_preds = oof_mean[valid_mask]
    
    auc = roc_auc_score(valid_y, valid_preds)
    ll = log_loss(valid_y, valid_preds)
    coverage = valid_mask.mean() * 100
    
    print(f"      {model_name}: AUC={auc:.4f}, LogLoss={ll:.4f}, Coverage={coverage:.1f}%")
    
    return oof_mean, valid_mask


# ===============================================
# TRAIN STACK WITH OOF META - NASA GOD MODE
# ===============================================

def train_stack_oof(X_train, y_train, X_val, y_val, dates_train, name_prefix=""):
    """
    ‚úÖ NASA GOD MODE: Meta entra√Æn√©e sur OOF, pas sur val!
    
    1. Compute OOF predictions (TimeSeriesSplit)
    2. Train meta on OOF (valid indices only)
    3. Train final models on full train
    4. Return everything
    """
    print(f"\n  {'='*60}")
    print(f"  TRAINING STACK: {name_prefix} (NASA OOF MODE)")
    print(f"  {'='*60}")
    
    models_config = {
        "lgbm": get_lgbm_model,
        "xgb": get_xgb_model,
        "cat": get_cat_model
    }
    
    # =========================================
    # STEP 1: Compute OOF predictions
    # =========================================
    print("\n  [1/3] Computing OOF predictions...")
    oof_preds = {}
    valid_mask = None
    
    for model_name, model_fn in models_config.items():
        oof, mask = compute_oof_multiseed(X_train, y_train, dates_train, model_fn, model_name.upper())
        oof_preds[model_name] = oof
        # ‚úÖ FIX: Combine masks with AND to ensure no NaN in any model
        if valid_mask is None:
            valid_mask = mask.copy()
        else:
            valid_mask &= mask
    
    print(f"\n  OOF Coverage: {valid_mask.mean()*100:.1f}% ({valid_mask.sum():,}/{len(valid_mask):,})")
    
    # =========================================
    # STEP 2: Train meta on OOF (NASA!)
    # =========================================
    print("\n  [2/3] Training meta-learner on OOF (NASA mode)...")
    
    X_meta_oof = np.column_stack([
        oof_preds["lgbm"][valid_mask],
        oof_preds["xgb"][valid_mask],
        oof_preds["cat"][valid_mask]
    ])
    y_meta = y_train[valid_mask]
    
    meta = LogisticRegression(C=1.0, max_iter=2000, solver="lbfgs")
    meta.fit(X_meta_oof, y_meta)
    
    # Meta OOF performance
    meta_oof_pred = np.clip(meta.predict_proba(X_meta_oof)[:, 1], CLIP_MIN, CLIP_MAX)
    meta_auc = roc_auc_score(y_meta, meta_oof_pred)
    meta_ll = log_loss(y_meta, meta_oof_pred)
    print(f"    META OOF: AUC={meta_auc:.4f}, LogLoss={meta_ll:.4f}")
    
    weights = dict(zip(["lgbm", "xgb", "cat"], meta.coef_[0]))
    print(f"    Weights: lgbm={weights['lgbm']:.3f}, xgb={weights['xgb']:.3f}, cat={weights['cat']:.3f}")
    
    # =========================================
    # STEP 3: Train final models on full train
    # =========================================
    print("\n  [3/3] Training final models on full train...")
    
    final_models = {}
    val_preds = {}
    
    for model_name, model_fn in models_config.items():
        seed_preds_val = []
        seed_models = []
        
        for seed in SEEDS:
            model = model_fn(seed)
            
            if model_name == "lgbm":
                model.fit(X_train, y_train, eval_set=[(X_val, y_val)],
                         callbacks=[lgb.early_stopping(200, verbose=False)])
            elif model_name == "xgb":
                # ‚úÖ FIX: Version-agnostic XGBoost early stopping
                try:
                    # Method 1: callbacks (XGBoost >= 2.0)
                    xgb_callbacks = [xgb.callback.EarlyStopping(rounds=200, save_best=True)]
                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],
                             callbacks=xgb_callbacks, verbose=False)
                except TypeError:
                    try:
                        # Method 2: early_stopping_rounds in fit() (XGBoost < 1.6)
                        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],
                                 early_stopping_rounds=200, verbose=False)
                    except TypeError:
                        # Method 3: No early stopping (fallback)
                        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
            elif model_name == "cat":
                model.fit(X_train, y_train, eval_set=(X_val, y_val),
                         early_stopping_rounds=200, verbose=False)
            
            pred_val = np.clip(model.predict_proba(X_val)[:, 1], CLIP_MIN, CLIP_MAX)
            seed_preds_val.append(pred_val)
            seed_models.append(model)
        
        val_preds[model_name] = np.mean(seed_preds_val, axis=0)
        final_models[model_name] = seed_models
        
        auc = roc_auc_score(y_val, val_preds[model_name])
        ll = log_loss(y_val, val_preds[model_name])
        print(f"    {model_name.upper()} VAL: AUC={auc:.4f}, LogLoss={ll:.4f}")
    
    # Meta on validation
    X_meta_val = np.column_stack([val_preds["lgbm"], val_preds["xgb"], val_preds["cat"]])
    meta_val_pred = np.clip(meta.predict_proba(X_meta_val)[:, 1], CLIP_MIN, CLIP_MAX)
    
    meta_val_auc = roc_auc_score(y_val, meta_val_pred)
    meta_val_ll = log_loss(y_val, meta_val_pred)
    print(f"    META VAL: AUC={meta_val_auc:.4f}, LogLoss={meta_val_ll:.4f}")
    
    return final_models, meta, val_preds, oof_preds, valid_mask


def predict_stack(final_models, meta, X):
    """Pr√©dit avec un stack."""
    preds = {}
    for name, seed_models in final_models.items():
        seed_preds = [np.clip(m.predict_proba(X)[:, 1], CLIP_MIN, CLIP_MAX) 
                      for m in seed_models]
        preds[name] = np.mean(seed_preds, axis=0)
    
    X_meta = np.column_stack([preds["lgbm"], preds["xgb"], preds["cat"]])
    return np.clip(meta.predict_proba(X_meta)[:, 1], CLIP_MIN, CLIP_MAX)


# ===============================================
# BLEND STRATEGY
# ===============================================

def blend_conditional(p_sota, p_market, has_odds, alpha):
    """
    Blend conditionnel bas√© sur has_odds.
    - has_odds == 0 ‚Üí p_sota (100% SOTA)
    - has_odds == 1 ‚Üí (1-Œ±)*p_sota + Œ±*p_market
    """
    alpha = min(alpha, ALPHA_MAX)
    return np.where(has_odds == 0, p_sota, (1 - alpha) * p_sota + alpha * p_market)


def optimize_alpha(p_sota_val, p_market_val, y_val, has_odds_val):
    """
    ‚úÖ FIX: Optimise Œ± sur LOGLOSS (pas hit-rate), sur matchs avec odds.
    """
    print("\n" + "="*70)
    print("   OPTIMIZE BLEND ALPHA (on LogLoss, capped at {:.0%})".format(ALPHA_MAX))
    print("="*70)
    
    results = []
    mask_odds = has_odds_val == 1
    n_with_odds = mask_odds.sum()
    
    print(f"\n  Matchs with odds in val: {n_with_odds:,} ({n_with_odds/len(y_val)*100:.1f}%)")
    
    for alpha in ALPHA_SEARCH:
        p_blend = blend_conditional(p_sota_val, p_market_val, has_odds_val, alpha)
        
        # M√©triques globales
        hit_rate = compute_hit_rate(y_val, p_blend)
        auc = roc_auc_score(y_val, p_blend)
        ll = log_loss(y_val, p_blend)
        
        # ‚úÖ FIX: M√©triques sur matchs AVEC odds (l√† o√π Œ± compte!)
        if n_with_odds > 0:
            ll_odds = log_loss(y_val[mask_odds], p_blend[mask_odds])
            auc_odds = roc_auc_score(y_val[mask_odds], p_blend[mask_odds])
            hr_odds = compute_hit_rate(y_val[mask_odds], p_blend[mask_odds])
        else:
            ll_odds = ll
            auc_odds = auc
            hr_odds = hit_rate
        
        results.append({
            "alpha": float(alpha),
            "hit_rate": float(hit_rate),
            "auc": float(auc),
            "logloss": float(ll),
            "logloss_odds": float(ll_odds),
            "auc_odds": float(auc_odds),
            "hit_rate_odds": float(hr_odds)
        })
        
        print(f"  Œ±={alpha:.2f}: LL_odds={ll_odds:.4f}, AUC_odds={auc_odds:.4f}, HR_odds={hr_odds:.4f}")
    
    # ‚úÖ FIX: Best Œ± by LOGLOSS on odds matches (not hit-rate!)
    best = min(results, key=lambda x: x["logloss_odds"])
    print(f"\n  ‚úÖ Best Œ± = {best['alpha']:.2f} (LogLoss_odds={best['logloss_odds']:.4f})")
    
    return best["alpha"], results


# ===============================================
# LEAKAGE DIAGNOSTIC FUNCTIONS (PRE-FLIGHT CHECKS)
# ===============================================

def quick_feature_sanity(feature_cols, name=""):
    """Check for odds-like features in feature list (should not be in NO_ODDS)."""
    bad_patterns = ["odds", "implied", "book", "market", "vig", "closing", "opening"]
    # Whitelist markov_fair_odds (derived from stats, not real odds)
    whitelist = ["markov_fair_odds", "markov_edge"]
    suspects = [c for c in feature_cols
                if any(p in c.lower() for p in bad_patterns)
                and not any(w in c.lower() for w in whitelist)]
    print(f"\nüîé Sanity {name}: {len(suspects)} odds-like features found")
    if suspects:
        print("   ‚ö†Ô∏è Examples:", suspects[:20])
        if "no_odds" in name.lower():
            raise RuntimeError(f"LEAKAGE RISK: odds-like features present in {name} feature list!")
    return suspects


def permutation_probe_auc_time(X, y, dates, seed=42, n=60000, train_frac=0.8):
    print("\nüß™ Running TIME-AWARE permutation probe (DATE-GROUPED)...")

    dates = np.asarray(dates)
    n = min(n, len(y))
    start = max(0, len(y) - n)  # bloc contigu le plus r√©cent
    Xs, ys, ds = X[start:], y[start:], dates[start:]

    # Split par DATES uniques (jamais couper une m√™me date)
    unique_dates = np.unique(ds)
    unique_dates.sort()

    cut = int(len(unique_dates) * train_frac)
    cut = max(1, min(cut, len(unique_dates) - 1))  # garde-fous

    cutoff_date = unique_dates[cut]  # val commence √† cette date
    train_mask = ds < cutoff_date
    val_mask = ds >= cutoff_date

    if train_mask.sum() < 2000 or val_mask.sum() < 2000:
        raise RuntimeError(f"Probe split too small: train={train_mask.sum()}, val={val_mask.sum()}")

    Xtr, Xva = Xs[train_mask], Xs[val_mask]
    ytr, yva = ys[train_mask], ys[val_mask]
    dtr, dva = ds[train_mask], ds[val_mask]

    print(f"   Cutoff date: {cutoff_date}")
    print(f"   Train dates: {dtr.min()} ‚Üí {dtr.max()}")
    print(f"   Val dates:   {dva.min()} ‚Üí {dva.max()}")
    assert dtr.max() < dva.min(), "TEMPORAL LEAK IN PROBE!"

    m = lgb.LGBMClassifier(
        n_estimators=300, learning_rate=0.05, num_leaves=31,
        random_state=seed, n_jobs=-1, verbose=-1, force_row_wise=True
    )
    m.fit(Xtr, ytr)
    p = m.predict_proba(Xva)[:, 1]
    auc_real = roc_auc_score(yva, p)

    rng = np.random.default_rng(seed)
    ytr_shuf = rng.permutation(ytr)
    yva_shuf = rng.permutation(yva)

    m2 = lgb.LGBMClassifier(
        n_estimators=300, learning_rate=0.05, num_leaves=31,
        random_state=seed, n_jobs=-1, verbose=-1, force_row_wise=True
    )
    m2.fit(Xtr, ytr_shuf)
    p2 = m2.predict_proba(Xva)[:, 1]
    auc_shuf = roc_auc_score(yva_shuf, p2)

    print(f"   AUC real={auc_real:.4f} | AUC shuffled={auc_shuf:.4f}")
    if auc_shuf > 0.55:
        raise RuntimeError("LEAKAGE/BUG: shuffled AUC too high")

    return auc_real, auc_shuf



def single_feature_auc_scan(X, y, feature_cols, topk=15, n=60000, seed=42):
    """
    Scan each feature individually to find suspiciously predictive ones.
    Single feature AUC > 0.80 is very suspicious for tennis prediction.
    """
    print("\nüîç Single-feature AUC scan...")
    rng = np.random.default_rng(seed)
    idx = rng.choice(len(y), size=min(n, len(y)), replace=False)
    Xs, ys = X[idx], y[idx]
    
    scores = []
    for j, col in enumerate(feature_cols):
        x = Xs[:, j]
        m = ~np.isnan(x)
        if m.sum() < 2000:
            continue
        try:
            auc = roc_auc_score(ys[m], x[m])
            auc = max(auc, 1 - auc)  # Symmetric AUC
            scores.append((col, auc))
        except:
            pass
    
    scores.sort(key=lambda t: t[1], reverse=True)
    print("   Top single-feature AUC (symmetric):")
    for col, auc in scores[:topk]:
        flag = "‚ö†Ô∏è" if auc > 0.60 else "  "
        print(f"   {flag} {auc:.4f}  {col}")
    
    if scores and scores[0][1] > 0.80:
        print("\n   üî¥ ALERT: Suspiciously strong single feature(s) detected!")
        print("   ‚Üí Inspect these features for potential leakage")
    
    return scores[:topk]


def run_preflight_checks(data):
    """Run all pre-flight diagnostic checks."""
    print("\n" + "="*70)
    print("   PRE-FLIGHT LEAKAGE CHECKS")
    print("="*70)
    
    # 1. Feature sanity check
    quick_feature_sanity(data["no_odds"]["feature_cols"], "NO_ODDS")
    quick_feature_sanity(data["with_odds"]["feature_cols"], "WITH_ODDS")
    
    # 2. Permutation probe
    auc_real, auc_shuf = permutation_probe_auc_time(
        data["no_odds"]["X_train"], data["y_train"], data["dates_train"]
    )
    
    # 3. Single-feature AUC scan
    top_features = single_feature_auc_scan(
        data["no_odds"]["X_train"], data["y_train"], 
        data["no_odds"]["feature_cols"]
    )
    
    print("\n" + "="*70)
    print(f"   ‚úÖ PRE-FLIGHT CHECKS PASSED (AUC real={auc_real:.4f})")
    print("="*70)
    
    return {
        "auc_real": auc_real,
        "auc_shuffled": auc_shuf,
        "top_features": top_features
    }


# ===============================================
# MAIN PIPELINE
# ===============================================

def main():
    t0 = time.perf_counter()
    
    print("\n" + "="*70)
    print("   PP_10 - NASA GOD MODE v6 FINAL")
    print("   OOF META + BLEND CAP√â + LOGLOSS OPTIMIZATION")
    print("="*70)
    print(f"   {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   Seeds: {SEEDS}")
    print(f"   Folds: {N_FOLDS}")
    print(f"   Alpha max: {ALPHA_MAX}")
    print("="*70)
    print("""
   ‚úÖ NASA GOD MODE FEATURES:
   - OOF-based meta training (TimeSeriesSplit)
   - Optimize Œ± on LogLoss (not hit-rate)
   - Blend conditionnel on has_odds
   - Cap Œ± ‚â§ 40%
   - Multi-seed (5 seeds)
   - Isotonic calibration
""")
    
    # 1. Load data
    print("\n[1/6] Loading dual datasets...")
    data = load_data_dual()
    
    # 1b. PRE-FLIGHT LEAKAGE CHECKS (30 sec - 2 min)
    preflight_results = run_preflight_checks(data)
    
    # 2. Train SOTA stack (no_odds) with OOF
    print("\n[2/6] Training SOTA stack (no_odds) - NASA OOF MODE...")
    models_sota, meta_sota, val_preds_sota, oof_sota, valid_mask_sota = train_stack_oof(
        data["no_odds"]["X_train"], data["y_train"],
        data["no_odds"]["X_val"], data["y_val"],
        data["dates_train"],  # ‚Üê AJOUTER
        name_prefix="SOTA (no_odds)"
    )
    
    gc.collect()
    
    # 3. Train MARKET stack (with_odds) with OOF
    print("\n[3/6] Training MARKET stack (with_odds) - NASA OOF MODE...")
    models_market, meta_market, val_preds_market, oof_market, valid_mask_market = train_stack_oof(
        data["with_odds"]["X_train"], data["y_train"],
        data["with_odds"]["X_val"], data["y_val"],
        data["dates_train"],
        name_prefix="MARKET (with_odds)"
    )
    
    gc.collect()
    
    # 4. Predict on val and test
    print("\n[4/6] Predicting on val and test...")
    p_sota_val = predict_stack(models_sota, meta_sota, data["no_odds"]["X_val"])
    p_sota_test = predict_stack(models_sota, meta_sota, data["no_odds"]["X_test"])
    
    p_market_val = predict_stack(models_market, meta_market, data["with_odds"]["X_val"])
    p_market_test = predict_stack(models_market, meta_market, data["with_odds"]["X_test"])
    
    # 5. Optimize alpha (on LogLoss!)
    print("\n[5/6] Optimizing blend alpha (on LogLoss)...")
    best_alpha, alpha_results = optimize_alpha(
        p_sota_val, p_market_val, data["y_val"], data["has_odds_val"]
    )
    
    # Final predictions
    p_final_val = blend_conditional(p_sota_val, p_market_val, data["has_odds_val"], best_alpha)
    p_final_test = blend_conditional(p_sota_test, p_market_test, data["has_odds_test"], best_alpha)
    
    # 6. Isotonic calibration
    print("\n[6/6] Applying Isotonic Calibration...")
    iso = IsotonicRegression(out_of_bounds='clip')
    iso.fit(p_final_val, data["y_val"])
    
    p_final_val_cal = np.clip(iso.predict(p_final_val), CLIP_MIN, CLIP_MAX)
    p_final_test_cal = np.clip(iso.predict(p_final_test), CLIP_MIN, CLIP_MAX)
    
    # =========================================
    # FINAL RESULTS
    # =========================================
    print("\n" + "="*70)
    print("   FINAL RESULTS (NASA GOD MODE)")
    print("="*70)
    
    print("\n  üìä Individual stacks (TEST):")
    sota_auc = roc_auc_score(data["y_test"], p_sota_test)
    sota_ll = log_loss(data["y_test"], p_sota_test)
    sota_hr = compute_hit_rate(data["y_test"], p_sota_test)
    print(f"    SOTA (no_odds):   HR={sota_hr:.4f}, AUC={sota_auc:.4f}, LL={sota_ll:.4f}")
    
    market_auc = roc_auc_score(data["y_test"], p_market_test)
    market_ll = log_loss(data["y_test"], p_market_test)
    market_hr = compute_hit_rate(data["y_test"], p_market_test)
    print(f"    MARKET (w/odds):  HR={market_hr:.4f}, AUC={market_auc:.4f}, LL={market_ll:.4f}")
    
    print(f"\n  üìä BLEND (Œ±={best_alpha:.2f}) on TEST:")
    blend_hr = compute_hit_rate(data["y_test"], p_final_test)
    blend_auc = roc_auc_score(data["y_test"], p_final_test)
    blend_ll = log_loss(data["y_test"], p_final_test)
    print(f"    Pre-cal:  HR={blend_hr:.4f}, AUC={blend_auc:.4f}, LL={blend_ll:.4f}")
    
    final_hr = compute_hit_rate(data["y_test"], p_final_test_cal)
    final_auc = roc_auc_score(data["y_test"], p_final_test_cal)
    final_ll = log_loss(data["y_test"], p_final_test_cal)
    final_brier = brier_score_loss(data["y_test"], p_final_test_cal)
    final_ece = compute_ece(data["y_test"], p_final_test_cal)
    print(f"    Post-cal: HR={final_hr:.4f}, AUC={final_auc:.4f}, LL={final_ll:.4f}")
    print(f"    Brier={final_brier:.4f}, ECE={final_ece:.4f}")
    
    print("\n  üìä Breakdown by has_odds (TEST):")
    for flag, label in [(0, "NO odds"), (1, "WITH odds")]:
        mask = data["has_odds_test"] == flag
        if mask.sum() > 0:
            hr = compute_hit_rate(data["y_test"][mask], p_final_test_cal[mask])
            auc = roc_auc_score(data["y_test"][mask], p_final_test_cal[mask])
            ll = log_loss(data["y_test"][mask], p_final_test_cal[mask])
            print(f"    {label}: n={mask.sum():,}, HR={hr:.4f}, AUC={auc:.4f}, LL={ll:.4f}")
    
    # Save
    print("\n" + "="*70)
    print("   SAVE ARTIFACTS")
    print("="*70)
    
    metrics = {
        "best_alpha": float(best_alpha),
        "alpha_max": float(ALPHA_MAX),
        "test": {
            "hit_rate": float(final_hr),
            "auc": float(final_auc),
            "logloss": float(final_ll),
            "brier": float(final_brier),
            "ece": float(final_ece)
        },
        "sota_only": {"hit_rate": float(sota_hr), "auc": float(sota_auc), "logloss": float(sota_ll)},
        "market_only": {"hit_rate": float(market_hr), "auc": float(market_auc), "logloss": float(market_ll)},
        "oof_coverage_sota": float(valid_mask_sota.mean()),
        "oof_coverage_market": float(valid_mask_market.mean()),
        "alpha_search": alpha_results
    }
    
    artifacts = {
        "models_sota": models_sota,
        "meta_sota": meta_sota,
        "models_market": models_market,
        "meta_market": meta_market,
        "iso": iso,
        "best_alpha": best_alpha,
        "feature_cols_sota": data["no_odds"]["feature_cols"],
        "feature_cols_market": data["with_odds"]["feature_cols"],
        "metrics": metrics,
        "seeds": SEEDS,
        "n_folds": N_FOLDS,
        "clip_bounds": (CLIP_MIN, CLIP_MAX),
        "created": datetime.now().isoformat(),
        "version": "v6_nasa_god_mode"
    }
    
    joblib.dump(artifacts, MODELS_DIR / "blend_stack_v6_nasa.joblib")
    print(f"  ‚úÖ Models saved to {MODELS_DIR}")
    
    with open(MODELS_DIR / "metrics_v6.json", "w") as f:
        json.dump(metrics, f, indent=2)
    print(f"  ‚úÖ Metrics saved")
    
    elapsed = time.perf_counter() - t0
    
    print("\n" + "="*70)
    print(f"   ‚úÖ PP_10 NASA GOD MODE v6 COMPLETE!")
    print(f"   Time: {elapsed/60:.1f} min")
    print("="*70)
    
    print(f"""
üìã NASA GOD MODE SUMMARY:
   - SOTA model: {len(data["no_odds"]["feature_cols"])} features, OOF coverage={valid_mask_sota.mean()*100:.1f}%
   - MARKET model: {len(data["with_odds"]["feature_cols"])} features, OOF coverage={valid_mask_market.mean()*100:.1f}%
   - Best Œ±: {best_alpha:.2f} (optimized on LogLoss, capped at {ALPHA_MAX:.0%})
   
üìä FINAL TEST RESULTS:
   HitRate: {final_hr:.4f} ({final_hr*100:.2f}%)
   AUC:     {final_auc:.4f}
   LogLoss: {final_ll:.4f}
   ECE:     {final_ece:.4f}
   
üéØ BLEND LOGIC:
   - has_odds=0: 100% SOTA
   - has_odds=1: {(1-best_alpha)*100:.0f}% SOTA + {best_alpha*100:.0f}% MARKET

‚úÖ NASA GOD MODE FEATURES:
   - OOF-based meta (not val-based) ‚úÖ
   - Œ± optimized on LogLoss (not hit-rate) ‚úÖ
   - TimeSeriesSplit with temporal sort ‚úÖ
   - Multi-seed averaging ‚úÖ
   - Isotonic calibration ‚úÖ
""")
    
    return artifacts


if __name__ == "__main__":
    results = main()